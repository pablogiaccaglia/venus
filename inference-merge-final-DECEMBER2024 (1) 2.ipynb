{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vTB9BPag1Gx"
   },
   "source": [
    "# Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BJLaMN6hpxW",
    "outputId": "9c556160-e8f8-47d7-ea09-2ae55ac98d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q albumentations\n",
    "!pip install -q natsort\n",
    "!pip install -q patchify\n",
    "!pip install -q lightning\n",
    "!pip install -q lightning[extra]\n",
    "!pip install -q segmentation_models_pytorch\n",
    "!pip install -q wandb\n",
    "!pip install -q \"monai[einops]==1.4\"\n",
    "!pip install -q nibabel\n",
    "!pip install -q ttach\n",
    "!pip install -q medpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zafBCz4qi2ca",
    "outputId": "646b3f4b-5fe5-4e5a-95c0-9e5454091bbb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "!pip install -q medpy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tTw5AU5i7M6",
    "outputId": "2ec5852a-05af-4415-867b-cd2c0498507d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.2.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: /opt/conda/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.25.2\n",
      "scipy version: 1.15.2\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.17.1\n",
      "tqdm version: 4.65.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "einops version: 0.8.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import tempfile\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Third-party libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "import albumentations as A\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from skimage import filters\n",
    "from skimage.measure import label as label_fn, regionprops\n",
    "from skimage import morphology\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "import pprint as pp\n",
    "\n",
    "# MONAI related imports\n",
    "from monai.config import print_config\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    AsDiscrete, AsDiscreted, EnsureChannelFirstd, Compose, CropForegroundd,\n",
    "    LoadImaged, Orientationd, RandCropByPosNegLabeld, SaveImaged, ScaleIntensityRanged,\n",
    "    Spacingd, Invertd, ResizeWithPadOrCropd, Resized, MapTransform, ScaleIntensityd,\n",
    "    LabelToContourd, ForegroundMaskd, HistogramNormalized, RandFlipd, RandGridDistortiond,\n",
    "    RandHistogramShiftd, RandRotated\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.utils.type_conversion import convert_to_numpy\n",
    "\n",
    "# PyTorch Lightning related imports\n",
    "import lightning.pytorch as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Patchify\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "from natsort import natsorted, ns\n",
    "from utiils import *\n",
    "import monai\n",
    "\n",
    "from boundaryloss.dataloader import dist_map_transform\n",
    "from boundaryloss.utils import simplex\n",
    "from boundaryloss.losses import *\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "# Set precision for matmul operations and print MONAI config\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "print_config()\n",
    "\n",
    "# Uncomment below line to ignore warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each segment\n",
    "def convert_cases_to_remove_txt(text):\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    case_dict = {}\n",
    "    current_case = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        if line.startswith('/'):\n",
    "            if current_case:\n",
    "                case_dict[current_case].append(\"Dataset-arrays-4/\"+ \"/\".join(line.split('/')[-3:]))\n",
    "        else:\n",
    "            current_case = line.strip()\n",
    "            case_dict[current_case] = []\n",
    "\n",
    "    case_dict = {k: v for k, v in case_dict.items() if v}\n",
    "    \n",
    "    return case_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_to_remove_txt = \"\"\"BNB1172(DF)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BNB1172(DF)/images/BNB1172(DF)_130.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BNB1172(DF)/images/BNB1172(DF)_131.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BNB1172(DF)/images/BNB1172(DF)_196.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BNB1172(DF)/images/BNB1172(DF)_197.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BNB1172(DF)/images/BNB1172(DF)_198.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BNB1172(DF)/images/BNB1172(DF)_199.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BNB1172(DF)/images/BNB1172(DF)_200.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BNB1172(DF)/images/BNB1172(DF)_201.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BNB1172(DF)/images/BNB1172(DF)_202.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BNB1172(DF)/images/BNB1172(DF)_203.npy\n",
    "\n",
    "D1AP5(VR)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/ D1AP5(VR)/images/D1AP5(VR)_91.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/ D1AP5(VR)/images/D1AP5(VR)_92.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/ D1AP5(VR)/images/D1AP5(VR)_97.npy\n",
    "\n",
    "D1AP7(VR)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D1AP7(VR)/images/D1AP7(VR)_56.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D1AP7(VR)/images/D1AP7(VR)_78.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D1AP7(VR)/images/D1AP7(VR)_79.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D1AP7(VR)/images/D1AP7(VR)_80.npy\n",
    "\n",
    "D1AP12(VR)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D1AP12(VR)/images/D1AP12(VR)_21.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D1AP12(VR)/images/D1AP12(VR)_22.npy\n",
    "\n",
    "D2MP1(VR)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP1(VR)/images/D2MP1(VR)_47.npy\n",
    "BRUTTINO\n",
    "\n",
    "D2MP3(VR)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP3(VR)/images/D2MP3(VR)_133.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP3(VR)/images/D2MP3(VR)_146.npy\n",
    "C’è massa non segmentata, probabilmente benigna ma chiarire\n",
    "\n",
    "D2MP4(VR)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP4(VR)/images/D2MP4(VR)_60.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP4(VR)/images/D2MP4(VR)_61.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP4(VR)/images/D2MP4(VR)_84.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP4(VR)/images/D2MP4(VR)_86.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP4(VR)/images/D2MP4(VR)_128.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP4(VR)/images/D2MP4(VR)_129.npy\n",
    "\n",
    "D2MP6(VR)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP6(VR)/images/D2MP6(VR)_24.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP6(VR)/images/D2MP6(VR)_31.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP6(VR)/images/D2MP6(VR)_35.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP6(VR)/images/D2MP6(VR)_52.npy\n",
    "\n",
    "D3MP7 (VR)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_38.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_39.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_40.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_41.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_42.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_64.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_65.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_66.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_67.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_68.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_69.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_70.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_71.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D3MP7 (VR)/images/D3MP7 (VR)_72.npy\n",
    "\n",
    "DFC1168(DF)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DFC1168(DF)/images/DFC1168(DF)_173.npy\n",
    "\n",
    "DTM0772(1,5)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DTM0772(1,5)/images/DTM0772(1,5)_21.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DTM0772(1,5)/images/DTM0772(1,5)_47.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DTM0772(1,5)/images/DTM0772(1,5)_50.npy\n",
    "\n",
    "GMG0961(3)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GMG0961(3)/images/GMG0961(3)_71.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GMG0961(3)/images/GMG0961(3)_81.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GMG0961(3)/images/GMG0961(3)_82.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GMG0961(3)/images/GMG0961(3)_83.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GMG0961(3)/images/GMG0961(3)_87.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GMG0961(3)/images/GMG0961(3)_101.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GMG0961(3)/images/GMG0961(3)_102.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GMG0961(3)/images/GMG0961(3)_104.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GMG0961(3)/images/GMG0961(3)_109.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GMG0961(3)/images/GMG0961(3)_110.npy\n",
    "LGM0159(1,5)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_41.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_42.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_43.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_44.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_45.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_46.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_47.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_48.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_49.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_70.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LGM0159(1,5)/images/LGM0159(1,5)_71.npy\n",
    "Bruttino\n",
    "\n",
    "MD0773(DF)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MD0773(DF)/images/MD0773(DF)_115.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MD0773(DF)/images/MD0773(DF)_116.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MD0773(DF)/images/MD0773(DF)_153.npy\n",
    "\n",
    "PF0473(1,5)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PF0473(1,5)/images/PF0473(1,5)_50.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PF0473(1,5)/images/PF0473(1,5)_51.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PF0473(1,5)/images/PF0473(1,5)_52.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PF0473(1,5)/images/PF0473(1,5)_59.npy\n",
    "Brutto\n",
    "\n",
    "PMG0761(3)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_73.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_85.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_89.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_90.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_91.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_92.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_93.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_93.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_95.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_96.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_97.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_98.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_99.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_112.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_113.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_124.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_125.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_126.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_133.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_134.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_135.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_136.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_137.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_145.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_146.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_147.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_148.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_149.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_150.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_151.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_152.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_153.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PMG0761(3)/images/PMG0761(3)_154.npy\n",
    "Bruttissimo\n",
    "\n",
    "RD0175\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/RD0175/images/RD0175_82.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/RD0175/images/RD0175_83.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/RD0175/images/RD0175_123.npy\n",
    "\n",
    "SM0972(DF)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_104.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_126.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_127.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_128.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_129.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_135.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_142.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_143.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_144.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_145.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_146.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_147.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_148.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_149.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_150.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_151.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_152.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_153.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_154.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_155.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_156.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_157.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_158.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_159.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_160.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_161.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_162.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_163.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM0972(DF)/images/SM0972(DF)_164.npy\n",
    "Usare in validation\n",
    "\n",
    "UFR0987\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/UFR0987/images/UFR0987_71.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/UFR0987/images/UFR0987_72.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/UFR0987/images/UFR0987_73.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/UFR0987/images/UFR0987_89.npy\n",
    "\n",
    "ZT0279(3)\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/ZT0279(3)/images/ZT0279(3)_137.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/ZT0279(3)/images/ZT0279(3)_138.npy\n",
    "\n",
    "BC1179B\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BC1179B-merged/images/BC1179B_57.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BC1179B-merged/images/BC1179B_71.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BC1179B-merged/images/BC1179B_96.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BC1179B-merged/images/BC1179B_97.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BC1179B-merged/images/BC1179B_115.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BC1179B-merged/images/BC1179B_116.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/BC1179B-merged/images/BC1179B_117.npy\n",
    "\n",
    "D2MP9b(VR)-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP9b(VR) merged/images/D2MP9b(VR)_35.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP9b(VR) merged/images/D2MP9b(VR)_36.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP9b(VR) merged/images/D2MP9b(VR)_37.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP9b(VR)-merged/images/D2MP9b(VR)_45.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP9b(VR)-merged/images/D2MP9b(VR)_63.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/D2MP9b(VR)-merged/images/D2MP9b(VR)_69.npy\n",
    "\n",
    "GA07(DF)B-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GA07(DF)B-merged/images/GA07(DF)B_62.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GA07(DF)B-merged/images/GA07(DF)B_83.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GA07(DF)B-merged/images/GA07(DF)B_104.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GA07(DF)B-merged/images/GA07(DF)B_105.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GA07(DF)B-merged/images/GA07(DF)B_106.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/GA07(DF)B-merged/images/GA07(DF)B_119.npy\n",
    "\n",
    "DCC0340(1,5)-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_27.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_28.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_29.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_30.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_31.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_32.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_33.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_46.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_47.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_48.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_49.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_50.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_51.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_55.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_56.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_57.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/DCC0340(1,5)-merged/images/DCC0340(1,5)_58.npy\n",
    "\n",
    "HV1263(1,5)-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_5.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_6.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_7.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_17.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_18.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_19.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_20.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_21.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_22.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_23.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_24.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_21.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_35.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/HV1263(1,5)-merged/images/HV1263(1,5)_36.npy\n",
    "\n",
    "LA0248B-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LA0248B-merged/images/LA0248B_115.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LA0248B-merged/images/LA0248B_120.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LA0248B-merged/images/LA0248B_128.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/LA0248B-merged/images/LA0248B_146.npy\n",
    "\n",
    "PE0468(1,5)B-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_17.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_18.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_19.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_28.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_29.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_30.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_38.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_48.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_49.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_50.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PE0468(1,5)B-merged/images/PE0468(1,5)B_51.npy\n",
    "\n",
    "MV1276(1,5)B-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MV1276(1,5)B-merged/images/MV1276(1,5)B_41.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MV1276(1,5)B-merged/images/MV1276(1,5)B_42.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MV1276(1,5)B-merged/images/MV1276(1,5)B_43.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MV1276(1,5)B-merged/images/MV1276(1,5)B_44.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MV1276(1,5)B-merged/images/MV1276(1,5)B_45.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MV1276(1,5)B-merged/images/MV1276(1,5)B_46.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MV1276(1,5)B-merged/images/MV1276(1,5)B_58.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MV1276(1,5)B-merged/images/MV1276(1,5)B_59.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/MV1276(1,5)B-merged/images/MV1276(1,5)B_60.npy\n",
    "\n",
    "VS0976(1,5)B-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_20.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_21.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_22.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_27.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_28.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_29.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_36.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_37.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_43.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_44.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_45.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_46.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_47.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_55.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_58.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VS0976(1,5)B-merged/images/VS0976(1,5)B_59.npy\n",
    "BRUTTO\n",
    "\n",
    "VDMB0751(DF)B-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_74.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_75.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_78.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_85.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_86.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_87.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_88.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_89.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_90.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_91.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_92.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_93.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_94.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_95.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_96.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_97.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_98.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_99.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_100.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_101.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_102.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_103.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_104.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_105.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_106.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_107.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_108.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_109.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_110.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_111.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_112.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_130.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_136.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_146.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_147.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_148.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_149.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_152.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/VDMB0751(DF)B-merged/images/VDMB0751(DF)B_157.npy\n",
    "VALIDATION\n",
    "\n",
    "SM1232B-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM1232B-merged/images/SM1232B_84.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM1232B-merged/images/SM1232B_85.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM1232B-merged/images/SM1232B_86.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/SM1232B-merged/images/SM1232B_138.npy\n",
    "\n",
    "PS0446(1.5)B-merged\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PS0446(1.5)B-merged/images/PS0446(1.5)B_28.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PS0446(1.5)B-merged/images/PS0446(1.5)B_29.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PS0446(1.5)B-merged/images/PS0446(1.5)B_34.npy\n",
    "/content/drive/MyDrive/Tesi/Dataset-arrays/PS0446(1.5)B-merged/images/PS0446(1.5)B_40.npy\"\"\"\n",
    "to_remove_dict = convert_cases_to_remove_txt(cases_to_remove_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpablo-giaccaglia\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Seed set to 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed 200...\n"
     ]
    }
   ],
   "source": [
    "checkpoints_dir=\"checkpoints\"\n",
    "wandb.login(key = \"2bc18e4744fb0771a16fd009b7aa2c98c79efc49\")\n",
    "\n",
    "\n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 1 - train_ratio\n",
    "SEED = 200\n",
    "n_cpu = os.cpu_count()\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = 200\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def reseed():\n",
    "    SEED = 200\n",
    "    print(f'Using random seed {SEED}...')\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "\n",
    "    seed_everything(SEED, workers=True)\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "g = reseed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_base_path = \"Dataset-arrays-4-FINAL\"\n",
    "all_folders = os.listdir(dataset_base_path)\n",
    "len(all_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YjFOR1ng1HT"
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.measure import label as LABEL, regionprops\n",
    "from scipy.spatial.distance import cdist\n",
    "import ttach as tta\n",
    "from ttach.base import Merger\n",
    "\n",
    "from monai.transforms import KeepLargestConnectedComponent, RemoveSmallObjects\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import label as labell, generate_binary_structure\n",
    "\n",
    "def compute_iou_imagewise_from_cumulator(TPs, FPs, FNs, TNs, exclude_empty=False, exclude_empty_only_gt = False,return_std=False):\n",
    "\n",
    "    tp = torch.cat([tp for tp in TPs])\n",
    "    fp = torch.cat([fp for fp in FPs])\n",
    "    fn = torch.cat([fn for fn in FNs])\n",
    "    tn = torch.cat([tn for tn in TNs])\n",
    "\n",
    "    if return_std:\n",
    "\n",
    "        mean_iou, std_iou = compute_iou_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise',exclude_empty=exclude_empty, exclude_empty_only_gt =exclude_empty_only_gt, return_std=return_std)\n",
    "        return mean_iou.item(), std_iou.item()\n",
    "\n",
    "    else:\n",
    "\n",
    "        return compute_iou_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise',exclude_empty=exclude_empty).item()\n",
    "\n",
    "def compute_dice_imagewise_from_cumulator(TPs, FPs, FNs, TNs, exclude_empty=False, exclude_empty_only_gt = False, return_std=False):\n",
    "\n",
    "    tp = torch.cat([tp for tp in TPs])\n",
    "    fp = torch.cat([fp for fp in FPs])\n",
    "    fn = torch.cat([fn for fn in FNs])\n",
    "    tn = torch.cat([tn for tn in TNs])\n",
    "\n",
    "    if return_std:\n",
    "        mean_dice, std_dice =  compute_dice_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise',exclude_empty=exclude_empty, exclude_empty_only_gt=exclude_empty_only_gt, return_std=return_std)\n",
    "        return mean_dice.item(), std_dice.item()\n",
    "\n",
    "    else:\n",
    "\n",
    "        return compute_dice_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise',exclude_empty=exclude_empty).item()\n",
    "\n",
    "\n",
    "def compute_mean_iou_imagewise_from_cumulator(TPs, FPs, FNs, TNs, exclude_empty=False, return_std=False, reduce_mean=True):\n",
    "    # Concatenate tensors for each metric\n",
    "\n",
    "    try:\n",
    "        tp = torch.cat([tp for tp in TPs])\n",
    "        fp = torch.cat([fp for fp in FPs])\n",
    "        fn = torch.cat([fn for fn in FNs])\n",
    "        tn = torch.cat([tn for tn in TNs])\n",
    "    except:\n",
    "        tp = TPs\n",
    "        fp = FPs\n",
    "        fn = FNs\n",
    "        tn = TNs\n",
    "\n",
    "    if exclude_empty:\n",
    "        # Calculate IOU per image excluding empty cases\n",
    "        iou1_per_image_no_empty = compute_iou_from_metrics(tp, fp, tn, fn, reduction='none', exclude_empty=True)\n",
    "        iou0_per_image_no_empty = compute_iou_from_metrics(tn, fn, tp, fp, reduction='none', exclude_empty=True)\n",
    "        \n",
    "        # Combine and filter valid IOU scores\n",
    "        combined_iou_scores = np.hstack((iou0_per_image_no_empty, iou1_per_image_no_empty))\n",
    "        valid_pairs = ~np.isnan(combined_iou_scores).any(axis=1)\n",
    "        \n",
    "        # Compute mean and optionally standard deviation\n",
    "        mean_iou_per_image_no_empty = np.nanmean(combined_iou_scores[valid_pairs], axis=1)\n",
    "\n",
    "        if not reduce_mean:\n",
    "            return mean_iou_per_image_no_empty\n",
    "        if return_std:\n",
    "            std_iou_per_image_no_empty = np.nanstd(mean_iou_per_image_no_empty)\n",
    "            return np.mean(mean_iou_per_image_no_empty), std_iou_per_image_no_empty\n",
    "        else:\n",
    "            return np.mean(mean_iou_per_image_no_empty)\n",
    "\n",
    "    else:\n",
    "        # Calculate IOU per image including empty cases\n",
    "        iou1_per_image = compute_iou_from_metrics(tp, fp, tn, fn, reduction='none')\n",
    "        iou0_per_image = compute_iou_from_metrics(tn, fn, tp, fp, reduction='none')\n",
    "        \n",
    "        # Compute mean and optionally standard deviation\n",
    "        combined_iou_scores = np.array([iou0_per_image.cpu().numpy(), iou1_per_image.cpu().numpy()])\n",
    "        mean_iou_per_image = np.nanmean(combined_iou_scores, axis=0)\n",
    "        \n",
    "        if not reduce_mean:\n",
    "            return mean_iou_per_image\n",
    "        if return_std:\n",
    "            std_iou_per_image = np.nanstd(mean_iou_per_image)\n",
    "            return np.mean(mean_iou_per_image), std_iou_per_image\n",
    "        else:\n",
    "            return np.mean(mean_iou_per_image)\n",
    "\n",
    "def compute_mean_dice_imagewise_from_cumulator(TPs, FPs, FNs, TNs, exclude_empty=False, return_std=False, reduce_mean=True):\n",
    "    try:\n",
    "        tp = torch.cat([tp for tp in TPs])\n",
    "        fp = torch.cat([fp for fp in FPs])\n",
    "        fn = torch.cat([fn for fn in FNs])\n",
    "        tn = torch.cat([tn for tn in TNs])\n",
    "    except:\n",
    "        tp = TPs\n",
    "        fp = FPs\n",
    "        fn = FNs\n",
    "        tn = TNs\n",
    "\n",
    "    if exclude_empty:\n",
    "        dice1_per_image_no_empty = compute_dice_from_metrics(tp, fp, tn, fn, reduction='none', exclude_empty=True)\n",
    "        dice0_per_image_no_empty = compute_dice_from_metrics(tn, fn, tp, fp, reduction='none', exclude_empty=True)\n",
    "        combined_dice_scores = np.hstack((dice0_per_image_no_empty, dice1_per_image_no_empty))\n",
    "        valid_pairs = ~np.isnan(combined_dice_scores).any(axis=1)\n",
    "        mean_dice_per_image_no_empty = np.nanmean(combined_dice_scores[valid_pairs], axis=1)\n",
    "        if not reduce_mean:\n",
    "            return mean_dice_per_image_no_empty\n",
    "        if return_std:\n",
    "            std_dice_per_image_no_empty = np.std(np.nanmean(combined_dice_scores[valid_pairs], axis=1))\n",
    "            return np.mean(mean_dice_per_image_no_empty), std_dice_per_image_no_empty\n",
    "        else:\n",
    "            return np.mean(mean_dice_per_image_no_empty)\n",
    "    else:\n",
    "        dice1_per_image = compute_dice_from_metrics(tp, fp, tn, fn, reduction='none')\n",
    "        dice0_per_image = compute_dice_from_metrics(tn, fn, tp, fp, reduction='none')\n",
    "        combined_dice_scores = np.array([dice0_per_image.cpu().numpy(), dice1_per_image.cpu().numpy()])\n",
    "        mean_dice_per_image = np.nanmean(combined_dice_scores, axis=0)\n",
    "        if not reduce_mean:\n",
    "            return mean_dice_per_image\n",
    "        if return_std:\n",
    "            std_dice_per_image = np.std(np.nanmean(combined_dice_scores, axis=0))\n",
    "            return np.mean(mean_dice_per_image), std_dice_per_image\n",
    "        else:\n",
    "            return np.mean(mean_dice_per_image)\n",
    "\n",
    "def plot_slices_side_by_side(volume1, volume2):\n",
    "    \"\"\"\n",
    "    Plot corresponding slices from two CxHxW volumes side by side.\n",
    "    \n",
    "    :param volume1: First volume with shape CxHxW.\n",
    "    :param volume2: Second volume with shape CxHxW.\n",
    "    \"\"\"\n",
    "    H, W, B = volume1.shape  # Assuming volume1 and volume2 have the same shape\n",
    "    \n",
    "    # Set up the figure size dynamically based on the number of slices\n",
    "    plt.figure(figsize=(10, 2 * B))\n",
    "    \n",
    "    for b in range(B):\n",
    "        # Plot slice from volume 1\n",
    "        plt.subplot(B, 2, 2*b + 1)  # Rows, Columns, Index\n",
    "        plt.imshow(volume1[:, :,b], cmap='gray')\n",
    "        plt.title(f'Slice {b + 1} - Volume 1')\n",
    "        plt.axis('off')  # Hide axes ticks\n",
    "        \n",
    "        # Plot corresponding slice from volume 2\n",
    "        plt.subplot(B, 2, 2*b + 2)  # Rows, Columns, Index\n",
    "        plt.imshow(volume2[:, :,b], cmap='gray')\n",
    "        plt.title(f'Slice {b + 1} - Volume 2')\n",
    "        plt.axis('off')  # Hide axes ticks\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_local_agreement(prob1, prob2, kernel_size=3, agreement_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Calculate local agreement between two probability masks using average pooling to simulate\n",
    "    the surrounding window effect.\n",
    "    \"\"\"\n",
    "    # Calculate absolute difference and apply threshold\n",
    "    diff = torch.abs(prob1 - prob2)\n",
    "    local_diff = F.avg_pool2d(diff.unsqueeze(0), kernel_size, stride=1, padding=kernel_size//2).squeeze(0)\n",
    "    local_agreement = local_diff < agreement_threshold\n",
    "    return local_agreement\n",
    "\n",
    "def fill_gaps_in_masses(binary_mask, gap_filling_kernel_size=5):\n",
    "    \"\"\"\n",
    "    Fills gaps in segmented masses using morphological closing.\n",
    "    \n",
    "    Parameters:\n",
    "    - binary_mask: numpy.ndarray, the binary segmentation mask with masses.\n",
    "    - gap_filling_kernel_size: int, the size of the square kernel used for gap filling.\n",
    "    \n",
    "    Returns:\n",
    "    - gap_filled_mask: numpy.ndarray, the mask after filling gaps.\n",
    "    \"\"\"\n",
    "\n",
    "    print(np.unique(binary_mask))\n",
    "    # Define the square kernel based on the specified size\n",
    "    kernel_gap_filling = np.ones((gap_filling_kernel_size, gap_filling_kernel_size), np.uint8)\n",
    "    \n",
    "    # Perform the morphological closing operation\n",
    "    gap_filled_mask = cv2.morphologyEx(binary_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel_gap_filling)\n",
    "    \n",
    "    return gap_filled_mask\n",
    "\n",
    "\n",
    "def perform_dilation(image, dilation_size=3):\n",
    "    \"\"\"\n",
    "    Perform dilation on an image using OpenCV.\n",
    "    \n",
    "    :param image: Input binary image with objects to dilate.\n",
    "    :param dilation_size: Determines the size of the dilation kernel. Default is 3.\n",
    "    :return: Image after dilation.\n",
    "    \"\"\"\n",
    "\n",
    "    image = (image > 0).astype(np.uint8)\n",
    "    # Create a square structuring element for dilation\n",
    "    kernel = np.ones((dilation_size, dilation_size), np.uint8)\n",
    "    \n",
    "    # Perform dilation\n",
    "    dilated_image = cv2.dilate(image, kernel, iterations=1)\n",
    "    \n",
    "    return dilated_image\n",
    "\n",
    "\n",
    "def remove_far_masses_based_on_largest_mass(batch_masks, distance_threshold):\n",
    "    \"\"\"\n",
    "    Remove masses in each mask of the batch that are far from the largest mass.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_masks: numpy array of shape BCHW.\n",
    "    - distance_threshold: distance beyond which a mass is considered far.\n",
    "\n",
    "    Returns:\n",
    "    - processed_masks: numpy array of shape BCHW with far masses removed.\n",
    "    \"\"\"\n",
    "    processed_masks = np.zeros_like(batch_masks)\n",
    "\n",
    "    for i, mask in enumerate(batch_masks):\n",
    "        # Ensure the mask is 2D\n",
    "        if mask.ndim == 3:  # BCHW where C=1\n",
    "            mask_2d = mask[0]\n",
    "        elif mask.ndim == 2:  # HW\n",
    "            mask_2d = mask\n",
    "        else:\n",
    "            raise ValueError(\"Mask dimension is not correct. Expected 2D or 3D with single channel.\")\n",
    "\n",
    "        # Label the connected components with 2D connectivity\n",
    "        labeled_mask = LABEL(mask_2d.cpu(), connectivity=1)\n",
    "\n",
    "        # Calculate properties of each component\n",
    "        regions = regionprops(labeled_mask)\n",
    "\n",
    "        if not regions:\n",
    "            continue\n",
    "\n",
    "        # Find the largest mass\n",
    "        largest_mass = max(regions, key=lambda x: x.area)\n",
    "\n",
    "        # Get the centroid of the largest mass\n",
    "        largest_mass_centroid = largest_mass.centroid\n",
    "\n",
    "        # Identify and keep components close to the centroid of the largest mass\n",
    "        for region in regions:\n",
    "            centroid = region.centroid\n",
    "            distance = np.sqrt((centroid[0] - largest_mass_centroid[0]) ** 2 + (centroid[1] - largest_mass_centroid[1]) ** 2)\n",
    "            if distance < distance_threshold:\n",
    "                processed_masks[i, 0, region.coords[:,0], region.coords[:,1]] = 1\n",
    "\n",
    "    return processed_masks\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        print(f\"Dictionary successfully saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving dictionary to file: {e}\")\n",
    "\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load and return the content of a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def get_filenames(suffix, base_path, patient_ids, remove_black_samples=False, get_random_samples_and_remove_black_samples=False, get_top_bottom_and_remove_black_samples=False,random_samples_indexes_list=None,\n",
    "                 remove_picked_samples=False):\n",
    "    filenames = []\n",
    "\n",
    "    create_random_samples_index_list = False\n",
    "\n",
    "    if random_samples_indexes_list is None:\n",
    "        random_samples_indexes_list = []\n",
    "        create_random_samples_index_list=True\n",
    "\n",
    "    for idx, patient_id in enumerate(patient_ids):\n",
    "        path = os.path.join(base_path, patient_id) + \"/\" + suffix + \"/\"\n",
    "        files = [os.path.join(path, p) for p in natsorted(os.listdir(path), alg=ns.IGNORECASE)]\n",
    "\n",
    "        if get_random_samples_and_remove_black_samples:\n",
    "              files_sampled = filter_samples_sample_aware(files, patient_id)\n",
    "              if remove_picked_samples:\n",
    "                  files_sampled = filter_samples_to_exclude(files_sampled, patient_id)\n",
    "              filenames += files_sampled\n",
    "              size = int(len(files_sampled)*0.25)\n",
    "\n",
    "\n",
    "              random_samples_indexes = None if create_random_samples_index_list else random_samples_indexes_list[idx]\n",
    "              files_random, random_samples_indexes = get_samples_size(files=files, patient_id=patient_id, size=size, random_samples=True, random_samples_indexes=random_samples_indexes)\n",
    "\n",
    "              if create_random_samples_index_list:\n",
    "                  random_samples_indexes_list.append(random_samples_indexes)\n",
    "\n",
    "              filenames += files_random\n",
    "\n",
    "        elif remove_black_samples:\n",
    "              files_sampled = filter_samples_sample_aware(files, patient_id)\n",
    "              if remove_picked_samples:\n",
    "                  files_sampled = filter_samples_to_exclude(files_sampled, patient_id)\n",
    "                \n",
    "              filenames += files_sampled\n",
    "\n",
    "        elif get_top_bottom_and_remove_black_samples:\n",
    "              files_sampled = filter_samples_sample_aware(files, patient_id)\n",
    "              if remove_picked_samples:\n",
    "                  files_sampled = filter_samples_to_exclude(files_sampled, patient_id)\n",
    "              filenames += files_sampled\n",
    "\n",
    "              size = int(len(files_sampled)*0.25)\n",
    "              files_top_bottom = get_samples_size(files=files, patient_id=patient_id, size=size, random_samples=False)\n",
    "              filenames += files_top_bottom\n",
    "\n",
    "        else:\n",
    "              filenames += files\n",
    "\n",
    "    if get_random_samples_and_remove_black_samples:\n",
    "      return filenames, random_samples_indexes_list\n",
    "    else:\n",
    "      return filenames, None\n",
    "\n",
    "def get_samples_size(files, patient_id, size=None, random_samples=False, random_samples_indexes=None):\n",
    "\n",
    "    top_slices_len = d[patient_id]['start']\n",
    "    top_slices = files[:top_slices_len]\n",
    "    sample_size_top_slices = size\n",
    "\n",
    "    bottom_slices_len = len(files) - d[patient_id]['end']\n",
    "    bottom_slices = files[d[patient_id]['end']:]\n",
    "    sample_size_bottom_slices = size\n",
    "\n",
    "    if random_samples:\n",
    "        if random_samples_indexes:\n",
    "          subset_top_slices_random_indexes = random_samples_indexes[0]\n",
    "        else:\n",
    "          if sample_size_top_slices > len(top_slices):\n",
    "            sample_size_top_slices=len(top_slices)\n",
    "\n",
    "          subset_top_slices_random_indexes = random.sample(range(len(top_slices)), sample_size_top_slices)\n",
    "\n",
    "        subset_top_slices = [top_slices[i] for i in subset_top_slices_random_indexes]\n",
    "\n",
    "        if random_samples_indexes:\n",
    "              subset_bottom_slices_random_indexes = random_samples_indexes[1]\n",
    "        else:\n",
    "          if sample_size_bottom_slices > len(bottom_slices):\n",
    "            sample_size_bottom_slices=len(bottom_slices)\n",
    "\n",
    "          subset_bottom_slices_random_indexes = random.sample(range(len(bottom_slices)), sample_size_bottom_slices)\n",
    "            \n",
    "        subset_bottom_slices=  [bottom_slices[i] for i in subset_bottom_slices_random_indexes]\n",
    "\n",
    "        files_to_return = subset_top_slices + subset_bottom_slices\n",
    "        return files_to_return, [subset_top_slices_random_indexes, subset_bottom_slices_random_indexes]\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        if sample_size_top_slices > len(top_slices):\n",
    "            sample_size_top_slices=len(top_slices)\n",
    "        if sample_size_bottom_slices > len(bottom_slices):\n",
    "            sample_size_bottom_slices=len(bottom_slices)\n",
    "\n",
    "        subset_top_slices = top_slices[-sample_size_top_slices:]\n",
    "        subset_bottom_slices = bottom_slices[:sample_size_bottom_slices]\n",
    "        files_to_return = subset_top_slices + subset_bottom_slices\n",
    "        return files_to_return\n",
    "\n",
    "\n",
    "\n",
    "def filter_samples_sample_aware(files, patient_id):\n",
    "    start, end = d[patient_id]['start'], d[patient_id]['end']\n",
    "    return files[start+1:end]\n",
    "\n",
    "def filter_samples_to_exclude(files, patient_id):\n",
    "    filtered_list = []\n",
    "    if patient_id not in to_remove_dict:\n",
    "        return files\n",
    "    files_to_exclude = to_remove_dict[patient_id]\n",
    "\n",
    "    filtered_list = []\n",
    "    for file in files:\n",
    "        file_clean = file.replace(\"mask_\",\"\")\n",
    "        file_clean = file_clean.replace(\"masks\",\"images\")\n",
    "        \n",
    "        if file_clean not in files_to_exclude:\n",
    "            filtered_list.append(file)\n",
    "\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "def reconstruct_label(label, original_shape, thorax_crop_coords, bottom_crop_coords, crop_coords, resize_dims, trim_breast_coords):\n",
    "    # First, resize the label back to the size before the final crop\n",
    "    label_resized = F.interpolate(label.unsqueeze(0), size=resize_dims, mode='nearest-exact').squeeze(0)\n",
    "\n",
    "    # Now, we need to reverse the crop operations in the correct order\n",
    "    # Start with the most recent crop and work backward to the original state\n",
    "\n",
    "    # Reverse the final crop to get to the state before bottom crop\n",
    "    y1, y2, x1, x2 = crop_coords\n",
    "    crop_reversed_label = torch.zeros((original_shape[0], y2-y1, x2-x1), dtype=label.dtype)\n",
    "    crop_reversed_label = label_resized\n",
    "\n",
    "    # Reverse the bottom crop to get to the state before breast trim\n",
    "    x1, y1, x2, y2 = bottom_crop_coords\n",
    "    bottom_crop_reversed_label = torch.zeros((original_shape[0], original_shape[1], crop_reversed_label.shape[2]), dtype=label.dtype)\n",
    "    bottom_crop_reversed_label[:, y1:y2, :] = crop_reversed_label\n",
    "\n",
    "    # Reverse the breast trim to get to the state before thorax crop\n",
    "    start, end = trim_breast_coords\n",
    "    breast_trim_reversed_label = torch.zeros((original_shape[0], original_shape[1], original_shape[2]), dtype=label.dtype)\n",
    "    breast_trim_reversed_label[:, :, start:end] = bottom_crop_reversed_label\n",
    "\n",
    "    # Reverse the thorax crop to get to the original state\n",
    "    x1, y1, x2, y2 = thorax_crop_coords\n",
    "    thorax_crop_reversed_label = torch.zeros(original_shape, dtype=label.dtype)\n",
    "    thorax_crop_reversed_label[:, y1:y2, :] = breast_trim_reversed_label\n",
    "\n",
    "    return thorax_crop_reversed_label\n",
    "\n",
    "\n",
    "def filter_fn(image, max_ratio):\n",
    "    c, h, w = image.shape\n",
    "\n",
    "    if h >= w:\n",
    "        if w == 0 or max_ratio < h/w:\n",
    "          return False\n",
    "    elif w >= h:\n",
    "        if h==0 or max_ratio < w/h:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def train_custom_collate(batch):\n",
    "    # Filter out None samples\n",
    "    augmentations = Compose([monai.transforms.RandHistogramShiftd(keys=['image'], prob=0.2, num_control_points=4), \n",
    "                                      monai.transforms.RandRotated(keys=['image', 'label'],mode='nearest-exact', range_x=[0.1, 0.1], prob=0.3),\n",
    "                                      monai.transforms.RandZoomd(keys=['image', 'label'],mode='nearest-exact', min_zoom = 1.3, max_zoom = 1.5, prob=0.3),\n",
    "                                      #monai.transforms.RandCoarseDropoutd(keys=['image', 'label'], prob=0.3, holes=20, spatial_size=20, fill_value =0)\n",
    "                                     ]\n",
    "        \n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "    # Filter out None samples\n",
    "    batch = [augmentations({\n",
    "                'image' : copy.deepcopy(item['image']),\n",
    "                'label' : copy.deepcopy(item['label']),\n",
    "                'boundary': copy.deepcopy(item[\"boundary\"])\n",
    "            }) for sublist in batch for item in sublist if item['keep_sample']]\n",
    "    \n",
    "    if len(batch)>0:\n",
    "        batch = default_collate(batch)\n",
    "        return batch\n",
    "    return None\n",
    "\n",
    "def train_custom_collate_no_patches(batch):\n",
    "\n",
    "    augmentations = Compose([monai.transforms.RandHistogramShiftd(keys=['image'], prob=0.2, num_control_points=4), \n",
    "                                      monai.transforms.RandRotated(keys=['image', 'label'],mode='nearest-exact', range_x=[0.1, 0.1], prob=0.3),\n",
    "                                      monai.transforms.RandZoomd(keys=['image', 'label'],mode='nearest-exact', min_zoom = 1.3, max_zoom = 1.5, prob=0.3),\n",
    "                                      #monai.transforms.RandCoarseDropoutd(keys=['image', 'label'], prob=0.3, holes=20, spatial_size=20, fill_value =0)\n",
    "                                     ]\n",
    "        \n",
    "        )\n",
    "\n",
    "    # Filter out None samples\n",
    "    batch = [augmentations({\n",
    "                'image' : copy.deepcopy(item['image']),\n",
    "                'label' : copy.deepcopy(item['label']),\n",
    "                'boundary': copy.deepcopy(item[\"boundary\"])\n",
    "            }) for item in batch if item['keep_sample']]\n",
    "\n",
    "    \n",
    "    if len(batch)>0:\n",
    "        batch = default_collate(batch)\n",
    "        return batch\n",
    "    return None\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "\n",
    "    \n",
    "    batch = [item for sublist in batch for item in sublist if item['keep_sample']]\n",
    "\n",
    "    \n",
    "    if len(batch)>0:\n",
    "        batch = default_collate(batch)\n",
    "        return batch\n",
    "    return None\n",
    "\n",
    "def  custom_collate_no_patches(batch):\n",
    "\n",
    "    # Filter out None samples\n",
    "    batch = [item for item in batch if item['keep_sample']]\n",
    "    \n",
    "    if len(batch)>0:\n",
    "        batch = default_collate(batch)\n",
    "        return batch\n",
    "    return None\n",
    "    \n",
    "def reverse_transformations(d, processed_label, mode='patches'):\n",
    "    # Extract the processed label and transformation coordinates\n",
    "    y1_crop, y2_crop, x1_crop, x2_crop = d['crop_coords']\n",
    "    x1_bottom, y1_bottom, x2_bottom, y2_bottom = d['bottom_crop_coords']\n",
    "\n",
    "    if mode=='patches':\n",
    "        start_breast, end_breast = d['trim_breast_coords']\n",
    "    x1_thorax, y1_thorax, x2_thorax, y2_thorax = d['thorax_crop_coords']\n",
    "    intermediate_spatial_dim = d['dim_before_resize_final']\n",
    "\n",
    "    # Step 0: Resize to the original spatial dimensions before the final crop\n",
    "    # Assuming 'resize' was a downscaling operation and the original_spatial_dim is the target size\n",
    "    label_resized = F.interpolate(processed_label.unsqueeze(0).unsqueeze(0).float(),\n",
    "                                  size=intermediate_spatial_dim.tolist(),  # Excluding the batch size dimension\n",
    "                                  mode='nearest-exact').squeeze(0).squeeze(0)  # Removing the added batch and channel dimensions\n",
    "\n",
    "    \n",
    "    \n",
    "    pad_post_crop_coords = d['pad_post_crop_coords'].tolist()\n",
    "    before1, before2, before3 = pad_post_crop_coords[0],pad_post_crop_coords[1],pad_post_crop_coords[2]\n",
    "\n",
    "    original_height = label_resized.shape[1] - before2[1]\n",
    "    original_width = label_resized.shape[2] - before3[1]\n",
    "\n",
    "    # Slice the image to remove the padding\n",
    "    reversed_pad = label_resized[:, :original_height, :original_width]\n",
    "\n",
    "    # Step 1: Reverse final crop\n",
    "    crop_height, crop_width = d['dim_before_crop'][1:]\n",
    "    padded_label = torch.zeros((1, crop_height, crop_width), dtype=label_resized.dtype)\n",
    "    padded_label[:, y1_crop:y2_crop, x1_crop:x2_crop] = reversed_pad\n",
    "\n",
    "    # Step 2: Reverse bottom crop\n",
    "    bottom_height = d['dim_before_bottom_crop'][1]\n",
    "    bottom_width = d['dim_before_bottom_crop'][2]\n",
    "    bottom_padded_label = torch.zeros((1, bottom_height, x2_bottom), dtype=padded_label.dtype)\n",
    "    bottom_padded_label[:, :y2_bottom, :x2_bottom] = padded_label\n",
    "\n",
    "    # Conditional steps based on whether breast trim was applied\n",
    "    if mode == 'patches':\n",
    "        # Step 3: Reverse breast trim\n",
    "        trim_width = d['dim_before_breast_crop'][2]\n",
    "        trim_padded_label = torch.zeros((bottom_height, trim_width), dtype=bottom_padded_label.dtype)\n",
    "        trim_padded_label[:, start_breast:end_breast] = bottom_padded_label\n",
    "    else:\n",
    "        # In other modes, use the bottom_padded_label directly for thorax crop reversal\n",
    "        trim_padded_label = bottom_padded_label\n",
    "        trim_width = bottom_width  # This assumes no trimming, hence the width is unchanged\n",
    "\n",
    "    \n",
    "    # Step 4: Reverse thorax crop\n",
    "    thorax_height = d['dim_before_thorax_crop'][1]\n",
    "    thorax_padded_label = torch.zeros((1, thorax_height, trim_width), dtype=trim_padded_label.dtype)\n",
    "    thorax_padded_label[:, y1_thorax:, :] = trim_padded_label\n",
    "\n",
    "\n",
    "    original_spatial_dim = d['dim_before_resize_preliminary']\n",
    "\n",
    "    # Step 0: Resize to the original spatial dimensions before the final crop\n",
    "    # Assuming 'resize' was a downscaling operation and the original_spatial_dim is the target size\n",
    "    reconstructed_mask = F.interpolate(thorax_padded_label.unsqueeze(0).unsqueeze(0).float(),\n",
    "                                  size=original_spatial_dim.tolist(),  # Excluding the batch size dimension\n",
    "                                  mode='nearest-exact').squeeze(0).squeeze(0)  # Removing the added batch and channel dimensions\n",
    "\n",
    "\n",
    "    # Update the original label in the dictionary\n",
    "    return reconstructed_mask\n",
    "\n",
    "def get_mean_std_dataloader(dataloader, masked=False):\n",
    "  # Variables to store sum and sum of squares\n",
    "  sum_of_images = 0.0\n",
    "  sum_of_squares = 0.0\n",
    "  num_pixels = 0\n",
    "\n",
    "  # Iterate over the DataLoader\n",
    "  for batch in tqdm(dataloader):\n",
    "      image = batch[\"image\"]\n",
    "\n",
    "      if masked:\n",
    "        mask = image > 0.0\n",
    "        image = image[mask]\n",
    "\n",
    "      sum_of_images += image.sum()\n",
    "      sum_of_squares += (image ** 2).sum()\n",
    "      num_pixels += image.numel()\n",
    "\n",
    "  # Calculate the mean and standard deviation\n",
    "  mean = sum_of_images / num_pixels\n",
    "  std_dev = (sum_of_squares / num_pixels - mean ** 2) ** 0.5\n",
    "\n",
    "  print(f'Mean: {mean}, Standard Deviation: {std_dev}')\n",
    "  return mean, std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YjFOR1ng1HT"
   },
   "source": [
    "# Transform functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hSXAbEXav1B8"
   },
   "outputs": [],
   "source": [
    "class EnhanceLesionsSelective(MapTransform):\n",
    "    \"\"\"\n",
    "    A MONAI MapTransform to enhance lesions selectively in post-contrast images using a soft mask derived from the subtracted image.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys, threshold=0.9):\n",
    "        super().__init__(keys)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def create_soft_mask(self, subtracted_norm, threshold=0.9):\n",
    "        \"\"\"\n",
    "        Create a soft mask where values are scaled between 0 and max_value,\n",
    "        with intensities above 'threshold' in the subtracted image being closer to max_value.\n",
    "        \"\"\"\n",
    "        subtracted_norm = np.where(subtracted_norm > threshold, threshold, subtracted_norm/threshold)\n",
    "        return subtracted_norm\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        \n",
    "        for key in self.keys:\n",
    "            \n",
    "            subtracted = d[key]  # Assuming d[key] is a tuple (post_contrast, subtracted)\n",
    "\n",
    "            fourth_image = d['processed_image'] # 4th sequence\n",
    "            subtracted = np.array(subtracted)\n",
    "            \n",
    "            # Normalize the subtracted image to [0, 1]\n",
    "            subtracted_norm = cv2.normalize(subtracted[0], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "            \n",
    "            # Create a soft mask from the normalized subtracted image\n",
    "            soft_mask = self.create_soft_mask(subtracted_norm, threshold=self.threshold)\n",
    "            \n",
    "            # Normalize the post-contrast image to [0, 1] and apply the soft mask\n",
    "            fouth_image_norm = cv2.normalize(fourth_image[0], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "            enhanced_image = soft_mask*fouth_image_norm\n",
    "            \n",
    "            # Combine enhanced image with the normalized subtracted image for final enhancement\n",
    "            enhanced_image_final = subtracted_norm + enhanced_image\n",
    "\n",
    "            enhanced_image_final=cv2.normalize(enhanced_image_final, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "            \n",
    "            # Assuming the result should maintain the original shape as 1 x H x W\n",
    "            enhanced_image_final = np.expand_dims(enhanced_image_final, 0)\n",
    "\n",
    "            d[key] = monai.data.MetaTensor(enhanced_image_final)\n",
    "        \n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "class RemoveThorax(MapTransform):\n",
    "\n",
    "    def __init__(self, threshold=250, value=0, margin=0,**kwargs):\n",
    "        super(RemoveThorax, self).__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.margin = margin\n",
    "\n",
    "    def remove_upper_portion_get_coords(self, image):\n",
    "        # IMAGE IS C, H, W\n",
    "        # Step 1: Find the vertical middle line of the image\n",
    "        middle_x = image.shape[2] // 2\n",
    "\n",
    "        non_zero_y = 0\n",
    "        for y in reversed(range(image.shape[1])):\n",
    "            if image[:,y, middle_x] > 0:\n",
    "                non_zero_y = y\n",
    "                break\n",
    "\n",
    "        return non_zero_y  # Also return the y-coordinate for reference\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        image_to_threshold = d['processed_image']\n",
    "        dim_before_thorax_crop = torch.tensor(image_to_threshold.shape)\n",
    "        image_to_threshold = np.where(image_to_threshold < self.threshold, self.value, image_to_threshold)\n",
    "        y_coord = self.remove_upper_portion_get_coords(image_to_threshold)-self.margin\n",
    "        thorax_crop_coords = torch.tensor([0, y_coord, image_to_threshold.shape[2], image_to_threshold.shape[1]], dtype=torch.int16)  # (x1, y1, x2, y2)\n",
    "        image = d['processed_image']\n",
    "        image = image[:,y_coord:, :]\n",
    "\n",
    "        if data['has_mask']:\n",
    "            mask = d['processed_label']\n",
    "            mask = mask[:,y_coord:, :]\n",
    "            d['processed_label'] = mask\n",
    "\n",
    "        d['processed_image'] = image\n",
    "        d['thorax_crop_coords'] = torch.cat((d['thorax_crop_coords'], thorax_crop_coords), dim=0)\n",
    "        d['dim_before_thorax_crop'] = torch.cat((d['dim_before_thorax_crop'], dim_before_thorax_crop), dim=0)\n",
    "        return d\n",
    "\n",
    "\n",
    "class RemoveBottom(MapTransform):\n",
    "\n",
    "    def __init__(self, threshold=250, value=0, margin=0, **kwargs):\n",
    "        super(RemoveBottom, self).__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.margin = margin\n",
    "\n",
    "    def remove_lower_portion_get_coords(self, image):\n",
    "\n",
    "        # Step 2: Starting from the bottom, find the first non-zero pixel\n",
    "        non_zero_y = image.shape[1]-1\n",
    "        for y in reversed(range(image.shape[1])):  # Start from the bottom\n",
    "            if np.sum(image[:,y,:]) > 0:\n",
    "                non_zero_y = y\n",
    "                break\n",
    "\n",
    "        return non_zero_y  # Also return the y-coordinate for reference\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        image_to_threshold = d['processed_image']\n",
    "\n",
    "        \"\"\"print(\"ciao prima\")\n",
    "        plt.imshow(image_to_threshold[0], cmap='gray')\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "        image_to_threshold = np.where(image_to_threshold < self.threshold, self.value, image_to_threshold)\n",
    "        y_coord = self.remove_lower_portion_get_coords(image_to_threshold)+self.margin\n",
    "\n",
    "        bottom_crop_coords = torch.tensor([0, 0, image_to_threshold.shape[2], y_coord], dtype=torch.int16)  # (x1, y1, x2, y2)\n",
    "        image = d['processed_image']\n",
    "        dim_before_bottom_crop = torch.tensor(image.shape)\n",
    "        image = image[:,:y_coord, :]\n",
    "\n",
    "        if d['has_mask']:\n",
    "            mask = d['processed_label']\n",
    "            mask = mask[:,:y_coord, :]\n",
    "            d['processed_label'] = mask\n",
    "\n",
    "        d['processed_image'] = image\n",
    "        d['bottom_crop_coords'] = torch.cat((d['bottom_crop_coords'], bottom_crop_coords), dim=0)\n",
    "        d['dim_before_bottom_crop'] = torch.cat((d['dim_before_bottom_crop'], dim_before_bottom_crop), dim=0)\n",
    "\n",
    "        return d\n",
    "class FilterBySize(MapTransform):\n",
    "\n",
    "    def __init__(self, max_ratio, **kwargs):\n",
    "        super(FilterBySize, self).__init__(**kwargs)\n",
    "        self.max_ratio = max_ratio\n",
    "        self.delete = monai.transforms.DeleteItemsd(keys = ['image', 'label'])\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        c, h, w = d['image'].shape\n",
    "\n",
    "        if h >= w:\n",
    "\n",
    "          if w == 0 or self.max_ratio < h/w:\n",
    "            return self.delete(d)\n",
    "        elif w >= h:\n",
    "          if h==0 or self.max_ratio < w/h:\n",
    "            return self.delete(d)\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "class MedianSmooth(MapTransform):\n",
    "\n",
    "    def __init__(self, radius, **kwargs):\n",
    "        super(MedianSmooth, self).__init__(**kwargs)\n",
    "        self.median_smooth = monai.transforms.MedianSmooth(radius=radius)\n",
    "\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        d['processed_image'] = self.median_smooth(d['processed_image'])\n",
    "        return d\n",
    "\n",
    "\n",
    "class TrimSides(MapTransform):\n",
    "\n",
    "    def __init__(self, keys, threshold, tolerance, **kwargs):\n",
    "        super(TrimSides, self).__init__(keys, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def trim_sides(self, image_data, threshold=0, tolerance=0):\n",
    "        # Calculate the sum of pixel values across the channel axis for each column\n",
    "        col_sum = np.sum(image_data, axis=0).sum(axis=0)\n",
    "\n",
    "\n",
    "        # Find indices where the sum exceeds the threshold\n",
    "        x_start = np.argmax(col_sum > threshold)\n",
    "        x_end = len(col_sum) - np.argmax(col_sum[::-1] > threshold) - 1\n",
    "\n",
    "        # Apply tolerance\n",
    "        x_start = max(0, x_start - tolerance)\n",
    "        x_end = min(len(col_sum) - 1, x_end + tolerance)\n",
    "\n",
    "        return x_start, x_end\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        image = d['processed_image']\n",
    "\n",
    "\n",
    "        x_start, x_end = self.trim_sides(image_data=image, threshold=self.threshold, tolerance=self.tolerance)\n",
    "\n",
    "        # Crop the image and mask\n",
    "        cropped_image = image[:, :, x_start:x_end+1]\n",
    "\n",
    "\n",
    "\n",
    "        trim_coords = torch.tensor([x_start, x_end+1], dtype=torch.int16)\n",
    "\n",
    "        # Update the dictionary\n",
    "        d['processed_image'] = cropped_image\n",
    "\n",
    "        if d['has_mask']:\n",
    "            mask = d['processed_label']\n",
    "            cropped_mask = mask[:, :, x_start:x_end+1]\n",
    "            d['processed_label'] = cropped_mask\n",
    "\n",
    "\n",
    "        d['trim_coords'] = torch.cat((d['trim_coords'], trim_coords), dim=0)\n",
    "\n",
    "        return d\n",
    "\n",
    "class RelativeThresholding(MapTransform):\n",
    "    def __init__(self, keys, relative_threshold):\n",
    "        super().__init__(keys)\n",
    "        self.relative_threshold = relative_threshold\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            # assuming the image's pixel values are in the range [0, 1]\n",
    "            # if your image has a different range, adjust the max_intensity and threshold_value calculation accordingly\n",
    "            max_intensity = np.max(d[key])\n",
    "            threshold_value = max_intensity * self.relative_threshold\n",
    "\n",
    "            # apply the thresholding\n",
    "            d[key] = torch.tensor(np.where(d[key] >= threshold_value, 1, 0))\n",
    "        return d\n",
    "\n",
    "\n",
    "class RelativeThresholdingSingleChannel(MapTransform):\n",
    "    def __init__(self, keys, relative_threshold, channel_index):\n",
    "        super().__init__(keys)\n",
    "        self.relative_threshold = relative_threshold\n",
    "        self.channel_index = channel_index\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            original_image = d[key]\n",
    "\n",
    "            # Extract the specific channel\n",
    "            single_channel = original_image[self.channel_index]\n",
    "\n",
    "            # assuming the image's pixel values are in the range [0, 1]\n",
    "            # if your image has a different range, adjust the max_intensity and threshold_value calculation accordingly\n",
    "            max_intensity = torch.max(single_channel)\n",
    "            threshold_value = max_intensity * self.relative_threshold\n",
    "\n",
    "            # apply the thresholding to the single channel\n",
    "            transformed_channel = torch.where(single_channel >= threshold_value, 1, 0)\n",
    "\n",
    "            # Replace the channel in the original image\n",
    "            transformed_image = torch.clone(original_image)  # make a copy of the original image\n",
    "            transformed_image[self.channel_index] = transformed_channel\n",
    "            d[key] = transformed_image\n",
    "\n",
    "        return d\n",
    "\n",
    "class ThresholdBlack(MapTransform):\n",
    "\n",
    "    def __init__(self, threshold, value, **kwargs):\n",
    "        super(ThresholdBlack, self).__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "\n",
    "        d['processed_image'] = monai.data.MetaTensor(np.where(d['processed_image'] < self.threshold, self.value, d['processed_image']))\n",
    "\n",
    "        if d['has_mask']:\n",
    "            d['processed_label'] = monai.data.MetaTensor(np.where(d['processed_label'] < self.threshold, self.value, d['processed_label']))\n",
    "\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "def get_crop_coordinates(image):\n",
    "    image = image[200:,:]\n",
    "    # Convert the image to uint8 type for compatibility with OpenCV functions\n",
    "    image_uint8 = (image * 255).astype(np.uint8)\n",
    "\n",
    "    # Apply a binary threshold to segment the breasts from the background\n",
    "    _, thresholded = cv2.threshold(image_uint8, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Sort the contours by area (largest first)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    # If there are not enough contours found, return the original image\n",
    "    if len(contours) < 2:\n",
    "        return image\n",
    "\n",
    "    # Get the bounding boxes of the two largest contours (likely the breasts)\n",
    "    x1, y1, w1, h1 = cv2.boundingRect(contours[0])\n",
    "    x2, y2, w2, h2 = cv2.boundingRect(contours[1])\n",
    "\n",
    "    # Compute the combined bounding box\n",
    "    x = min(x1, x2)\n",
    "    y = min(y1, y2)\n",
    "    w = max(x1 + w1, x2 + w2) - x\n",
    "    h = max(y1 + h1, y2 + h2) - y\n",
    "\n",
    "    y_start, y_end, x_start, x_end = y, y+h, x, x+w\n",
    "\n",
    "    return y_start, y_end, x_start, x_end\n",
    "\n",
    "def remove_black_borders_get_coordinates(image):\n",
    "    \"\"\"\n",
    "    Remove black borders on the left and right of the image.\n",
    "    \"\"\"\n",
    "    # Sum the pixel values across rows to get a profile of the image's columns\n",
    "    column_sum = image.sum(axis=0)\n",
    "\n",
    "    # Set a threshold (small value) to identify black regions\n",
    "    threshold = 500\n",
    "\n",
    "    # Identify where the profile transitions from nearly zero to non-zero\n",
    "    non_black_columns = np.where(column_sum > threshold)[0]\n",
    "\n",
    "    y_start, y_end = non_black_columns[0], non_black_columns[-1]+1\n",
    "\n",
    "    return y_start, y_end\n",
    "\n",
    "def remove_black_borders_2D_get_coordinates(image):\n",
    "    \"\"\"\n",
    "    Remove black borders on all sides of the image.\n",
    "    \"\"\"\n",
    "    # Sum the pixel values across columns and rows to get profiles of the image's columns and rows\n",
    "    column_sum = image.sum(axis=0)\n",
    "    row_sum = image.sum(axis=1)\n",
    "\n",
    "    # Set a threshold (small value) to identify black regions\n",
    "    col_threshold = 500\n",
    "    row_threshold = 500\n",
    "\n",
    "    # Identify where the profile transitions from nearly zero to non-zero for both columns and rows\n",
    "    non_black_columns = np.where(column_sum > col_threshold)[0]\n",
    "    non_black_rows = np.where(row_sum > row_threshold)[0]\n",
    "\n",
    "    # Crop the image using the identified transition points\n",
    "    y_start, y_end = non_black_columns[0], non_black_columns[-1]+1\n",
    "    x_start, x_end = non_black_rows[0], non_black_rows[-1]+1\n",
    "    return y_start, y_end, x_start, x_end\n",
    "\n",
    "class RemoveBlack(MapTransform):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RemoveBlack, self).__init__(**kwargs)\n",
    "\n",
    "    def __call__(self, data):\n",
    "\n",
    "        d = dict(data)\n",
    "        y_start, y_end, x_start, x_end = get_crop_coordinates(d['image'][0])\n",
    "        if y_end-y_start < 100:\n",
    "            return d\n",
    "        d['image'] = d['image'][:,200:]\n",
    "        d['label'] = d['label'][:,200:]\n",
    "\n",
    "        d['image'] = d['image'][:, y_start: y_end+50]\n",
    "        d['label'] = d['label'][:, y_start: y_end+50]\n",
    "\n",
    "        y_start, y_end, x_start, x_end = remove_black_borders_2D_get_coordinates(d['image'][0])\n",
    "\n",
    "        d['image'] = d['image'][:, x_start:x_end, y_start: y_end]\n",
    "        d['label'] = d['label'][:, x_start:x_end, y_start: y_end]\n",
    "\n",
    "        return d\n",
    "\n",
    "class PrepareSample(MapTransform):\n",
    "\n",
    "    def __init__(self, target_size, subtracted_images, patches, **kwargs):\n",
    "        super(PrepareSample, self).__init__(**kwargs)\n",
    "        self.resize = monai.transforms.Resized(keys=['image', 'label'],spatial_size=target_size, mode='nearest-exact')\n",
    "        self.patches = patches\n",
    "        self.resize_original = monai.transforms.Resized(keys=['original_image', 'original_label'],spatial_size=target_size,  mode='nearest-exact')\n",
    "        self.subtracted_images = subtracted_images\n",
    "        self.loadimage = monai.transforms.LoadImage(ensure_channel_first=True, reader=monai.data.PILReader(converter=lambda image: image.convert(\"L\")))\n",
    "        \n",
    "                \n",
    "    def prepare_with_patches(self,data):\n",
    "        trim_breast_coords = data['trim_breast_coords'].tolist()\n",
    "        #trim_coords = data['trim_coords'].tolist()\n",
    "        thorax_crop_coords= data['thorax_crop_coords'].tolist()\n",
    "        bottom_crop_coords= data['bottom_crop_coords'].tolist()\n",
    "\n",
    "        crop_coords = data['crop_coords'].tolist()\n",
    "        pad_post_crop_coords = data['pad_post_crop_coords'].tolist()\n",
    "\n",
    "        if self.subtracted_images:\n",
    "            image_path = data['image_meta_dict']['subtracted_filename_or_obj']\n",
    "            if image_path.endswith(\".npy\"):\n",
    "                image = np.load(image_path)\n",
    "                image = np.expand_dims(image, 0)\n",
    "                image = monai.data.MetaTensor(image)\n",
    "\n",
    "            else:\n",
    "                label_path = data['label_meta_dict']['subtracted_filename_or_obj']\n",
    "                image = self.loadimage(image_path)\n",
    "                image = monai.transforms.Rotate90()(image)\n",
    "                image = monai.data.MetaTensor(image)\n",
    "                \n",
    "                label = self.loadimage(label_path)\n",
    "                label = monai.transforms.Rotate90()(label)\n",
    "                label = monai.data.MetaTensor(label)\n",
    "                data['label'] = label\n",
    "                \n",
    "\n",
    "            data['image'] = image\n",
    "\n",
    "       \n",
    "        data = self.resize(data)\n",
    "\n",
    "        data = self.resize_original(data)\n",
    "\n",
    "        image = data['image']\n",
    "\n",
    "        original_image = data['original_image']\n",
    "\n",
    "        target_size = data['preliminary_target_size'].tolist()\n",
    "\n",
    "        x1, y1, x2, y2 = thorax_crop_coords\n",
    "        \n",
    "        image = image[:,y1:, :]\n",
    "        original_image = original_image[:,y1:, :]\n",
    "\n",
    "\n",
    "        start, end = trim_breast_coords\n",
    "\n",
    "        image = image[:,:, start:end]\n",
    "        original_image = original_image[:,:, start:end]\n",
    "\n",
    "\n",
    "        x1, y1, x2, y2 = bottom_crop_coords\n",
    "        image = image[:,:y2, :]\n",
    "        original_image = original_image[:,:y2, :]\n",
    "\n",
    "\n",
    "        \"\"\"start, end = trim_coords\n",
    "\n",
    "        image = image[:, :, start:end]\n",
    "        label = label[:, :,start: end]\n",
    "        \"\"\"\n",
    "        y_min, y_max, x_min, x_max = crop_coords\n",
    "\n",
    "        image = image[:, y_min:y_max, x_min:x_max]\n",
    "        original_image = original_image[:, y_min:y_max, x_min:x_max]\n",
    "\n",
    "        before1, before2, before3 = pad_post_crop_coords[0],pad_post_crop_coords[1],pad_post_crop_coords[2]\n",
    "\n",
    "        image = np.pad(image, ((before1[0], before1[1]), (before2[0], before2[1]), (before3[0], before3[1])), 'constant')\n",
    "        original_image = np.pad(original_image, ((before1[0], before1[1]), (before2[0], before2[1]), (before3[0], before3[1])), 'constant')\n",
    "\n",
    "        data['image']=image\n",
    "        data['processed_image'] = original_image\n",
    "\n",
    "\n",
    "        if data['has_mask']:\n",
    "            label = data['label']\n",
    "            original_label = data['original_label']\n",
    "            \n",
    "            x1, y1, x2, y2 = thorax_crop_coords\n",
    "            \n",
    "            #label =  monai.transforms.Resize(spatial_size=target_size)(label)\n",
    "            label = label[:,y1:, :]\n",
    "            original_label = original_label[:,y1:, :]\n",
    "\n",
    "            start, end = trim_breast_coords\n",
    "            label = label[:,:, start:end]\n",
    "            original_label = original_label[:,:, start:end]\n",
    "\n",
    "            x1, y1, x2, y2 = bottom_crop_coords\n",
    "            label = label[:,:y2, :]\n",
    "            original_label = original_label[:,:y2, :]\n",
    "\n",
    "            y_min, y_max, x_min, x_max = crop_coords\n",
    "            label = label[:, y_min:y_max, x_min:x_max]\n",
    "            original_label = original_label[:, y_min:y_max, x_min:x_max]\n",
    "\n",
    "            before1, before2, before3 = pad_post_crop_coords[0],pad_post_crop_coords[1],pad_post_crop_coords[2]\n",
    "\n",
    "            label = np.pad(label, ((before1[0], before1[1]), (before2[0], before2[1]), (before3[0], before3[1])), 'constant')\n",
    "            original_label = np.pad(original_label, ((before1[0], before1[1]), (before2[0], before2[1]), (before3[0], before3[1])), 'constant')\n",
    "\n",
    "            data['label']=label\n",
    "            data['processed_label'] = original_label\n",
    "\n",
    "        return data\n",
    "\n",
    "    def prepare_without_patches(self,data):\n",
    "\n",
    "        trim_breast_coords = data['trim_breast_coords'].tolist()\n",
    "        #trim_coords = data['trim_coords'].tolist()\n",
    "        thorax_crop_coords= data['thorax_crop_coords'].tolist()\n",
    "        bottom_crop_coords= data['bottom_crop_coords'].tolist()\n",
    "\n",
    "        crop_coords = data['crop_coords'].tolist()\n",
    "        pad_post_crop_coords = data['pad_post_crop_coords'].tolist()\n",
    "\n",
    "        if self.subtracted_images:\n",
    "            image_path = data['image_meta_dict']['subtracted_filename_or_obj']\n",
    "            if image_path.endswith(\".npy\"):\n",
    "                image = np.load(image_path)\n",
    "                image = np.expand_dims(image, 0)\n",
    "                image = monai.data.MetaTensor(image)\n",
    "                data['image'] = image\n",
    "\n",
    "                 \n",
    "\n",
    "            else: #FOR BRADM\n",
    "                label_path = data['label_meta_dict']['subtracted_filename_or_obj']\n",
    "                image = self.loadimage(image_path)\n",
    "                image = monai.transforms.Rotate90()(image)\n",
    "                image = monai.data.MetaTensor(image)\n",
    "                \n",
    "                label = self.loadimage(label_path)\n",
    "                label = monai.transforms.Rotate90()(label)\n",
    "                label = monai.data.MetaTensor(label)\n",
    "                data['label'] = label\n",
    "                data['image'] = image\n",
    "\n",
    "        data = self.resize(data)\n",
    "        data = self.resize_original(data)\n",
    "\n",
    "        image = data['image']\n",
    "        original_image = data['original_image']\n",
    "\n",
    "        target_size = data['preliminary_target_size'].tolist()\n",
    "        \n",
    "        x1, y1, x2, y2 = thorax_crop_coords\n",
    "        \n",
    "        image = image[:,y1:, :]\n",
    "        original_image = original_image[:,y1:, :]\n",
    "\n",
    "        #start, end = trim_breast_coords\n",
    "\n",
    "        #image = image[:,:, start:end]\n",
    "        #original_image = original_image[:,:, start:end]\n",
    "\n",
    "\n",
    "        x1, y1, x2, y2 = bottom_crop_coords\n",
    "        image = image[:,:y2, :]\n",
    "        original_image = original_image[:,:y2, :]\n",
    "\n",
    "        y_min, y_max, x_min, x_max = crop_coords\n",
    "\n",
    "        image = image[:, y_min:y_max, x_min:x_max]\n",
    "        original_image = original_image[:, y_min:y_max, x_min:x_max]\n",
    "\n",
    "        before1, before2, before3 = pad_post_crop_coords[0],pad_post_crop_coords[1],pad_post_crop_coords[2]\n",
    "\n",
    "        image = np.pad(image, ((before1[0], before1[1]), (before2[0], before2[1]), (before3[0], before3[1])), 'constant')\n",
    "        original_image = np.pad(original_image, ((before1[0], before1[1]), (before2[0], before2[1]), (before3[0], before3[1])), 'constant')\n",
    "\n",
    "        data['image']=image\n",
    "        data['processed_image'] = original_image\n",
    "\n",
    "\n",
    "        \n",
    "        if data['has_mask']:\n",
    "            label = data['label']\n",
    "            original_label = data['original_label']\n",
    "            \n",
    "            x1, y1, x2, y2 = thorax_crop_coords\n",
    "            \n",
    "            #label =  monai.transforms.Resize(spatial_size=target_size)(label)\n",
    "            label = label[:,y1:, :]\n",
    "            original_label = original_label[:,y1:, :]\n",
    "\n",
    "            #start, end = trim_breast_coords\n",
    "            #label = label[:,:, start:end]\n",
    "            #original_label = original_label[:,:, start:end]\n",
    "\n",
    "            x1, y1, x2, y2 = bottom_crop_coords\n",
    "            label = label[:,:y2, :]\n",
    "            original_label = original_label[:,:y2, :]\n",
    "\n",
    "            y_min, y_max, x_min, x_max = crop_coords\n",
    "            label = label[:, y_min:y_max, x_min:x_max]\n",
    "            original_label = original_label[:, y_min:y_max, x_min:x_max]\n",
    "\n",
    "            before1, before2, before3 = pad_post_crop_coords[0],pad_post_crop_coords[1],pad_post_crop_coords[2]\n",
    "\n",
    "            label = np.pad(label, ((before1[0], before1[1]), (before2[0], before2[1]), (before3[0], before3[1])), 'constant')\n",
    "            original_label = np.pad(original_label, ((before1[0], before1[1]), (before2[0], before2[1]), (before3[0], before3[1])), 'constant')\n",
    "\n",
    "            data['label']=label\n",
    "            data['processed_label'] = original_label\n",
    "\n",
    "        return data\n",
    "        \n",
    "    def __call__(self, data):\n",
    "\n",
    "        if self.patches:\n",
    "            return self.prepare_with_patches(data)\n",
    "        else:\n",
    "            return self.prepare_without_patches(data)\n",
    "\n",
    "    \n",
    "\n",
    "class ForegroundMaskdSingleChannel(MapTransform):\n",
    "    def __init__(self, keys, channel_index, num_bins=10):\n",
    "        super().__init__(keys)\n",
    "        self.foregroundMask= monai.transforms.ForegroundMask(invert=True)\n",
    "        self.channel_index = channel_index\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "\n",
    "        for key in self.keys:\n",
    "\n",
    "            original_image = d[key]\n",
    "\n",
    "            # Extract the specific channel\n",
    "            single_channel = original_image[self.channel_index]\n",
    "\n",
    "            single_channel = np.expand_dims(single_channel, 0)\n",
    "\n",
    "            # Apply the transform to this channel\n",
    "            transformed_channel = self.foregroundMask(single_channel)\n",
    "\n",
    "            # Replace the channel in the original image\n",
    "            transformed_image = np.array(original_image)  # make a copy of the original image\n",
    "            transformed_image[self.channel_index] = transformed_channel\n",
    "            d[key] = torch.tensor(transformed_image)\n",
    "\n",
    "        return d\n",
    "\n",
    "class NormalizedSingleChannel(MapTransform):\n",
    "    def __init__(self, keys, channel_index):\n",
    "        super().__init__(keys)\n",
    "        self.normalize = monai.transforms.NormalizeIntensity(subtrahend  = 0.1046, divisor=335.7632)\n",
    "        self.channel_index = channel_index\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "\n",
    "        for key in self.keys:\n",
    "\n",
    "            original_image = d[key]\n",
    "\n",
    "            # Extract the specific channel\n",
    "            single_channel = original_image[self.channel_index]\n",
    "\n",
    "            single_channel = np.expand_dims(single_channel, 0)\n",
    "\n",
    "            # Apply the transform to this channel\n",
    "            transformed_channel = self.normalize(single_channel)[0]\n",
    "\n",
    "\n",
    "            # Replace the channel in the original image\n",
    "            transformed_image = np.array(original_image)  # make a copy of the original image\n",
    "\n",
    "            transformed_image[self.channel_index] = transformed_channel\n",
    "            d[key] = torch.tensor(transformed_image)\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "class Convert3D(MapTransform):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super( Convert3D, self).__init__(**kwargs)\n",
    "\n",
    "    def __call__(self, data):\n",
    "\n",
    "        d = data\n",
    "\n",
    "        image = d['image']\n",
    "        label = d['label']\n",
    "\n",
    "        image = monai.transforms.utils_pytorch_numpy_unification.repeat(image,(3, 1, 1), axis=0)\n",
    "        d['image']=image\n",
    "        return d\n",
    "\n",
    "\n",
    "class Convert3DEnhanced(MapTransform):\n",
    "\n",
    "    def __init__(self, keys,**kwargs):\n",
    "        super( Convert3DEnhanced, self).__init__(keys, **kwargs)\n",
    "        self.relativeThresholding = RelativeThresholdingSingleChannel(keys=['image'], relative_threshold=0.4, channel_index = 1)\n",
    "        self.foregroundMaskdSingleChannel = ForegroundMaskdSingleChannel(keys = ['image'], channel_index = 2)\n",
    "\n",
    "    def __call__(self, data):\n",
    "\n",
    "        d = data\n",
    "\n",
    "        image = d['image']\n",
    "        label = d['label']\n",
    "\n",
    "        # Convert MetaTensor to torch.Tensor\n",
    "        image_tensor = convert_to_numpy(image)\n",
    "\n",
    "        # Perform the repeat operation\n",
    "        image_tensor = np.repeat(image_tensor, 3, 0)\n",
    "\n",
    "        # Convert back to MetaTensor if necessary\n",
    "        image = torch.tensor(image_tensor)\n",
    "\n",
    "\n",
    "        d['image'] = image\n",
    "        d = self.relativeThresholding(d)\n",
    "        d = self.foregroundMaskdSingleChannel(d)\n",
    "        return d\n",
    "\n",
    "class BoundingBoxSplit(MapTransform):\n",
    "    def __init__(self, keys=(\"image\", \"label\"), allow_missing_keys=False, bbox_size=(256, 256)):\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.bbox_size = bbox_size\n",
    "\n",
    "    def pad_image(self, image):\n",
    "        \"\"\"\n",
    "        Pad the image to the desired bounding box size if it's smaller.\n",
    "\n",
    "        Parameters:\n",
    "        - image (numpy.ndarray): The image to be padded.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The padded image.\n",
    "        \"\"\"\n",
    "        channels, height, width = image.shape\n",
    "        pad_height = max(0, self.bbox_size[0] - height)\n",
    "        pad_width = max(0, self.bbox_size[1] - width)\n",
    "\n",
    "        # Padding format should be [(0, 0), (pad_height, 0), (pad_width, 0)] to maintain the channel dimension\n",
    "        padded_image = np.pad(image, [(0, 0), (0, pad_height), (0, pad_width)], mode='constant', constant_values=0)\n",
    "        return padded_image\n",
    "\n",
    "    def _positive_bounding_box(self, mask):\n",
    "        \"\"\"\n",
    "        Computes the bounding box for a region of interest in a binary mask.\n",
    "\n",
    "        Parameters:\n",
    "        - mask (numpy.ndarray): A binary mask.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: (y_min, y_max, x_min, x_max) coordinates of the bounding box.\n",
    "        \"\"\"\n",
    "        # Find the row and column indices where the mask is 1.\n",
    "        mask = mask[0]\n",
    "        rows, cols = np.where(mask == 1)\n",
    "\n",
    "        # If no ROI is found, return None.\n",
    "        if len(rows) == 0 or len(cols) == 0:\n",
    "            return None\n",
    "\n",
    "        y_min, y_max = np.min(rows), np.max(rows)\n",
    "        x_min, x_max = np.min(cols), np.max(cols)\n",
    "\n",
    "        return y_min, y_max, x_min, x_max\n",
    "\n",
    "    def _negative_bounding_box(self, mask, num_boxes=1):\n",
    "        \"\"\"\n",
    "        Extracts two random bounding boxes of negative regions from a binary mask.\n",
    "\n",
    "        Parameters:\n",
    "        - mask (numpy.ndarray): A binary mask of shape (1, H, W).\n",
    "\n",
    "        Returns:\n",
    "        - list: Two tuples with (y_min, y_max, x_min, x_max) coordinates of the bounding boxes of the negative regions.\n",
    "        \"\"\"\n",
    "        height, width = self.bbox_size[0], self.bbox_size[1]\n",
    "        mask = mask[0]  # Remove the singleton dimension: (1, H, W) -> (H, W)\n",
    "\n",
    "        H, W = mask.shape\n",
    "\n",
    "        step_y = height // 2\n",
    "        step_x = width // 2\n",
    "\n",
    "        bboxes = []\n",
    "        trials = 0\n",
    "        max_trials = 50  # To avoid infinite loops, though this value can be adjusted\n",
    "\n",
    "        while len(bboxes) < num_boxes and trials < max_trials:\n",
    "            # Randomly sample a starting point\n",
    "            y = np.random.randint(0, H - height + 1, 1)[0]\n",
    "            x = np.random.randint(0, W - width + 1, 1)[0]\n",
    "\n",
    "            # Align the sampled point to the nearest half-sized step grid\n",
    "            y = (y // step_y) * step_y\n",
    "            x = (x // step_x) * step_x\n",
    "\n",
    "            window = mask[y:y+height, x:x+width]\n",
    "            if np.sum(window) == 0 and (y, y+height-1, x, x+width-1) not in bboxes:\n",
    "                bboxes.append((x, x+width-1, y, y+height-1))\n",
    "            trials += 1\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "\n",
    "    def _get_bboxes(self, mask):\n",
    "        if mask.sum() == 0:\n",
    "            return self._negative_bounding_box(mask, num_boxes=1)\n",
    "        else:\n",
    "            bbox_negative = self._negative_bounding_box(mask, num_boxes=1)\n",
    "            bbox_positive = self._positive_bounding_box(mask)\n",
    "            if not bbox_positive:\n",
    "                return bbox_negative\n",
    "\n",
    "            y_min, y_max, x_min, x_max = bbox_positive\n",
    "            width, height = self.bbox_size\n",
    "\n",
    "            # Calculate the sizes of the positive bounding box\n",
    "            pos_width = x_max - x_min + 1\n",
    "            pos_height = y_max - y_min + 1\n",
    "\n",
    "            # Ensure the new bounding box includes the positive bounding box\n",
    "            x_min_new = max(x_min - (width - pos_width) // 2, 0)\n",
    "            y_min_new = max(y_min - (height - pos_height) // 2, 0)\n",
    "\n",
    "            x_max_new = x_min_new + width - 1\n",
    "            y_max_new = y_min_new + height - 1\n",
    "\n",
    "            # Adjust the bounding box if it extends beyond the mask's boundaries\n",
    "            if y_max_new >= mask.shape[1]:\n",
    "                y_max_new = mask.shape[1] - 1\n",
    "                y_min_new = max(y_max_new - height + 1, 0)  # Ensure it doesn't go negative\n",
    "            if x_max_new >= mask.shape[2]:\n",
    "                x_max_new = mask.shape[2] - 1\n",
    "                x_min_new = max(x_max_new - width + 1, 0)  # Ensure it doesn't go negative\n",
    "\n",
    "            # Ensure the positive region is included in the new bounding box\n",
    "            x_min_new = min(x_min_new, x_min)\n",
    "            y_min_new = min(y_min_new, y_min)\n",
    "            x_max_new = max(x_max_new, x_max)\n",
    "            y_max_new = max(y_max_new, y_max)\n",
    "\n",
    "            bbox_positive = [(x_min_new, x_max_new, y_min_new, y_max_new)]\n",
    "            return bbox_negative + bbox_positive\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "\n",
    "\n",
    "        d['image'] = self.pad_image(d['image'])\n",
    "        d['label'] = self.pad_image(d['label'])\n",
    "\n",
    "        data = []\n",
    "\n",
    "        label = d['label']\n",
    "        bboxes = self._get_bboxes(label)\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            xmin, xmax, ymin, ymax = bbox\n",
    "            new_d= d.copy()\n",
    "            # Crop using bounding box\n",
    "            new_d['image'] = torch.tensor(d[\"image\"][:, ymin:ymax+1, xmin:xmax+1])\n",
    "            new_d['label'] = torch.tensor(label[:, ymin:ymax+1, xmin:xmax+1])\n",
    "\n",
    "\n",
    "            # Adjust meta-data for cropped image and label\n",
    "            new_d[\"image_meta_dict\"] = dict(d[\"image_meta_dict\"])\n",
    "            new_d[\"image_meta_dict\"][\"original_affine\"] = d[\"image_meta_dict\"][\"affine\"]\n",
    "            new_d[\"image_meta_dict\"][\"original_affine\"] = monai.data.MetaTensor(new_d[\"image_meta_dict\"][\"original_affine\"])\n",
    "\n",
    "            affine_adjust = np.array([[1, 0, 0, xmin], [0, 1, 0, ymin], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "            new_d[\"image_meta_dict\"][\"affine\"] = d[\"image_meta_dict\"][\"affine\"] @ affine_adjust\n",
    "            new_d[\"image_meta_dict\"][\"affine\"] = monai.data.MetaTensor(new_d[\"image_meta_dict\"]['affine'])\n",
    "\n",
    "\n",
    "\n",
    "            new_d[\"label_meta_dict\"] = dict(d[\"label_meta_dict\"])\n",
    "            new_d[\"label_meta_dict\"][\"original_affine\"] = d[\"label_meta_dict\"][\"affine\"]\n",
    "            new_d[\"label_meta_dict\"][\"original_affine\"] = monai.data.MetaTensor(new_d[\"label_meta_dict\"][\"original_affine\"])\n",
    "\n",
    "\n",
    "            new_d[\"label_meta_dict\"][\"affine\"] = d[\"label_meta_dict\"][\"affine\"] @affine_adjust\n",
    "            new_d[\"label_meta_dict\"][\"affine\"] = monai.data.MetaTensor(new_d[\"label_meta_dict\"]['affine'])\n",
    "\n",
    "            data.append(new_d)\n",
    "        if len(data) == 0:\n",
    "          return d\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "class AdaptiveCropBreasts2(MapTransform):\n",
    "\n",
    "    def __init__(self, keys=['processed_image','processed_label'], strict_boundary_perc=0.001):\n",
    "        \"\"\"\n",
    "        Initializes the adaptive crop transform.\n",
    "\n",
    "        :param keys: The data keys to apply the transform to.\n",
    "        \"\"\"\n",
    "        super().__init__(keys)\n",
    "        self.strict_boundary_perc = strict_boundary_perc\n",
    "\n",
    "    def find_strict_breast_region(self, half_image_sum, peak_index, total_width):\n",
    "        # Set a stricter percentage of the peak value to consider as the breast boundary\n",
    "        peak_value = half_image_sum[peak_index]\n",
    "        boundary_threshold = peak_value * self.strict_boundary_perc\n",
    "\n",
    "        # Find the left boundary of the breast region\n",
    "        left_boundary = peak_index\n",
    "        while left_boundary > 0 and half_image_sum[left_boundary] > boundary_threshold:\n",
    "            left_boundary -= 1\n",
    "\n",
    "        # Find the right boundary of the breast region\n",
    "        right_boundary = peak_index\n",
    "        while right_boundary < total_width and half_image_sum[right_boundary] > boundary_threshold:\n",
    "            right_boundary += 1\n",
    "\n",
    "        return left_boundary, right_boundary\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        data_list = []\n",
    "\n",
    "        x1, y1, x2, y2 = d['thorax_crop_coords']\n",
    "\n",
    "        image = copy.deepcopy(d['image'])\n",
    "        mask = copy.deepcopy(d['label'])\n",
    "\n",
    "        processed_image = copy.deepcopy(d['processed_image'])\n",
    "        processed_mask = copy.deepcopy(d['processed_label'])\n",
    "\n",
    "        dim_before_breast_crop = torch.tensor(image.shape)\n",
    "        image = image[:,y1:, :]\n",
    "        mask = mask[:,y1:, :]\n",
    "\n",
    "        image_for_check = image[:, 20:, :]\n",
    "\n",
    "        # Calculate the vertical sum for the left and right halves\n",
    "        mid_point = image_for_check.shape[2] // 2\n",
    "\n",
    "        left_half_sum = image_for_check.sum(axis=(0, 1))[:mid_point]\n",
    "        right_half_sum = image_for_check.sum(axis=(0, 1))[mid_point:]\n",
    "\n",
    "        # Find the peak in each half\n",
    "        left_peak_index = np.argmax(left_half_sum)\n",
    "        right_peak_index = np.argmax(right_half_sum) + mid_point\n",
    "\n",
    "        # Find the breast regions\n",
    "        left_breast_boundaries = self.find_strict_breast_region(left_half_sum, left_peak_index, mid_point)\n",
    "        right_breast_boundaries = self.find_strict_breast_region(right_half_sum, right_peak_index - mid_point, image.shape[2] - mid_point)\n",
    "\n",
    "        # Extract the breast regions\n",
    "        left_breast_region_image_strict = processed_image[:, :, left_breast_boundaries[0]:left_breast_boundaries[1]]\n",
    "        left_breast_region_mask_strict = processed_mask[:, :, left_breast_boundaries[0]:left_breast_boundaries[1]]\n",
    "\n",
    "        right_breast_region_mask_strict = processed_mask[:, :, right_breast_boundaries[0] + mid_point:right_breast_boundaries[1] + mid_point]\n",
    "        right_breast_region_image_strict = processed_image[:, :, right_breast_boundaries[0] + mid_point:right_breast_boundaries[1] + mid_point]\n",
    "\n",
    "        left_breast_trim_coords = torch.tensor([left_breast_boundaries[0], left_breast_boundaries[1]], dtype=torch.int16)\n",
    "        right_breast_trim_coords = torch.tensor([right_breast_boundaries[0] + mid_point, right_breast_boundaries[1] + mid_point], dtype=torch.int16)\n",
    "\n",
    "        regions = [\n",
    "            (left_breast_region_image_strict, left_breast_region_mask_strict, left_breast_trim_coords),\n",
    "            (right_breast_region_image_strict, right_breast_region_mask_strict, right_breast_trim_coords)\n",
    "        ]\n",
    "\n",
    "        # Loop through the two largest regions to crop the image and mask\n",
    "        for i, region in enumerate(regions):\n",
    "            new_d = d.copy()\n",
    "\n",
    "            new_d['processed_image'] = region[0]\n",
    "            new_d['processed_label'] = region[1]\n",
    "\n",
    "            # Adjust meta-data for cropped image and label if meta-data is available\n",
    "            if \"image_meta_dict\" in d and \"label_meta_dict\" in d:\n",
    "                left_boundary = left_breast_boundaries[0] if i == 0 else right_breast_boundaries[0] + mid_point\n",
    "                affine_adjust = np.array([[1, 0, 0, -left_boundary], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "\n",
    "                new_d[\"image_meta_dict\"] = dict(d[\"image_meta_dict\"])\n",
    "                new_d[\"image_meta_dict\"][\"original_affine\"] = d[\"image_meta_dict\"][\"affine\"]\n",
    "                new_d[\"image_meta_dict\"][\"original_affine\"] = monai.data.MetaTensor(new_d[\"image_meta_dict\"][\"original_affine\"])\n",
    "\n",
    "                new_d[\"image_meta_dict\"][\"affine\"] = d[\"image_meta_dict\"][\"affine\"] @ affine_adjust\n",
    "                new_d[\"image_meta_dict\"][\"affine\"] = monai.data.MetaTensor(new_d[\"image_meta_dict\"][\"affine\"])\n",
    "\n",
    "                new_d[\"label_meta_dict\"] = dict(d[\"label_meta_dict\"])\n",
    "                new_d[\"label_meta_dict\"][\"original_affine\"] = d[\"label_meta_dict\"][\"affine\"]\n",
    "                new_d[\"label_meta_dict\"][\"original_affine\"] = monai.data.MetaTensor(new_d[\"label_meta_dict\"][\"original_affine\"])\n",
    "\n",
    "                new_d[\"label_meta_dict\"][\"affine\"] = d[\"label_meta_dict\"][\"affine\"] @ affine_adjust\n",
    "                new_d[\"label_meta_dict\"][\"affine\"]= monai.data.MetaTensor(new_d[\"label_meta_dict\"][\"affine\"])\n",
    "\n",
    "\n",
    "                new_d['trim_breast_coords'] = torch.cat((new_d['trim_breast_coords'], region[2]), dim=0)\n",
    "                new_d['dim_before_breast_crop'] = torch.cat((d['dim_before_breast_crop'], dim_before_breast_crop), dim=0)\n",
    "            data_list.append(new_d)\n",
    "\n",
    "        return data_list\n",
    "\n",
    "\n",
    "\n",
    "class AdaptiveCropBreasts(MapTransform):\n",
    "\n",
    "    def __init__(self,  keys=[\"image\", \"label\"], margin_size=100, min_size=200, threshold=1):\n",
    "        \"\"\"\n",
    "        Initializes the adaptive crop transform.\n",
    "\n",
    "        :param margin_size: The maximum margin size to add\n",
    "        :param min_size: The minimum size of regions to keep\n",
    "        :param threshold: Threshold value to create a binary image\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(keys)\n",
    "        self.margin_size = margin_size\n",
    "        self.min_size = min_size\n",
    "        self.threshold = threshold\n",
    "        self.label_fn = label_fn\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \"\"\"\n",
    "        Applies the adaptive crop to the image and mask in the input data.\n",
    "\n",
    "        :param data: Dictionary containing image and mask\n",
    "        :return: Dictionary with cropped image and mask\n",
    "        \"\"\"\n",
    "        d = dict(data)\n",
    "        data_list = []\n",
    "        image = d['image']\n",
    "        mask = d['label']\n",
    "\n",
    "        image_mean, image_std = image.mean(), image.std()\n",
    "\n",
    "        # Create a binary image based on the threshold\n",
    "        threshold = int(image_mean + image_std)\n",
    "        binary_image = image[0] > threshold\n",
    "\n",
    "        # Label the connected components in the binary image\n",
    "        labeled_image = self.label_fn(binary_image)\n",
    "\n",
    "        # Remove small objects\n",
    "        cleaned_image = morphology.remove_small_objects(labeled_image, min_size=self.min_size)\n",
    "\n",
    "        # Calculate region properties\n",
    "        regions = regionprops(cleaned_image)\n",
    "\n",
    "        # Sort regions based on area size, keeping the two largest\n",
    "        largest_regions = sorted(regions, key=lambda x: -x.area)[1:3]\n",
    "\n",
    "        # Initialize lists to store the cropped images and masks\n",
    "        cropped_images = []\n",
    "        cropped_masks = []\n",
    "\n",
    "        # Loop through the two largest regions to crop the image and mask\n",
    "        for region in largest_regions:\n",
    "            new_d= d.copy()\n",
    "            bbox = region.bbox\n",
    "            top, left, bottom, right = bbox\n",
    "\n",
    "            # Add margin\n",
    "            top = max(top - self.margin_size, 0)\n",
    "            left = max(left - self.margin_size, 0)\n",
    "            bottom = min(bottom + self.margin_size, image.shape[1])\n",
    "            right = min(right + self.margin_size, image.shape[2])\n",
    "\n",
    "            # Crop the image and mask\n",
    "            cropped_image = image[: ,top:bottom, left:right]\n",
    "            cropped_mask = mask[:,top:bottom, left:right]\n",
    "\n",
    "            new_d['image'] = torch.tensor(cropped_image)\n",
    "            new_d['label'] = torch.tensor(cropped_mask)\n",
    "\n",
    "            # Adjust meta-data for cropped image and label\n",
    "            new_d[\"image_meta_dict\"] = dict(d[\"image_meta_dict\"])\n",
    "\n",
    "            new_d[\"image_meta_dict\"][\"original_affine\"] = d[\"image_meta_dict\"][\"affine\"]\n",
    "            new_d[\"image_meta_dict\"][\"original_affine\"] = monai.data.MetaTensor(new_d[\"image_meta_dict\"][\"original_affine\"])\n",
    "\n",
    "            affine_adjust = np.array([[1, 0, 0, left], [0, 1, 0, top], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "            new_d[\"image_meta_dict\"][\"affine\"] = d[\"image_meta_dict\"][\"affine\"] @ affine_adjust\n",
    "            new_d[\"image_meta_dict\"][\"affine\"] = monai.data.MetaTensor(new_d[\"image_meta_dict\"][\"affine\"])\n",
    "\n",
    "            new_d[\"label_meta_dict\"] = dict(d[\"label_meta_dict\"])\n",
    "            new_d[\"label_meta_dict\"][\"original_affine\"] = d[\"label_meta_dict\"][\"affine\"]\n",
    "            new_d[\"label_meta_dict\"][\"original_affine\"] = monai.data.MetaTensor(new_d[\"label_meta_dict\"][\"original_affine\"])\n",
    "\n",
    "\n",
    "            new_d[\"label_meta_dict\"][\"affine\"] = d[\"label_meta_dict\"][\"affine\"] @affine_adjust\n",
    "            new_d[\"label_meta_dict\"][\"affine\"] = monai.data.MetaTensor(new_d[\"label_meta_dict\"][\"affine\"])\n",
    "\n",
    "\n",
    "            data_list.append(new_d)\n",
    "\n",
    "        if len(data_list) < 2:\n",
    "            return [data]\n",
    "\n",
    "        return data_list\n",
    "\n",
    "\n",
    "from monai.transforms import Resize\n",
    "from monai.transforms import SpatialCrop\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CropToSquare(MapTransform):\n",
    "\n",
    "    def __init__(self, keys=[\"image\", \"label\"], shrink_factor=10, black_threshold=400):\n",
    "        super().__init__(keys)\n",
    "        self.shrink_factor = shrink_factor\n",
    "        self.black_threshold = black_threshold  # Pixel intensity threshold for 'almost black'\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        image = d['processed_image']\n",
    "        label = d['processed_label']\n",
    "\n",
    "        \"\"\"print(\"prima 22\")\n",
    "        plt.imshow(image[0], cmap='gray')\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "        d['dim_before_crop'] = torch.cat((d['dim_before_crop'], torch.tensor(image.shape)), dim=0)\n",
    "\n",
    "        # Crop the image along the longest dimension to remove almost black regions\n",
    "        max_intensity = np.max(image[0], axis=0)  # Max intensity for each column\n",
    "\n",
    "        valid_columns = np.argwhere(max_intensity > self.black_threshold).flatten()\n",
    "\n",
    "        if len(valid_columns) == 0:\n",
    "            x_min = 0\n",
    "            x_max = image.shape[2]\n",
    "        else:\n",
    "            x_min, x_max = valid_columns[0], valid_columns[-1]\n",
    "\n",
    "            if x_min-30 >=0:\n",
    "                x_min = x_min-30\n",
    "            if x_max+30 < image.shape[2]:\n",
    "                x_max = x_max+30\n",
    "\n",
    "        y_min = self.shrink_factor\n",
    "        y_max = image.shape[1]\n",
    "\n",
    "        # Crop both image and label\n",
    "        image = image[:, y_min:y_max, x_min:x_max]\n",
    "        label = label[:, y_min:y_max, x_min:x_max]\n",
    "\n",
    "        # Determine the new longest and shortest dimensions\n",
    "        new_longest_dim = max(image.shape[1], image.shape[2])\n",
    "        new_shortest_dim = min(image.shape[1], image.shape[2])\n",
    "\n",
    "        # Pad to make a square\n",
    "        pad_bottom = new_longest_dim - image.shape[1] if image.shape[1] < new_longest_dim else 0\n",
    "        pad_right = new_longest_dim - image.shape[2] if image.shape[2] < new_longest_dim else 0\n",
    "\n",
    "        d['processed_image'] = np.pad(image, ((0, 0), (0, pad_bottom), (0, pad_right)), 'constant')\n",
    "        d['processed_label'] = np.pad(label, ((0, 0), (0, pad_bottom), (0, pad_right)), 'constant')\n",
    "\n",
    "        # Save the crop coordinates and dimensions\n",
    "        crop_coords = torch.tensor([y_min, y_max, x_min, x_max], dtype=torch.int16)\n",
    "        pad_post_crop_coords = torch.tensor([[0, 0], [0, pad_bottom], [0, pad_right]], dtype=torch.int16)\n",
    "        d['crop_coords'] = torch.cat((d['crop_coords'], crop_coords), dim=0)\n",
    "        d['pad_post_crop_coords'] = torch.cat((d['pad_post_crop_coords'], pad_post_crop_coords), dim=0)\n",
    "        \n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class Resize(MapTransform):\n",
    "\n",
    "    def __init__(self,  step, keys=[\"image\", \"label\"], spatial_size=256):\n",
    "\n",
    "        super().__init__(keys)\n",
    "        self.spatial_size = spatial_size\n",
    "        self.resize = monai.transforms.Resize(spatial_size=spatial_size , mode='nearest-exact')\n",
    "        self.step = step\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "\n",
    "        image_key = 'processed_image' if self.step ==\"preliminary\" else 'image'\n",
    "        label_key = 'processed_label' if self.step ==\"preliminary\" else 'label'\n",
    "\n",
    "        image_shape = d[image_key].shape\n",
    "        dim_before_resize = torch.tensor(image_shape)\n",
    "\n",
    "        dim_before_resize_dict_key = \"dim_before_resize_preliminary\" if self.step ==\"preliminary\" else \"dim_before_resize_final\"\n",
    "        spatial_size_info_dict_key = \"spatial_size_info_preliminary\" if self.step ==\"preliminary\" else \"spatial_size_info_final\"\n",
    "\n",
    "        original_spatial_dim = torch.tensor([image_shape[1], image_shape[2]], dtype=torch.int16)\n",
    "\n",
    "        #print(self.step)\n",
    "        #print(image_shape)\n",
    "\n",
    "        #print(d[image_key])\n",
    "        #print(self.resize)\n",
    "        d[image_key] = self.resize(d[image_key])\n",
    "        if self.step!='preliminary':\n",
    "            d['processed_image'] = self.resize(d['processed_image'])\n",
    "        if d['has_mask']:\n",
    "            d[label_key] = self.resize(d[label_key])\n",
    "            if self.step!='preliminary':\n",
    "                d['processed_label'] = self.resize(d['processed_label'])\n",
    "\n",
    "        d[spatial_size_info_dict_key] = torch.cat((d[spatial_size_info_dict_key], original_spatial_dim), dim=0)\n",
    "        d[dim_before_resize_dict_key ] = torch.cat((d[dim_before_resize_dict_key ], dim_before_resize), dim=0)\n",
    "\n",
    "        if self.step=='preliminary':\n",
    "            d['preliminary_target_size'] = torch.cat((d['preliminary_target_size'], torch.tensor(self.spatial_size)), dim=0)\n",
    "        return d\n",
    "\n",
    "class FilterByDim(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)\n",
    "\n",
    "        # Define a simple filter function for demonstration\n",
    "    def filter_by_dim(self, data):\n",
    "        # Implement your filtering condition here\n",
    "        # For example, filter out images with a certain property:\n",
    "\n",
    "        # OLD VALUE WAS 180\n",
    "        keep_sample = data['processed_label'].shape[1] > 100\n",
    "        return keep_sample\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Apply the filter function to determine if the sample should be kept\n",
    "        keep_sample = torch.tensor([self.filter_by_dim(data)])\n",
    "        data['keep_sample'] = torch.cat((data['keep_sample'], keep_sample), dim=0)\n",
    "        return data\n",
    "\n",
    "class FilterByMean(MapTransform):\n",
    "    def __init__(self, keys, mean_threshold, start_pos):\n",
    "        super().__init__(keys)\n",
    "        self.mean_threshold = mean_threshold\n",
    "        self.start_pos = start_pos\n",
    "\n",
    "    def filter_by_mean(self, data):\n",
    "        #print(data['processed_image'].std())\n",
    "        keep_sample = data['processed_image'][:,self.start_pos:,:].std() > self.mean_threshold\n",
    "        return keep_sample\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Apply the filter function to determine if the sample should be kept\n",
    "        keep_sample = torch.tensor([self.filter_by_mean(data)])\n",
    "        if data['keep_sample'] and not keep_sample:\n",
    "            data['keep_sample'] = keep_sample\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\"\"\"              median_smooth_radius=2,\n",
    "                 hist_norm_num_bins=20,\n",
    "                 rm_thorax_threshold=140,\n",
    "                 rm_thorax_margin=80,\n",
    "                 rm_bottom_threshold=140,\n",
    "                 rm_bottom_margin=20,\n",
    "                 threshold_black_threshold=500,\n",
    "                 threshold_black_value=0,\n",
    "                 trim_sides_threshold=50000,\n",
    "                 trim_sides_tolerance=40,\n",
    "                 bbox_size=(384,384),\n",
    "                 pad_spatial_size=[512,512],\n",
    "                 target_size_preliminary=(512,512),\n",
    "                 target_size_final = (256,256),\n",
    "                 mode='train',\n",
    "                 has_mask=True,\n",
    "                 **kwargs):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                     def __init__(self,\n",
    "                 median_smooth_radius=2,\n",
    "                 hist_norm_num_bins=20,\n",
    "                 rm_thorax_threshold=10, #80\n",
    "                 rm_thorax_margin=40,\n",
    "                 rm_bottom_threshold=140,\n",
    "                 rm_bottom_margin=50,\n",
    "                 threshold_black_threshold=800,\n",
    "                 threshold_black_value=0,\n",
    "                 trim_sides_threshold=20000,\n",
    "                 trim_sides_tolerance=20,\n",
    "                 bbox_size=(384,384),\n",
    "                 pad_spatial_size=[512,512],\n",
    "                 target_size_preliminary=(512,512),\n",
    "                 target_size_final = (256,256),\n",
    "                 mode='train',\n",
    "                 has_mask=True,\n",
    "                 **kwargs):\"\"\"\n",
    "\n",
    "\n",
    "import copy\n",
    "class Preprocess(MapTransform):\n",
    "\n",
    "    def __init__(self,\n",
    "                 subtrahend=48.85898971557617, \n",
    "                 divisor=123.9007568359375,\n",
    "                 median_smooth_radius=2,\n",
    "                 hist_norm_num_bins=20,\n",
    "                 rm_thorax_threshold=150, #80\n",
    "                 rm_thorax_margin=60,\n",
    "                 rm_bottom_threshold=120,\n",
    "                 rm_bottom_margin=50,\n",
    "                 threshold_black_threshold=1300,\n",
    "                 threshold_black_value=0,\n",
    "                 trim_sides_threshold=20000,\n",
    "                 trim_sides_tolerance=20,\n",
    "                 bbox_size=(384,384),\n",
    "                 pad_spatial_size=[512,512],\n",
    "                 target_size_preliminary=(512,512),\n",
    "                 target_size_final = (256,256),\n",
    "                 mode='train',\n",
    "                 subtracted_images_path_prefixes = None,\n",
    "                 has_mask=True, dataset = \"private\",\n",
    "                 get_patches = False,\n",
    "                 get_boundaryloss=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        super(Preprocess, self).__init__(**kwargs)\n",
    "\n",
    "        self.subtracted_images_path_prefixes = subtracted_images_path_prefixes\n",
    "        self.subtrahend = subtrahend\n",
    "        self.divisor = divisor\n",
    "        self.get_patches = get_patches\n",
    "\n",
    "        if dataset == \"BRADM\":\n",
    "            self.subtracted_images = True\n",
    "        else:\n",
    "            if subtracted_images_path_prefixes:\n",
    "                self.subtracted_images = True\n",
    "            else:\n",
    "                self.subtracted_images = False\n",
    "\n",
    "            if get_patches:\n",
    "                mean_threshold = 50\n",
    "                start_pos = 75\n",
    "            else:\n",
    "                mean_threshold = 35\n",
    "                start_pos = 40\n",
    "            \n",
    "        self.median_smooth = MedianSmooth(keys=['processed_image'], radius=median_smooth_radius)\n",
    "        self.histogram_normalized = HistogramNormalized(keys=['processed_image'], num_bins = hist_norm_num_bins)\n",
    "        self.remove_thorax = RemoveThorax(keys=['processed_image', 'processed_label'], threshold= rm_thorax_threshold, margin=rm_thorax_margin)\n",
    "        self.remove_bottom = RemoveBottom(keys=['processed_image', 'processed_label'], threshold = rm_bottom_threshold,margin=rm_bottom_margin)\n",
    "        self.threshold_black = ThresholdBlack(keys=['processed_image'], threshold = threshold_black_threshold, value=threshold_black_value)\n",
    "        self.trim_sides = TrimSides(keys=['processed_image','processed_label'], threshold=trim_sides_threshold, tolerance=trim_sides_tolerance)\n",
    "        self.normalize = monai.transforms.NormalizeIntensityd(keys=['image'], subtrahend  = self.subtrahend, divisor=self.divisor)\n",
    "        self.bbox_split = BoundingBoxSplit(keys=['image','label'], bbox_size=bbox_size)\n",
    "        self.pad = monai.transforms.SpatialPadd(keys=['image','label'], spatial_size=pad_spatial_size)\n",
    "        self.convert3d =  monai.transforms.RepeatChanneld(keys=['image'], repeats=3)\n",
    "        self.convert3denhanced = Convert3DEnhanced(keys=['image', 'label'])\n",
    "        self.normalizedSingleChannel = NormalizedSingleChannel(keys=['image'], channel_index = 0)\n",
    "        self.prepare_image = PrepareSample(keys=None, target_size=target_size_preliminary, subtracted_images=self.subtracted_images, patches=self.get_patches)\n",
    "        self.adaptiveCropBreasts = AdaptiveCropBreasts2(keys=['processed_image','processed_label'])\n",
    "        self.crop = CropToSquare(keys=['processed_image','processed_label'], shrink_factor=15)\n",
    "        self.resizePreliminary = Resize(step='preliminary', keys=['image', 'label'], spatial_size=target_size_preliminary)\n",
    "        self.resizePreliminaryCleanImg = monai.transforms.Resized(keys=['image', 'label'],spatial_size=target_size_preliminary, mode='nearest-exact')\n",
    "        self.resizeFinal = Resize(step='final', keys=['image', 'label'], spatial_size=target_size_final)\n",
    "        self.foreground_mask = monai.transforms.ForegroundMaskd(keys=['image'], invert=True)\n",
    "        self.filterbyDim = FilterByDim(keys=['processed_image', 'processed_label'])\n",
    "        self.filterbyMean = FilterByMean(keys=['image', 'label'], mean_threshold=mean_threshold, start_pos = start_pos)\n",
    "        self.has_mask = has_mask\n",
    "        self.randVFlip = RandFlipd(prob=0.3, spatial_axis=1, keys=['image', 'label'])\n",
    "        self.randGridDistortion = RandGridDistortiond(keys=['image', 'label'], num_cells=10, prob=0.3, distort_limit=(-0.1, 0.1))\n",
    "        self.randHistShift = RandHistogramShiftd(keys=['image', 'label'], num_control_points=15, prob=0.3)\n",
    "        self.randomRotate = RandRotated(range_x=0.15, keys = ['image', 'label'], prob=0.3, keep_size=True)\n",
    "        self.enhance =EnhanceLesionsSelective(keys=['image'])\n",
    "        self.get_boundaryloss = get_boundaryloss\n",
    "        \n",
    "\n",
    "        self.train_functions_patches=[\n",
    "            self.resizePreliminary,\n",
    "            self.median_smooth,\n",
    "            self.histogram_normalized,\n",
    "            self.resizePreliminaryCleanImg,\n",
    "            self.remove_thorax,\n",
    "            self.adaptiveCropBreasts,\n",
    "            self.remove_bottom,\n",
    "            self.filterbyDim,\n",
    "            self.crop,\n",
    "            self.prepare_image,\n",
    "            self.resizeFinal,\n",
    "            self.filterbyMean,\n",
    "            self.normalize,\n",
    "            \n",
    "        ]\n",
    "\n",
    "        self.train_functions_no_patches=[\n",
    "            self.resizePreliminary,\n",
    "            self.median_smooth,\n",
    "            self.histogram_normalized,\n",
    "            self.resizePreliminaryCleanImg,\n",
    "            self.remove_thorax,\n",
    "            self.remove_bottom,\n",
    "            self.filterbyDim,\n",
    "            self.crop,\n",
    "            self.prepare_image,\n",
    "            self.resizeFinal,\n",
    "            self.filterbyMean,\n",
    "            self.normalize,\n",
    "        ]\n",
    "\n",
    "\n",
    "        self.test_functions_patches=[\n",
    "            self.resizePreliminary,\n",
    "            self.median_smooth,\n",
    "            self.histogram_normalized,\n",
    "            self.resizePreliminaryCleanImg,\n",
    "            self.remove_thorax,\n",
    "            self.adaptiveCropBreasts,\n",
    "            self.remove_bottom,\n",
    "            self.filterbyDim,\n",
    "            self.crop,\n",
    "            self.prepare_image,\n",
    "            self.resizeFinal,\n",
    "            self.filterbyMean,\n",
    "            self.normalize,\n",
    "        ]\n",
    "\n",
    "        self.test_functions_no_patches=[\n",
    "            self.resizePreliminary,\n",
    "            self.median_smooth,\n",
    "            self.histogram_normalized,\n",
    "            self.resizePreliminaryCleanImg,\n",
    "            self.remove_thorax,\n",
    "            self.remove_bottom,\n",
    "            self.filterbyDim,\n",
    "            self.crop,\n",
    "            self.prepare_image,\n",
    "            self.resizeFinal,\n",
    "            self.filterbyMean,\n",
    "            self.normalize,\n",
    "        ]\n",
    "\n",
    "        self.statistics_functions_patches=[\n",
    "            self.resizePreliminary,\n",
    "            self.median_smooth,\n",
    "            self.histogram_normalized,\n",
    "            self.resizePreliminaryCleanImg,\n",
    "            self.remove_thorax,\n",
    "            self.adaptiveCropBreasts,  # todo separate original image restoring!\n",
    "            self.remove_bottom,\n",
    "            self.filterbyDim,\n",
    "            self.crop,\n",
    "            self.prepare_image,\n",
    "            self.resizeFinal,\n",
    "            self.filterbyMean,\n",
    "        ]\n",
    "\n",
    "        self.statistics_functions_no_patches=[\n",
    "            self.resizePreliminary,\n",
    "            self.median_smooth,\n",
    "            self.histogram_normalized,\n",
    "            self.resizePreliminaryCleanImg,\n",
    "            self.remove_thorax,\n",
    "            self.remove_bottom,\n",
    "            self.filterbyDim,\n",
    "            self.crop,\n",
    "            self.prepare_image,\n",
    "            self.resizeFinal,\n",
    "            self.filterbyMean,\n",
    "        ]\n",
    "\n",
    "        if mode=='train':\n",
    "          if self.get_patches:\n",
    "              self.transforms = Compose(self.train_functions_patches)\n",
    "          else:\n",
    "              self.transforms = Compose(self.train_functions_no_patches)\n",
    "            \n",
    "        elif mode==\"test\":\n",
    "            if self.get_patches:\n",
    "                  self.transforms = Compose(self.test_functions_patches)\n",
    "            else:\n",
    "                self.transforms = Compose(self.test_functions_no_patches)\n",
    "        elif mode=='statistics':\n",
    "            if self.get_patches:\n",
    "                self.transforms = Compose(self.statistics_functions_patches)\n",
    "            else:\n",
    "                self.transforms = Compose(self.statistics_functions_no_patches)\n",
    "                \n",
    "                \n",
    "        self.disttransform = dist_map_transform([1,1], 2)\n",
    "                \n",
    "\n",
    "    def __call__(self, data):\n",
    "\n",
    "        data['thorax_crop_coords']=torch.tensor([], dtype=torch.int16)\n",
    "        data['dim_before_thorax_crop']=torch.tensor([], dtype=torch.int16)\n",
    "\n",
    "        data['trim_breast_coords']=torch.tensor([],dtype=torch.int16)\n",
    "        data['dim_before_breast_crop']=torch.tensor([], dtype=torch.int16)\n",
    "\n",
    "        data['bottom_crop_coords']=torch.tensor([],dtype=torch.int16)\n",
    "        data['dim_before_bottom_crop']=torch.tensor([], dtype=torch.int16)\n",
    "\n",
    "        data['pad_post_crop_coords'] = torch.tensor([], dtype=torch.int16)\n",
    "\n",
    "        data['crop_coords']=torch.tensor([],dtype=torch.int16)\n",
    "        data['dim_before_crop']=torch.tensor([], dtype=torch.int16)\n",
    "        data['preliminary_target_size']=torch.tensor([], dtype=torch.int16)\n",
    "\n",
    "        #data['trim_coords']=torch.tensor([], dtype=torch.int16)\n",
    "\n",
    "        data['spatial_size_info_preliminary']=torch.tensor([],dtype=torch.int16)\n",
    "        data['spatial_size_info_final']=torch.tensor([],dtype=torch.int16)\n",
    "        data['dim_before_resize_final']=torch.tensor([], dtype=torch.int16)\n",
    "        data['dim_before_resize_preliminary']=torch.tensor([], dtype=torch.int16)\n",
    "\n",
    "        data['processed_image']=copy.deepcopy(data['image'])\n",
    "        data['original_image'] = copy.deepcopy(data['image'])\n",
    "\n",
    "        data['keep_sample'] = torch.tensor([], dtype=torch.bool)\n",
    "        data['has_mask'] = monai.data.MetaTensor(self.has_mask)\n",
    "\n",
    "        if self.has_mask:\n",
    "             data['processed_label']=copy.deepcopy(data['label'])\n",
    "             data['original_label'] = copy.deepcopy(data['label'])\n",
    "\n",
    "        else:\n",
    "            data['processed_label']= np.zeros_like(data['image'])\n",
    "            data['original_label'] =  np.zeros_like(data['image'])\n",
    "            data['label'] =  np.zeros_like(data['image'])\n",
    "\n",
    "            data['processed_label']=monai.data.MetaTensor(data['image'])\n",
    "            data['original_label'] =  monai.data.MetaTensor(data['image'])\n",
    "            data['label'] =  monai.data.MetaTensor(data['image'])\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        if self.subtracted_images_path_prefixes:\n",
    "            #path = path.replace(\"Dataset-arrays-images-only-4\", \"Dataset-arrays-images-only\")\n",
    "            pfx1, pfx2 = self.subtracted_images_path_prefixes[0], self.subtracted_images_path_prefixes[1]\n",
    "            data['image_meta_dict']['subtracted_filename_or_obj'] = data['image_meta_dict']['filename_or_obj'].replace(pfx1, pfx2)\n",
    "            if self.has_mask:\n",
    "                data['label_meta_dict']['subtracted_filename_or_obj'] = data['label_meta_dict']['filename_or_obj'].replace(pfx1, pfx2)\n",
    "\n",
    "        \n",
    "        data = self.transforms(data)\n",
    "\n",
    "        if self.get_patches:\n",
    "            c, h, w = data[0]['image'].shape\n",
    "    \n",
    "            data[0]['image_meta_dict']['spatial_shape'] = np.array([h,w])\n",
    "            data[0]['label_meta_dict']['spatial_shape'] = np.array([h,w])\n",
    "    \n",
    "            data[1]['image_meta_dict']['spatial_shape'] = np.array([h,w])\n",
    "            data[1]['label_meta_dict']['spatial_shape'] = np.array([h,w])\n",
    "    \n",
    "            data[0]['image_meta_dict']['original_channel_dim'] = monai.data.MetaTensor(data[0]['image_meta_dict']['original_channel_dim'])\n",
    "            data[0]['label_meta_dict']['original_channel_dim'] =  monai.data.MetaTensor(data[0]['label_meta_dict']['original_channel_dim'])\n",
    "    \n",
    "            data[1]['image_meta_dict']['original_channel_dim'] =  monai.data.MetaTensor(data[1]['image_meta_dict']['original_channel_dim'])\n",
    "            data[1]['label_meta_dict']['original_channel_dim'] =  monai.data.MetaTensor(data[1]['label_meta_dict']['original_channel_dim'])\n",
    "    \n",
    "            \"\"\"del data[0]['image_meta_dict']\n",
    "            del data[0]['label_meta_dict']\n",
    "    \n",
    "            del data[1]['image_meta_dict']\n",
    "            del data[1]['label_meta_dict']\"\"\"\n",
    "            \n",
    "            #del data[0]['processed_image']\n",
    "            #del data[1]['processed_image']\n",
    "\n",
    "                \n",
    "            del data[0]['original_image']\n",
    "            del data[1]['original_image']\n",
    "            \n",
    "    \n",
    "            if self.has_mask:\n",
    "                #del data[0]['processed_label']\n",
    "                #del data[1]['processed_label']\n",
    "\n",
    "                if self.get_boundaryloss:\n",
    "\n",
    "                    boundary = self.disttransform(data[0]['label'][0])\n",
    "                    data[0]['boundary']=boundary \n",
    "        \n",
    "                    boundary = self.disttransform(data[1]['label'][0])\n",
    "                    data[1]['boundary']=boundary \n",
    "                del data[0]['original_label']\n",
    "                del data[1]['original_label']\n",
    "\n",
    "                data[0]['has_mass'] = monai.data.MetaTensor(np.sum(data[0]['label']) != 0)\n",
    "                data[1]['has_mass'] = monai.data.MetaTensor(np.sum(data[1]['label']) != 0)\n",
    "\n",
    "    \n",
    "\n",
    "        else:\n",
    "            c, h, w = data['image'].shape\n",
    "    \n",
    "            data['image_meta_dict']['spatial_shape'] = np.array([h,w])\n",
    "            data['label_meta_dict']['spatial_shape'] = np.array([h,w])\n",
    "    \n",
    "            data['image_meta_dict']['original_channel_dim'] = monai.data.MetaTensor(data['image_meta_dict']['original_channel_dim'])\n",
    "            data['label_meta_dict']['original_channel_dim'] =  monai.data.MetaTensor(data['label_meta_dict']['original_channel_dim'])\n",
    "    \n",
    "            #del data['processed_image']\n",
    "    \n",
    "            if self.has_mask:\n",
    "                #del data['processed_label']\n",
    "\n",
    "                if self.get_boundaryloss:\n",
    "                    boundary = self.disttransform(data['label'][0])\n",
    "                    data['boundary']=boundary \n",
    "                data['has_mass'] = monai.data.MetaTensor(np.sum(data['label']) != 0)\n",
    "                del data['original_label']\n",
    "    \n",
    "            del data['original_image']\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSSES & METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to identify the axis for aggregation\n",
    "def identify_axis(shape):\n",
    "    if len(shape) == 5:\n",
    "        return [2, 3, 4]\n",
    "    elif len(shape) == 4:\n",
    "        return [2, 3]\n",
    "    else:\n",
    "        raise ValueError('Shape of tensor is neither 2D or 3D.')\n",
    "\n",
    "\n",
    "\n",
    "def compute_dice_from_metrics(tp, fp, tn, fn, reduction='micro', exclude_empty=False, exclude_empty_only_gt=False, return_std=False):\n",
    "    dice_denominator = 2 * tp + fp + fn\n",
    "    dice_numerator = 2 * tp\n",
    "\n",
    "    if reduction == 'micro':\n",
    "        dice_score = dice_numerator.sum() / dice_denominator.sum()\n",
    "        if return_std:\n",
    "            # Standard deviation does not apply in micro mode because it is a single value\n",
    "            return dice_score, torch.tensor(0.0)\n",
    "        return dice_score\n",
    "\n",
    "    elif reduction == 'micro-imagewise':\n",
    "        dice_per_sample = dice_numerator / dice_denominator\n",
    "        if exclude_empty:\n",
    "            dice_per_sample = torch.where(dice_denominator == 0, torch.tensor(float('nan')), dice_per_sample)\n",
    "            if exclude_empty_only_gt:\n",
    "                # Exclude samples where there are no positives in the ground truth\n",
    "                exclude_mask = (tp + fn) == 0  # Ground truth empty cases\n",
    "                dice_per_sample = torch.where(exclude_mask, torch.tensor(float('nan')), dice_per_sample)\n",
    "\n",
    "            mean_dice = torch.nanmean(dice_per_sample)\n",
    "            if return_std:\n",
    "                std_dice = np.nanstd(dice_per_sample)\n",
    "                return mean_dice, std_dice\n",
    "            return mean_dice\n",
    "        else:\n",
    "            dice_per_sample = torch.where(dice_denominator == 0, 1, dice_per_sample)\n",
    "            mean_dice = torch.nanmean(dice_per_sample)\n",
    "            if return_std:\n",
    "                std_dice = np.nanstd(dice_per_sample)\n",
    "                return mean_dice, std_dice\n",
    "            return mean_dice\n",
    "\n",
    "    elif reduction == 'none':\n",
    "        dice_score = dice_numerator / dice_denominator\n",
    "        if exclude_empty:\n",
    "            dice_score = torch.where(dice_denominator == 0, torch.tensor(float('nan')), dice_score)\n",
    "            if exclude_empty_only_gt:\n",
    "                # Exclude samples where there are no positives in the ground truth\n",
    "                exclude_mask = (tp + fn) == 0  # Ground truth empty cases\n",
    "                dice_score = torch.where(exclude_mask, torch.tensor(float('nan')), dice_score)\n",
    "        else:\n",
    "            dice_score = torch.where(dice_denominator == 0, 1, dice_score)\n",
    "        return dice_score  # Standard deviation is not applicable for 'none' reduction\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Reduction method must be either 'micro', 'micro-imagewise', or 'none'.\")\n",
    "\n",
    "    # Ensure Dice scores are within the [0, 1] range\n",
    "    dice_score = torch.clamp(dice_score, min=0, max=1)\n",
    "\n",
    "    return dice_score \n",
    "\n",
    "def compute_iou_from_metrics(tp, fp, tn, fn, reduction='micro', exclude_empty=False, exclude_empty_only_gt=False, return_std=False):\n",
    "    denominator = tp + fp + fn\n",
    "    with torch.no_grad():  # Avoid tracking these operations in the autograd graph\n",
    "        if reduction == 'micro':\n",
    "            # Sum the counts across all samples and compute IoU\n",
    "            iou = tp.sum() / denominator.sum()\n",
    "            if return_std:\n",
    "                # Standard deviation does not apply in micro mode because it is a single value\n",
    "                return iou, torch.tensor(0.0)\n",
    "            return iou\n",
    "\n",
    "        elif reduction == 'micro-imagewise':\n",
    "            # Avoid division by zero; set IoU to NaN for samples with denominator == 0\n",
    "            valid = denominator != 0\n",
    "            iou_per_sample = torch.zeros_like(tp, dtype=torch.float)\n",
    "            iou_per_sample[valid] = tp[valid] / denominator[valid]\n",
    "            if exclude_empty:\n",
    "                if exclude_empty_only_gt:\n",
    "                    # Exclude samples with no positives in the ground truth\n",
    "                    exclude_mask = (tp + fn) == 0\n",
    "                    iou_per_sample = torch.where(exclude_mask, torch.tensor(float('nan')), iou_per_sample)\n",
    "\n",
    "                # Compute mean and optionally standard deviation only for valid samples\n",
    "                mean_iou = torch.nanmean(iou_per_sample[valid])\n",
    "                if return_std:\n",
    "                    std_iou = np.nanstd(iou_per_sample[valid])\n",
    "                    return mean_iou, std_iou\n",
    "                return mean_iou\n",
    "            else:\n",
    "                iou_per_sample[~valid] = torch.tensor(1.0)  # Set invalid samples to 1\n",
    "                mean_iou = torch.mean(iou_per_sample)\n",
    "                if return_std:\n",
    "                    std_iou = torch.std(iou_per_sample)\n",
    "                    return mean_iou, std_iou\n",
    "                return mean_iou\n",
    "\n",
    "        elif reduction == 'none':\n",
    "            # Compute IoU for each sample, handling division by zero\n",
    "            iou = torch.zeros_like(tp, dtype=torch.float)\n",
    "            valid = denominator != 0\n",
    "            iou[valid] = tp[valid] / denominator[valid]\n",
    "            if exclude_empty:\n",
    "                iou[~valid] = torch.tensor(float('nan'))  # Mark invalid samples as NaN\n",
    "                if exclude_empty_only_gt:\n",
    "                    # Exclude samples with no positives in the ground truth\n",
    "                    exclude_mask = (tp + fn) == 0\n",
    "                    iou = torch.where(exclude_mask, torch.tensor(float('nan')), iou)\n",
    "            else:\n",
    "                iou[~valid] = torch.tensor(1.0)  # Optionally: Set a default value for invalid samples if not excluding them\n",
    "            return iou  # Standard deviation is not applicable for 'none' reduction\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Reduction method must be either 'micro', 'micro-imagewise', or 'none'.\")\n",
    "\n",
    "    return iou  # Return IoU, handle std outside this condition if needed\n",
    "\n",
    "def compute_iou(y_true, y_pred, class_id, reduction='micro', exclude_empty=False):\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union for a specific class\n",
    "\n",
    "    Args:\n",
    "    y_true (torch.Tensor): batch of ground truth, 4D tensor (first dimension is batch size)\n",
    "    y_pred (torch.Tensor): batch of prediction, 4D tensor (first dimension is batch size)\n",
    "    class_id (int): the class to compute IoU for\n",
    "    reduction (str): the method of reduction across the batch, can be 'micro' or 'micro image-wise'\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: IoU score\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_iou_single(y_true_single, y_pred_single, class_id_single, exclude_empty=False):\n",
    "        y_true_class = torch.where(y_true_single == class_id_single, 1, 0)\n",
    "        y_pred_class = torch.where(y_pred_single == class_id_single, 1, 0)\n",
    "\n",
    "        intersection = torch.logical_and(y_true_class, y_pred_class)\n",
    "        union = torch.logical_or(y_true_class, y_pred_class)\n",
    "\n",
    "        union_sum = torch.sum(union)\n",
    "        if union_sum == 0:\n",
    "            if exclude_empty:\n",
    "                iou_score = float('nan')\n",
    "            else:\n",
    "                iou_score = 1.0\n",
    "        else:\n",
    "            iou_score = torch.sum(intersection).float() / union_sum.float()\n",
    "\n",
    "        return iou_score\n",
    "\n",
    "    assert reduction in ['micro', 'micro_image_wise'], \"Reduction method should be either 'micro' or 'micro_image_wise'\"\n",
    "\n",
    "    if reduction == 'micro':\n",
    "        y_true = y_true.view(-1)\n",
    "        y_pred = y_pred.view(-1)\n",
    "        return torch.tensor(compute_iou_single(y_true, y_pred, class_id,exclude_empty)).float()\n",
    "\n",
    "    elif reduction == 'micro_image_wise':\n",
    "        iou_scores = torch.tensor([compute_iou_single(y, p, class_id, exclude_empty) for y, p in zip(y_true, y_pred)], dtype=torch.float32)\n",
    "        return torch.nanmean(iou_scores)  # Using nanmean to ignore NaN values\n",
    "\n",
    "\n",
    "\n",
    "def compute_dice_score(y_true, y_pred, class_id=1, reduction='micro', exclude_empty=False):\n",
    "    \"\"\"\n",
    "    Compute Dice Score for a specific class with reduction options, for input tensors in HxWxB format using PyTorch.\n",
    "    \n",
    "    Args:\n",
    "    y_true (torch.Tensor): Ground truth, a 3D tensor (height, width, batch size).\n",
    "    y_pred (torch.Tensor): Predictions, a 3D tensor (height, width, batch size).\n",
    "    class_id (int): The class ID for which to compute the Dice Score.\n",
    "    reduction (str): Method of reduction across the batch, either 'micro' or 'micro_image_wise'.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The Dice Score.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_dice_score_single(y_true_single, y_pred_single, class_id_single, exclude_empty=False):\n",
    "        y_true_class = (y_true_single == class_id_single).float()\n",
    "        y_pred_class = (y_pred_single == class_id_single).float()\n",
    "\n",
    "        intersection = torch.sum(y_true_class * y_pred_class)\n",
    "        union = torch.sum(y_true_class) + torch.sum(y_pred_class)\n",
    "\n",
    "        if union == 0:\n",
    "            if exclude_empty:\n",
    "                dice_score = torch.tensor(float('nan'))  # Assuming need to handle NaN explicitly\n",
    "            else:\n",
    "                dice_score = torch.tensor(1.0)  # Assuming perfect IoU score when both prediction and GT are empty\n",
    "        else:\n",
    "            dice_score = (2. * intersection) / (union)\n",
    "        \n",
    "        return dice_score\n",
    "\n",
    "    if reduction == 'micro':\n",
    "        y_true_flat = y_true.view(-1)\n",
    "        y_pred_flat = y_pred.view(-1)\n",
    "        dice_score = torch.tensor(compute_dice_score_single(y_true_flat, y_pred_flat, class_id, exclude_empty)).float()\n",
    "        return dice_score\n",
    "\n",
    "    elif reduction == 'micro_image_wise':\n",
    "        dice_scores = torch.tensor([compute_dice_score_single(y, p, class_id, exclude_empty) for y, p in zip(y_true, y_pred)], dtype=torch.float32)\n",
    "        return torch.nanmean(dice_scores)  # Using nanmean to ignore NaN values in case of empty classes\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Reduction method should be either 'micro' or 'micro_image_wise'\")\n",
    "\n",
    "def compute_mean_precision(tp, fp, fn, tn):\n",
    "    \"\"\"\n",
    "    Compute the mean precision for binary classification across two classes.\n",
    "\n",
    "    Args:\n",
    "        tp (torch.Tensor): True Positives, tensor of shape (B, 1).\n",
    "        fp (torch.Tensor): False Positives, tensor of shape (B, 1).\n",
    "        tn (torch.Tensor): True Negatives, tensor of shape (B, 1).\n",
    "        fn (torch.Tensor): False Negatives, tensor of shape (B, 1).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The mean precision across classes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Precision for class 1\n",
    "    precision_class_1 = torch.div(tp, tp + fp)\n",
    "    precision_class_1[torch.isnan(precision_class_1)] = 1  # Handle division by zero\n",
    "\n",
    "    # Precision for class 0 (inverting perspective)\n",
    "    precision_class_0 = torch.div(tn, tn + fn)\n",
    "    precision_class_0[torch.isnan(precision_class_0)] = 1  # Handle division by zero\n",
    "\n",
    "    # Mean precision across both classes\n",
    "    mean_precision = (precision_class_1 + precision_class_0) / 2\n",
    "\n",
    "    # Average across the batch\n",
    "    mean_precision = torch.mean(mean_precision)\n",
    "\n",
    "    return mean_precision\n",
    "\n",
    "def compute_mean_recall(tp, fp, fn, tn):\n",
    "    \"\"\"\n",
    "    Compute the mean recall for binary classification across two classes.\n",
    "\n",
    "    Args:\n",
    "        tp (torch.Tensor): True Positives, tensor of shape (B, 1).\n",
    "        fp (torch.Tensor): False Positives, tensor of shape (B, 1).\n",
    "        tn (torch.Tensor): True Negatives, tensor of shape (B, 1).\n",
    "        fn (torch.Tensor): False Negatives, tensor of shape (B, 1).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The mean recall across classes.\n",
    "    \"\"\"\n",
    "    recall_class_1 = torch.div(tp, tp + fn)\n",
    "    recall_class_1[torch.isnan(recall_class_1)] = 1  # Handle division by zero\n",
    "\n",
    "    recall_class_0 = torch.div(tn, tn + fp)\n",
    "    recall_class_0[torch.isnan(recall_class_0)] = 1  # Handle division by zero\n",
    "\n",
    "    mean_recall = (recall_class_1 + recall_class_0) / 2\n",
    "    mean_recall = torch.mean(mean_recall)\n",
    "\n",
    "    return mean_recall\n",
    "\n",
    "\n",
    "def compute_dice_score_from_cm(tp, fp, fn, tn, reduction='micro', exclude_empty=False):\n",
    "    # Convert to float for division\n",
    "    tp = tp.float()\n",
    "    fp = fp.float()\n",
    "    fn = fn.float()\n",
    "    \n",
    "    if reduction == 'micro':\n",
    "        # Sum across all classes and samples for micro averaging\n",
    "        tp_sum = tp.sum()\n",
    "        fp_sum = fp.sum()\n",
    "        fn_sum = fn.sum()\n",
    "        \n",
    "        # Compute Dice score, handling division by zero\n",
    "        denominator = 2 * tp_sum + fp_sum + fn_sum\n",
    "        dice_score = 2 * tp_sum / denominator if denominator != 0 else torch.tensor(1.0)\n",
    "        \n",
    "    elif reduction == 'micro-imagewise':\n",
    "        # Compute Dice Score per sample, then average across samples\n",
    "        denominator = 2 * tp + fp + fn\n",
    "        valid = denominator != 0\n",
    "        dice_scores = torch.zeros_like(tp)\n",
    "        dice_scores[valid] = 2 * tp[valid] / denominator[valid]\n",
    "\n",
    "        if exclude_empty:\n",
    "            dice_scores[~valid] = torch.tensor(float('nan'))  # Assuming perfect score for invalid cases\n",
    "            dice_score = dice_scores.nanmean(dim=0)  # Average across samples\n",
    "        else:\n",
    "            dice_scores[~valid] = torch.tensor(1.0)  # Assuming perfect score for invalid cases\n",
    "            dice_score = dice_scores.mean(dim=0)  # Average across samples\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Reduction method must be either 'micro' or 'micro-imagewise'\")\n",
    "    \n",
    "    return dice_score\n",
    "\n",
    "def ra_iou(tp, fp, fn, beta=0.5):\n",
    "    \n",
    "    ra_iou = tp / (tp + beta * fn + fp + 1e-6)  # Adding epsilon to avoid division by zero\n",
    "    \n",
    "    return ra_iou.mean()\n",
    "\n",
    "# Helper function to identify the axis for aggregation\n",
    "def identify_axis(shape):\n",
    "    if len(shape) == 5:\n",
    "        return [2, 3, 4]\n",
    "    elif len(shape) == 4:\n",
    "        return [2, 3]\n",
    "    else:\n",
    "        raise ValueError('Shape of tensor is neither 2D or 3D.')\n",
    "\n",
    "# Asymmetric Focal Loss for single-channel output\n",
    "class AsymmetricFocalLoss(nn.Module):\n",
    "    def __init__(self, delta=0.7, gamma=2., epsilon=1e-07):\n",
    "        super(AsymmetricFocalLoss, self).__init__()\n",
    "        self.delta = delta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, logits, y_true):\n",
    "        y_pred = torch.sigmoid(logits)  # Applying sigmoid to convert logits to probabilities\n",
    "        y_pred = torch.clamp(y_pred, self.epsilon, 1. - self.epsilon)\n",
    "        cross_entropy = -y_true * torch.log(y_pred) - (1 - y_true) * torch.log(1 - y_pred)\n",
    "\n",
    "        # Calculate the loss for positive and negative classes\n",
    "        pos_loss = torch.pow(1 - y_pred, self.gamma) * cross_entropy\n",
    "        neg_loss = torch.pow(y_pred, self.gamma) * cross_entropy\n",
    "\n",
    "        # Weighted sum of the losses\n",
    "        loss = torch.mean((self.delta * pos_loss) + ((1 - self.delta) * neg_loss))\n",
    "        return loss\n",
    "\n",
    "# Asymmetric Focal Tversky Loss for single-channel output\n",
    "class AsymmetricFocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, delta=0.7, gamma=0.75, epsilon=1e-07):\n",
    "        super(AsymmetricFocalTverskyLoss, self).__init__()\n",
    "        self.delta = delta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, logits, y_true):\n",
    "        y_pred = torch.sigmoid(logits)  # Applying sigmoid to convert logits to probabilities\n",
    "        y_pred = torch.clamp(y_pred, self.epsilon, 1. - self.epsilon)\n",
    "        axis = identify_axis(y_true.size())\n",
    "\n",
    "        tp = torch.sum(y_true * y_pred, axis=axis)\n",
    "        fn = torch.sum(y_true * (1 - y_pred), axis=axis)\n",
    "        fp = torch.sum((1 - y_true) * y_pred, axis=axis)\n",
    "\n",
    "        tversky_index = (tp + self.epsilon) / (tp + self.delta * fn + (1 - self.delta) * fp + self.epsilon)\n",
    "        loss = (1 - tversky_index) * torch.pow(1 - tversky_index, -self.gamma)\n",
    "\n",
    "        return torch.mean(loss)\n",
    "\n",
    "# Asymmetric Unified Focal Loss for single-channel output\n",
    "class AsymmetricUnifiedFocalLoss(nn.Module):\n",
    "    def __init__(self, weight=0.5, delta=0.6, gamma=0.2):\n",
    "        super(AsymmetricUnifiedFocalLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.delta = delta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, y_true):\n",
    "        asymmetric_ftl = AsymmetricFocalTverskyLoss(delta=self.delta, gamma=self.gamma)(logits, y_true)\n",
    "        asymmetric_fl = AsymmetricFocalLoss(delta=self.delta, gamma=self.gamma)(logits, y_true)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            return (self.weight * asymmetric_ftl) + ((1 - self.weight) * asymmetric_fl)\n",
    "        else:\n",
    "            return asymmetric_ftl + asymmetric_fl\n",
    "\n",
    "class SurfaceLossBinary(nn.Module):\n",
    "    def __init__(self, idc):\n",
    "        super(SurfaceLossBinary, self).__init__()\n",
    "        # Self.idc is used to filter out some classes of the target mask. Use fancy indexing\n",
    "        self.idc = idc\n",
    "        print(f\"Initialized {self.__class__.__name__} with {idc}\")\n",
    "\n",
    "    def forward(self, probs, dist_maps):\n",
    "\n",
    "        pc = probs[:, 0, ...].type(torch.float32)\n",
    "        dc = dist_maps[:, 1, ...].type(torch.float32)\n",
    "\n",
    "        multipled = einsum(\"bwh,bwh->bwh\", pc, dc)\n",
    "\n",
    "        loss = multipled.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "class CABFL(nn.Module):\n",
    "    def __init__(self, idc, weight_aufl=0.5, delta=0.6, gamma=0.2):\n",
    "        super(CABFL, self).__init__()\n",
    "        self.boundaryLoss = SurfaceLossBinary(idc=idc)\n",
    "        self.aufl =  AsymmetricUnifiedFocalLoss(delta=delta, gamma=gamma, weight=weight_aufl)\n",
    "        self.alpha = 0.01\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def norm_distmap(self,distmap):\n",
    "        _m: float = torch.abs(distmap).max()\n",
    "        return distmap / _m\n",
    "\n",
    "    def forward(self, logits, probs, dist_maps, gts, current_epoch):\n",
    "        if current_epoch != self.current_epoch:\n",
    "            self.current_epoch = current_epoch\n",
    "            self.alpha = min(self.alpha+0.01, 0.99)\n",
    "\n",
    "        bl = self.boundaryLoss(probs, self.norm_distmap(dist_maps))\n",
    "        aufl = self.aufl(logits, gts)\n",
    "        \n",
    "        return (1-self.alpha)*aufl + self.alpha*bl\n",
    "\n",
    "def compute_dice_score_npy(y_true, y_pred, class_id=1, reduction='micro',exclude_empty=False):\n",
    "    \"\"\"\n",
    "    Compute Dice Score for a specific class with reduction options, for input arrays in HxWxB format using NumPy.\n",
    "    \n",
    "    Args:\n",
    "    y_true (np.array): Ground truth, a 3D array (height, width, batch size).\n",
    "    y_pred (np.array): Predictions, a 3D array (height, width, batch size).\n",
    "    class_id (int): The class ID for which to compute the Dice Score.\n",
    "    reduction (str): Method of reduction across the batch, either 'micro' or 'micro-imagewise'.\n",
    "    \n",
    "    Returns:\n",
    "    float: The Dice Score.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_dice_score_single(y_true_single, y_pred_single, class_id_single, exclude_empty=False):\n",
    "        y_true_class = (y_true_single == class_id_single).astype(np.float32)\n",
    "        y_pred_class = (y_pred_single == class_id_single).astype(np.float32)\n",
    "\n",
    "        intersection = np.sum(y_true_class * y_pred_class)\n",
    "        union = np.sum(y_true_class) + np.sum(y_pred_class)\n",
    "\n",
    "        if union == 0:\n",
    "            # Both prediction and ground truth are empty for this class\n",
    "            if exclude_empty:\n",
    "                dice_score =  float('nan')\n",
    "            else:\n",
    "                dice_score= torch.tensor(1.0)  # Assuming perfect IoU score when both prediction and GT are empty\n",
    "        else:\n",
    "            dice_score = (2. * intersection) / (union)  # Adding epsilon to avoid division by zero\n",
    "        \n",
    "        return dice_score\n",
    "\n",
    "    if reduction == 'micro':\n",
    "        # Reshape to combine height, width, and batch into a single dimension\n",
    "        y_true_flat = y_true.reshape(-1)\n",
    "        y_pred_flat = y_pred.reshape(-1)\n",
    "        dice_score = compute_dice_score_single(y_true_flat, y_pred_flat, class_id, exclude_empty)\n",
    "        return dice_score\n",
    "\n",
    "    elif reduction == 'micro-imagewise':\n",
    "        # Compute Dice Score for each image separately and then average\n",
    "        dice_scores = np.array([compute_dice_score_single(y_true[:, :, i], y_pred[:, :, i], class_id,exclude_empty) for i in range(y_true.shape[2])])\n",
    "        return np.nanmean(dice_scores)  # Using nanmean to ignore NaN values in case of empty classes\n",
    "\n",
    "    elif reduction == 'none':\n",
    "        # Compute and return Dice Score for each image separately without averaging\n",
    "        return np.array([compute_dice_score_single(y_true[:, :, i], y_pred[:, :, i], class_id, exclude_empty) for i in range(y_true.shape[2])])\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Reduction method should be either 'micro' or 'micro-imagewise'\")\n",
    "\n",
    "def compute_iou_npy(y_true, y_pred, class_id=1, reduction='micro', exclude_empty=False):\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union for a specific class using NumPy.\n",
    "\n",
    "    Args:\n",
    "    y_true (np.ndarray): batch of ground truth, 4D tensor (first dimension is batch size)\n",
    "    y_pred (np.ndarray): batch of prediction, 4D tensor (first dimension is batch size)\n",
    "    class_id (int): the class to compute IoU for\n",
    "    reduction (str): the method of reduction across the batch, can be 'micro' or 'micro-imagewise'\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: IoU score\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_iou_single(y_true_single, y_pred_single, class_id_single, exclude_empty=False):\n",
    "        y_true_class = np.where(y_true_single == class_id_single, 1, 0)\n",
    "        y_pred_class = np.where(y_pred_single == class_id_single, 1, 0)\n",
    "\n",
    "        intersection = np.logical_and(y_true_class, y_pred_class)\n",
    "        union = np.logical_or(y_true_class, y_pred_class)\n",
    "\n",
    "        union_sum = np.sum(union)\n",
    "        if union_sum == 0:\n",
    "            # Both prediction and ground truth are empty\n",
    "            if exclude_empty:\n",
    "                iou_score = float('nan')\n",
    "            else:\n",
    "                iou_score = 1.0\n",
    "        else:\n",
    "            iou_score = np.sum(intersection).astype(float) / union_sum.astype(float)\n",
    "\n",
    "        return iou_score\n",
    "\n",
    "    assert reduction in ['micro', 'micro-imagewise'], \"Reduction method should be either 'micro' or 'micro_image_wise'\"\n",
    "\n",
    "    if reduction == 'micro':\n",
    "        y_true = y_true.reshape(-1)\n",
    "        y_pred = y_pred.reshape(-1)\n",
    "        return compute_iou_single(y_true, y_pred, class_id, exclude_empty)\n",
    "\n",
    "    elif reduction == 'micro-imagewise':\n",
    "        iou_scores = np.array([compute_iou_single(y, p, class_id, exclude_empty) for y, p in zip(y_true, y_pred)], dtype=np.float32)\n",
    "        return np.nanmean(iou_scores)  # Using nanmean to ignore NaN values\n",
    "\n",
    "\n",
    "\n",
    "def select_slices_based_on_gt(gt_volume, pred_volume):\n",
    "    # Select slices with positive pixels in ground truth\n",
    "    positive_slices = (gt_volume > 0)\n",
    "    return gt_volume[positive_slices], pred_volume[positive_slices]\n",
    "\n",
    "def compute_classwise_volumetric_iou(gt_volume, pred_volume, num_classes, exclude_empty=False):\n",
    "    # Initialize IoU scores for each class\n",
    "    iou_scores = np.zeros(num_classes)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        # Consider only voxels belonging to the current class\n",
    "        gt_class_bool = gt_volume == class_id\n",
    "        pred_class_bool = pred_volume == class_id\n",
    "\n",
    "        intersection = np.logical_and(gt_class_bool, pred_class_bool)\n",
    "        union = np.logical_or(gt_class_bool, pred_class_bool)\n",
    "\n",
    "        # Calculate IoU, setting it to 1 if the union is zero\n",
    "        if np.sum(union) == 0:\n",
    "            if exclude_empty:\n",
    "                iou_scores[class_id] = float('nan')\n",
    "            else:\n",
    "                iou_scores[class_id] = 1.0\n",
    "        else:\n",
    "            iou_scores[class_id] = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    return iou_scores\n",
    "\n",
    "\n",
    "def ra_iou(tp, fp, fn, beta=0.5):\n",
    "    \n",
    "    ra_iou = tp / (tp + beta * fn + fp + 1e-6)  # Adding epsilon to avoid division by zero\n",
    "    \n",
    "    return ra_iou.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedDataLoader(DataLoader):\n",
    "    def __init__(self, dataset1, dataset2, batch_size, shuffle, worker_init_fn, generator, drop_last,augment=False):\n",
    "        paired_dataset = PairedDataset(dataset1, dataset2,augment=augment)\n",
    "        super().__init__(paired_dataset, batch_size=batch_size, shuffle=shuffle, worker_init_fn=worker_init_fn, generator=generator, drop_last=drop_last)\n",
    "\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, dataset1, dataset2, augment):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.augmentations = Compose([monai.transforms.RandHistogramShiftd(keys=['image'], prob=0.2, num_control_points=4), \n",
    "                                      monai.transforms.RandRotated(keys=['image', 'label'],mode='nearest-exact', range_x=[0.1, 0.1], prob=0.3),\n",
    "                                      monai.transforms.RandZoomd(keys=['image', 'label'],mode='nearest-exact', min_zoom = 1.3, max_zoom = 1.5, prob=0.3),\n",
    "                                      #monai.transforms.RandCoarseDropoutd(keys=['image', 'label'], prob=0.3, holes=20, spatial_size=20, fill_value =0)\n",
    "                                     ]\n",
    "            \n",
    "        \n",
    "        )\n",
    "\n",
    "        self.augment=augment\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset1), len(self.dataset2))\n",
    "\n",
    "    def filter_data(self,data):\n",
    "        if not data['keep_sample']:\n",
    "            data['image']=monai.data.MetaTensor(torch.zeros_like(data['image']))\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data1 = self.dataset1[idx]\n",
    "        data2 = self.dataset2[idx]\n",
    "\n",
    "        data2_flat = [self.filter_data(item) for item in data2]\n",
    "        data2_1 = data2_flat[0]\n",
    "        data2_2 = data2_flat[1]\n",
    "\n",
    "        if self.augment:\n",
    "            self.augmentations.set_random_state(seed=idx)\n",
    "\n",
    "            data2_1 = {\n",
    "                'image' : copy.deepcopy(data2_1['image']),\n",
    "                'label' : copy.deepcopy(data2_1['label']),\n",
    "                'boundary' : copy.deepcopy(data2_1['boundary'])\n",
    "            }\n",
    "            data2_2 = {\n",
    "                'image' : copy.deepcopy(data2_2['image']),\n",
    "                'label' : copy.deepcopy(data2_2['label']),\n",
    "                'boundary' : copy.deepcopy(data2_2['boundary'])\n",
    "            }\n",
    "            data1 = {\n",
    "                'image' : copy.deepcopy(data1['image']),\n",
    "                'label' : copy.deepcopy(data1['label']),\n",
    "                'boundary' : copy.deepcopy(data1['boundary'])\n",
    "            }                                    \n",
    "                                        \n",
    "            data2_1 = self.augmentations(data2_1)\n",
    "            \n",
    "            self.augmentations.set_random_state(seed=idx)\n",
    "            data2_2 = self.augmentations(data2_2)\n",
    "            \n",
    "            self.augmentations.set_random_state(seed=idx)\n",
    "            data1= self.augmentations(data1)\n",
    "\n",
    "        return data1, data2_1, data2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionGate, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, F_int, use_attention=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, mid_channels, kernel_size=2, stride=2)\n",
    "        # Adjust the attention gate to handle the combined skip connections\n",
    "        if use_attention:\n",
    "            self.attention = AttentionGate(F_g=mid_channels, F_l=mid_channels, F_int=F_int)\n",
    "        self.conv = ConvBlock(mid_channels + mid_channels, out_channels)  # Adjust for concatenated skip connection size\n",
    "        self.use_attention=use_attention\n",
    "\n",
    "    def forward(self, x, combined_skip):\n",
    "\n",
    "        x = self.up(x)\n",
    "        # Apply attention to the combined skip connections\n",
    "        if self.use_attention:\n",
    "            combined_skip = self.attention(g=x, x=combined_skip)\n",
    "        \n",
    "        x = torch.cat([x, combined_skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction_ratio, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction_ratio, in_channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_pooled = self.avg_pool(x).view(b, c)\n",
    "        max_pooled = self.max_pool(x).view(b, c)\n",
    "        avg_out = self.fc2(self.relu(self.fc1(avg_pooled)))\n",
    "        max_out = self.fc2(self.relu(self.fc1(max_pooled)))\n",
    "        out = self.sigmoid(avg_out + max_out).view(b, c, 1, 1)\n",
    "        return x * out\n",
    "        \n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([max_out, avg_out], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class FeatureFusionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, reduction=16):\n",
    "        super(FeatureFusionBlock, self).__init__()\n",
    "        self.channel_attention_local = ChannelAttention(in_channels)\n",
    "        self.spatial_attention_local = SpatialAttention()\n",
    "\n",
    "        self.channel_attention_global = ChannelAttention(in_channels)\n",
    "        self.spatial_attention_global = SpatialAttention()\n",
    "        \n",
    "        self.fusion_conv = nn.Conv2d(in_channels*3, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, global_feat, local_feat1, local_feat2):\n",
    "        # Apply channel attention to each feature map\n",
    "        global_ca = global_feat * self.channel_attention_global(global_feat)\n",
    "        local_ca1 = local_feat1 * self.channel_attention_local(local_feat1)\n",
    "        local_ca2 = local_feat2 * self.channel_attention_local(local_feat2)\n",
    "\n",
    "        # Apply spatial attention to each feature map\n",
    "        global_sa = global_ca * self.spatial_attention_global(global_ca)\n",
    "        local_sa1 = local_ca1 * self.spatial_attention_local(local_ca1)\n",
    "        local_sa2 = local_ca2 * self.spatial_attention_local(local_ca2)\n",
    "\n",
    "        # Concatenate the feature maps\n",
    "        fused_features = torch.cat((global_sa, local_sa1, local_sa2), dim=1)\n",
    "        \n",
    "        # Fuse them using a convolutional layer\n",
    "        fused_features = self.fusion_conv(fused_features)\n",
    "        fused_features = self.relu(fused_features)\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "class SimpleFeatureFusionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimpleFeatureFusionBlock, self).__init__()\n",
    "        # Since we're concatenating three feature maps, the input to the fusion_conv will be 3 times in_channels\n",
    "        self.fusion_conv = nn.Conv2d(in_channels * 3, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, global_feat, local_feat1, local_feat2):\n",
    "        # Concatenate the feature maps along the channel dimension\n",
    "        fused_features = torch.cat((global_feat, local_feat1, local_feat2), dim=1)\n",
    "        \n",
    "        # Apply a convolutional layer to reduce dimensions\n",
    "        fused_features = self.fusion_conv(fused_features)\n",
    "        fused_features = self.relu(fused_features)\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "\n",
    "\n",
    "class MultiInputUNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, use_simple_fusion=False, use_decoder_attention=True):\n",
    "        super(MultiInputUNet, self).__init__()\n",
    "\n",
    "        if use_simple_fusion:\n",
    "            self.fusion_skip1 = SimpleFeatureFusionBlock(64, 64)\n",
    "            self.fusion_skip2 = SimpleFeatureFusionBlock(128, 128)\n",
    "            self.fusion_skip3 = SimpleFeatureFusionBlock(256, 256)\n",
    "            self.fusion_skip4 = SimpleFeatureFusionBlock(512, 512)\n",
    "        else:\n",
    "          # Initialize fusion blocks with channel reduction\n",
    "            self.fusion_skip1 = FeatureFusionBlock(64, 64)\n",
    "            self.fusion_skip2 = FeatureFusionBlock(128, 128)\n",
    "            self.fusion_skip3 = FeatureFusionBlock(256, 256)\n",
    "            self.fusion_skip4 = FeatureFusionBlock(512,512)\n",
    "        \n",
    "        # Encoders for each input stream\n",
    "        self.encoder1 = nn.ModuleList([EncoderBlock(n_channels, 64), EncoderBlock(64, 128), EncoderBlock(128, 256), EncoderBlock(256,512)])\n",
    "        self.encoder2 = nn.ModuleList([EncoderBlock(n_channels, 64), EncoderBlock(64, 128), EncoderBlock(128, 256), EncoderBlock(256,512)])\n",
    "        self.encoder3 = nn.ModuleList([EncoderBlock(n_channels, 64), EncoderBlock(64, 128), EncoderBlock(128, 256), EncoderBlock(256,512)])\n",
    "\n",
    "        if use_simple_fusion:\n",
    "            self.deep_feature_fusion = SimpleFeatureFusionBlock(512, 512)\n",
    "        else:\n",
    "             self.deep_feature_fusion = FeatureFusionBlock(512,512)\n",
    "        \n",
    "        \n",
    "        # Decoder Blocks\n",
    "        self.decoder1 = DecoderBlock(512,512, 256,256, use_attention=use_decoder_attention)  # Input channels adjusted for merged features\n",
    "        self.decoder2 = DecoderBlock(256, 256, 128, 128,use_attention=use_decoder_attention)  # Input channels adjusted for merged features\n",
    "        self.decoder3 = DecoderBlock(128, 128, 64, 64,use_attention=use_decoder_attention)\n",
    "        self.decoder4 = DecoderBlock(64, 64, 32, 32,use_attention=use_decoder_attention)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        # Process each input through its respective encoders\n",
    "        skips1, p1 = self.process_through_encoders(x1, self.encoder1)\n",
    "        skips2, p2 = self.process_through_encoders(x2, self.encoder2)\n",
    "        skips3, p3 = self.process_through_encoders(x3, self.encoder3)\n",
    "\n",
    "        fused_skips1 = self.fusion_skip1(skips1[0], skips2[0], skips3[0])\n",
    "        fused_skips2 = self.fusion_skip2(skips1[1], skips2[1], skips3[1])\n",
    "        fused_skips3 = self.fusion_skip3(skips1[2], skips2[2], skips3[2])\n",
    "        fused_skips4 = self.fusion_skip4(skips1[3], skips2[3], skips3[3])\n",
    "\n",
    "        fused_features = self.deep_feature_fusion(p1, p2, p3)\n",
    "        \n",
    "        # Decode the combined features\n",
    "        # Note: Attention mechanism applies to concatenated skip connections from corresponding layers of each input\n",
    "        d1 = self.decoder1(fused_features, fused_skips4)\n",
    "        d2 = self.decoder2(d1, fused_skips3)\n",
    "        d3 = self.decoder3(d2, fused_skips2)\n",
    "        d4 = self.decoder4(d3, fused_skips1)\n",
    "\n",
    "        return self.final_conv(d4)\n",
    "\n",
    "    def process_through_encoders(self, x, encoders):\n",
    "        skips = []\n",
    "        p = x\n",
    "        for encoder in encoders:\n",
    "            x, p = encoder(p)\n",
    "            skips.append(x)\n",
    "        return skips, p  # Reverse skips for correct order in decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(self,input_nbr,label_nbr):\n",
    "        super(SegNet, self).__init__()\n",
    "\n",
    "        batchNorm_momentum = 0.1\n",
    "\n",
    "        self.conv11 = nn.Conv2d(input_nbr, 64, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n",
    "        self.conv22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "        self.conv32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "        self.conv33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn33 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn43 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn51 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn52 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn53 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv53d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn53d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv52d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn52d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv51d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn51d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv43d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn43d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv42d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn42d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv41d = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.bn41d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv33d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn33d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "        self.conv32d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn32d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "        self.conv31d = nn.Conv2d(256,  128, kernel_size=3, padding=1)\n",
    "        self.bn31d = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv22d = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn22d = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n",
    "        self.conv21d = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn21d = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv12d = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn12d = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n",
    "        self.conv11d = nn.Conv2d(64, label_nbr, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Stage 1\n",
    "        x11 = F.relu(self.bn11(self.conv11(x)))\n",
    "        x12 = F.relu(self.bn12(self.conv12(x11)))\n",
    "        x1p, id1 = F.max_pool2d(x12,kernel_size=2, stride=2,return_indices=True)\n",
    "\n",
    "        # Stage 2\n",
    "        x21 = F.relu(self.bn21(self.conv21(x1p)))\n",
    "        x22 = F.relu(self.bn22(self.conv22(x21)))\n",
    "        x2p, id2 = F.max_pool2d(x22,kernel_size=2, stride=2,return_indices=True)\n",
    "\n",
    "        # Stage 3\n",
    "        x31 = F.relu(self.bn31(self.conv31(x2p)))\n",
    "        x32 = F.relu(self.bn32(self.conv32(x31)))\n",
    "        x33 = F.relu(self.bn33(self.conv33(x32)))\n",
    "        x3p, id3 = F.max_pool2d(x33,kernel_size=2, stride=2,return_indices=True)\n",
    "\n",
    "        # Stage 4\n",
    "        x41 = F.relu(self.bn41(self.conv41(x3p)))\n",
    "        x42 = F.relu(self.bn42(self.conv42(x41)))\n",
    "        x43 = F.relu(self.bn43(self.conv43(x42)))\n",
    "        x4p, id4 = F.max_pool2d(x43,kernel_size=2, stride=2,return_indices=True)\n",
    "\n",
    "        # Stage 5\n",
    "        x51 = F.relu(self.bn51(self.conv51(x4p)))\n",
    "        x52 = F.relu(self.bn52(self.conv52(x51)))\n",
    "        x53 = F.relu(self.bn53(self.conv53(x52)))\n",
    "        x5p, id5 = F.max_pool2d(x53,kernel_size=2, stride=2,return_indices=True)\n",
    "\n",
    "\n",
    "        # Stage 5d\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "        x5d = F.max_unpool2d(x5p, id5, kernel_size=2, stride=2)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        x53d = F.relu(self.bn53d(self.conv53d(x5d)))\n",
    "        x52d = F.relu(self.bn52d(self.conv52d(x53d)))\n",
    "        x51d = F.relu(self.bn51d(self.conv51d(x52d)))\n",
    "\n",
    "        # Stage 4d\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "        x4d = F.max_unpool2d(x51d, id4, kernel_size=2, stride=2)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        x43d = F.relu(self.bn43d(self.conv43d(x4d)))\n",
    "        x42d = F.relu(self.bn42d(self.conv42d(x43d)))\n",
    "        x41d = F.relu(self.bn41d(self.conv41d(x42d)))\n",
    "\n",
    "        # Stage 3d\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "        x3d = F.max_unpool2d(x41d, id3, kernel_size=2, stride=2)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        x33d = F.relu(self.bn33d(self.conv33d(x3d)))\n",
    "        x32d = F.relu(self.bn32d(self.conv32d(x33d)))\n",
    "        x31d = F.relu(self.bn31d(self.conv31d(x32d)))\n",
    "\n",
    "        # Stage 2d\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "        x2d = F.max_unpool2d(x31d, id2, kernel_size=2, stride=2)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        x22d = F.relu(self.bn22d(self.conv22d(x2d)))\n",
    "        x21d = F.relu(self.bn21d(self.conv21d(x22d)))\n",
    "\n",
    "        # Stage 1d\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "        x1d = F.max_unpool2d(x21d, id1, kernel_size=2, stride=2)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        x12d = F.relu(self.bn12d(self.conv12d(x1d)))\n",
    "        x11d = self.conv11d(x12d)\n",
    "\n",
    "        return x11d\n",
    "\n",
    "    def load_from_segnet(self, model_path):\n",
    "        s_dict = self.state_dict()# create a copy of the state dict\n",
    "        th = torch.load(model_path).state_dict() # load the weigths\n",
    "        # for name in th:\n",
    "            # s_dict[corresp_name[name]] = th[name]\n",
    "        self.load_state_dict(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCFFNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "    \n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "    \n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class NeighConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.neigh_conv = nn.Conv2d(in_channels, out_channels, kernel_size=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.neigh_conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "def compute_class_weight(labels):\n",
    "    \"\"\"\n",
    "    Compute class weights for binary classification with labels of shape (B, 1, C, H, W).\n",
    "    \"\"\"\n",
    "    labels = labels.cpu()\n",
    "    labels = labels.squeeze(1).reshape(-1)  # Flatten labels to (B * C * H * W)\n",
    "    unique_labels = np.unique(labels.numpy())  # Get unique class labels\n",
    "    \n",
    "    class_freq = {}\n",
    "    for label in unique_labels:\n",
    "        class_freq[label] = np.sum(labels.numpy() == label)  # Count occurrences of each class\n",
    "    \n",
    "    max_value = max(class_freq.values())  # Get the largest class frequency\n",
    "    \n",
    "    class_weights = {}\n",
    "    for label in unique_labels:\n",
    "        class_weights[label] = max_value / class_freq[label]  # Compute class weights\n",
    "    \n",
    "    # Convert to a PyTorch tensor\n",
    "    return torch.tensor([class_weights[label] for label in sorted(class_weights.keys())]).float()\n",
    "\n",
    "\n",
    "\n",
    "def CrossEntropy2d(input, target, weight=None, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Binary cross entropy for 2D inputs with shape (B, H, W).\n",
    "    `weight` is applied per class.\n",
    "    \"\"\"\n",
    "    # Ensure input and target have the correct dimensions\n",
    "    target = target.float()  # Convert target to float for BCEWithLogitsLoss\n",
    "    \n",
    "    # Handle class weights\n",
    "    if weight is not None:\n",
    "        # Expand weight to match the shape of the input/target\n",
    "        weight = weight[target.long()]  # Select weights based on the class labels\n",
    "    \n",
    "    return F.binary_cross_entropy_with_logits(input, target, weight=weight, reduction=reduction)\n",
    "\n",
    "\n",
    "class FcnnFnet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(FcnnFnet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.fc1 = OutConv(256, n_classes)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.fc2 = OutConv(128, n_classes)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.fc3 = OutConv(64, n_classes)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "        self.neigh = NeighConv(n_classes, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        activations = []\n",
    "        out_fc = []\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        out1 = self.fc1(x)\n",
    "        out_fc.append(out1)\n",
    "        activations.append(x)\n",
    "        x = self.up2(x, x3)\n",
    "        out2 = self.fc2(x)\n",
    "        out_fc.append(out2)\n",
    "        activations.append(x)\n",
    "        x = self.up3(x, x2)\n",
    "        out3 = self.fc3(x)\n",
    "        out_fc.append(out3)\n",
    "        activations.append(x)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        neigh = self.neigh(logits)[:,:,:-1, :-1]\n",
    "        return logits, out_fc, neigh, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skinny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filters_count(level: int, base_filters: int) -> int:\n",
    "    \"\"\"Calculate the number of filters at each level.\"\"\"\n",
    "    return base_filters * (2 ** (level - 1))\n",
    "\n",
    "###############################################################################\n",
    "# Inception Module\n",
    "###############################################################################\n",
    "class InceptionModule(nn.Module):\n",
    "    \"\"\"Inception module with 4 parallel branches.\"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 activation=nn.LeakyReLU(0.3, inplace=True)):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        branch_ch = out_channels // 4\n",
    "\n",
    "        # Branch 1: 1×1\n",
    "        self.branch1_1x1 = nn.Conv2d(in_channels, branch_ch, kernel_size=1)\n",
    "\n",
    "        # Branch 2: 1×1 -> 3×3\n",
    "        self.branch2_1x1 = nn.Conv2d(in_channels, branch_ch, kernel_size=1)\n",
    "        self.branch2_3x3 = nn.Conv2d(branch_ch, branch_ch, kernel_size=3, padding=1)\n",
    "\n",
    "        # Branch 3: 1×1 -> 5×5\n",
    "        self.branch3_1x1 = nn.Conv2d(in_channels, branch_ch, kernel_size=1)\n",
    "        self.branch3_5x5 = nn.Conv2d(branch_ch, branch_ch, kernel_size=5, padding=2)\n",
    "\n",
    "        # Branch 4: MaxPool -> 1×1\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.branch4_1x1 = nn.Conv2d(in_channels, branch_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.activation(self.branch1_1x1(x))\n",
    "        b2 = self.activation(self.branch2_3x3(self.branch2_1x1(x)))\n",
    "        b3 = self.activation(self.branch3_5x5(self.branch3_1x1(x)))\n",
    "        b4 = self.activation(self.branch4_1x1(self.pool(x)))\n",
    "        return torch.cat([b1, b2, b3, b4], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class SkinnyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    'Skinny' U-Net\n",
    "    \"\"\"\n",
    "    def __init__(self, image_channels=3, levels=6, base_filters=19):\n",
    "        super().__init__()\n",
    "        self.levels = levels\n",
    "        self.base_filters = base_filters\n",
    "        self.activation = nn.LeakyReLU(0.3, inplace=True)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Contracting (Down) Path\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.down_convs = nn.ModuleList()\n",
    "        self.down_inceptions = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "\n",
    "        # Record the actual # of output channels at each level\n",
    "        self.down_channels = []\n",
    "\n",
    "        in_ch = image_channels\n",
    "        for lvl in range(1, levels + 1):\n",
    "            out_ch = get_filters_count(lvl, base_filters)\n",
    "\n",
    "            # Conv -> BN -> Activation\n",
    "            conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
    "            bn = nn.BatchNorm2d(out_ch)\n",
    "            self.down_convs.append(nn.Sequential(conv, bn, self.activation))\n",
    "\n",
    "            # Inception\n",
    "            inc = InceptionModule(in_channels=out_ch, out_channels=out_ch,\n",
    "                                  activation=self.activation)\n",
    "            self.down_inceptions.append(inc)\n",
    "\n",
    "            # Actual output channels after Inception\n",
    "            actual_out_ch = (out_ch // 4) * 4\n",
    "            self.down_channels.append(actual_out_ch)\n",
    "\n",
    "            # MaxPool except at the last (bottom) level\n",
    "            if lvl < levels:\n",
    "                self.pools.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            else:\n",
    "                self.pools.append(None)\n",
    "\n",
    "            # Update input channels for the next level\n",
    "            in_ch = actual_out_ch\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Expanding (Up) Path\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.up_inceptions = nn.ModuleList()\n",
    "\n",
    "        for lvl in range(levels, 1, -1):\n",
    "            # Skip connection channels from contracting path\n",
    "            skip_channels = self.down_channels[lvl - 2]\n",
    "            # Bottom (upsampled) feature channels from the contracting path\n",
    "            bottom_channels = self.down_channels[lvl - 1]\n",
    "            # Total input channels = skip + bottom\n",
    "            total_channels = bottom_channels + skip_channels\n",
    "\n",
    "            # Create convolutional layers for expanding path\n",
    "            up_conv = nn.Conv2d(total_channels, skip_channels, kernel_size=3, padding=1)\n",
    "            bn_up = nn.BatchNorm2d(skip_channels)\n",
    "            self.up_convs.append(nn.Sequential(up_conv, bn_up, self.activation))\n",
    "\n",
    "            # Create InceptionModule for expanding path\n",
    "            up_inception = InceptionModule(in_channels=skip_channels, out_channels=skip_channels,\n",
    "                                           activation=self.activation)\n",
    "            self.up_inceptions.append(up_inception)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Final: 1×1 or 3×3 conv to 1 channel\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.final_conv = nn.Conv2d(self.down_channels[0], 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1) Contracting path:\n",
    "           for i in [0..levels-1]:\n",
    "             x -> conv -> inception -> store skip\n",
    "             if i < levels-1: pool\n",
    "        2) Expanding path:\n",
    "           for i in [levels-1..1]:\n",
    "             upsample\n",
    "             cat with skip\n",
    "             conv + inception\n",
    "        3) Final conv -> sigmoid\n",
    "        \"\"\"\n",
    "        # -------------------------------------\n",
    "        # Contracting Path\n",
    "        # -------------------------------------\n",
    "        downs = []\n",
    "        curr = x\n",
    "        for i in range(self.levels):\n",
    "            curr = self.down_convs[i](curr)       # Conv\n",
    "            curr = self.down_inceptions[i](curr) # Inception\n",
    "            downs.append(curr)\n",
    "            if self.pools[i] is not None:\n",
    "                curr = self.pools[i](curr)\n",
    "\n",
    "        # -------------------------------------\n",
    "        # Expanding Path\n",
    "        # -------------------------------------\n",
    "        for i, (up_conv, up_inception) in enumerate(zip(self.up_convs, self.up_inceptions)):\n",
    "            curr = F.interpolate(curr, scale_factor=2, mode='nearest')\n",
    "            skip = downs[-(i + 2)]  # Skip connection from contracting path\n",
    "            curr = torch.cat([curr, skip], dim=1)  # Concatenate along channels\n",
    "            curr = up_conv(curr)  # Apply Conv -> BN -> Activation\n",
    "            curr = up_inception(curr)  # Apply Inception\n",
    "\n",
    "        # -------------------------------------\n",
    "        # Final Layer\n",
    "        # -------------------------------------\n",
    "        curr = self.final_conv(curr)\n",
    "        return curr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class BreastModel2(L.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 arch,\n",
    "                 encoder_name,\n",
    "                 in_channels,\n",
    "                 out_classes,\n",
    "                 batch_size,\n",
    "                 len_train_loader,\n",
    "                 threshold = 0.4,\n",
    "                 t_loss = AsymmetricUnifiedFocalLoss(delta=0.4, gamma=0.1),\n",
    "                 boundaryloss=False,\n",
    "                 img_size = None,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.mode = \"base\"\n",
    "\n",
    "        if arch == 'base_unet':\n",
    "            self.model = UNet(n_channels=in_channels, n_classes = out_classes)\n",
    "\n",
    "        elif arch == \"segnet\":\n",
    "            self.model = SegNet(input_nbr=in_channels, label_nbr=out_classes)\n",
    "            \n",
    "        elif arch == \"swin_unetr\":\n",
    "            self.model = SwinUNETR(img_size = img_size, in_channels=in_channels, out_channels = out_classes, spatial_dims=2, use_v2=True, downsample=\"mergingv2\")\n",
    "\n",
    "        elif arch == \"unetplusplus\":\n",
    "            self.model = BasicUNetPlusPlus(in_channels=in_channels, out_channels = out_classes, spatial_dims=2)\n",
    "\n",
    "        elif arch == \"skinny\":\n",
    "            self.model = SkinnyNet(image_channels=in_channels, levels=6, base_filters=19)\n",
    "\n",
    "        elif arch == \"fcn_ffnet\":\n",
    "            self.model = FcnnFnet(n_channels=in_channels, n_classes=out_classes, bilinear=True)\n",
    "            self.mode = \"fcn_ffnet\"\n",
    "\n",
    "            \n",
    "        else:\n",
    "            aux_params = dict(\n",
    "                pooling = 'avg',  # one of 'avg', 'max'\n",
    "                dropout = 0.5,  # dropout ratio, default is None\n",
    "                activation = None,  # activation function, default is None\n",
    "                classes = out_classes,  # define number of output labels\n",
    "            )\n",
    "        \n",
    "            self.model = smp.create_model(\n",
    "                   arch, encoder_name=encoder_name,\n",
    "                    aux_params = aux_params, \n",
    "                   in_channels = in_channels, encoder_weights=None,**kwargs)\n",
    "\n",
    "        self.t_loss = t_loss\n",
    "        self.train_outputs = []\n",
    "        self.val_outputs = []\n",
    "        self.test_outputs = []\n",
    "        self.save_hyperparameters()\n",
    "        self.threshold = threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.len_train_loader = len_train_loader\n",
    "        self.augment_inference=False\n",
    "        self.val=80\n",
    "        self.boundaryloss = boundaryloss\n",
    "\n",
    "    def ttaug(self, model, image):\n",
    "        transforms = tta.Compose(\n",
    "        [\n",
    "            tta.HorizontalFlip(),\n",
    "            tta.VerticalFlip(),\n",
    "            tta.Rotate90(angles=[0, 180]),\n",
    "            #tta.Scale(scales=[1, 2, 4]),\n",
    "            #tta.Multiply(factors=[0.8]),        \n",
    "        ]\n",
    "        )\n",
    "    \n",
    "        model.eval()\n",
    "        merger = Merger(type=\"tsharpen\", n=len(transforms))\n",
    "        for transformer in transforms: # custom transforms or e.g. tta.aliases.d4_transform() \n",
    "        \n",
    "            # augment image\n",
    "            augmented_image = transformer.augment_image(image)\n",
    "            augmented_image = augmented_image.to(\"cuda\")\n",
    "    \n",
    "            model_output = model(augmented_image)[0]\n",
    "            \n",
    "            # reverse augmentation for mask and label\n",
    "            deaug_mask = transformer.deaugment_mask(model_output)\n",
    "            #deaug_mask = deaug_mask.sigmoid()\n",
    "            \n",
    "            merger.append(deaug_mask)\n",
    "    \n",
    "        masks = merger.result\n",
    "        #masks = masks.sigmoid()\n",
    "\n",
    "        return masks\n",
    "\n",
    "\n",
    "    def base_step(self,stage, image, mask, batch):\n",
    "        if stage=='test' and self.augment_inference:\n",
    "            prob_mask = self.ttaug(self.model, image)\n",
    "            logits_mask = self.forward(image)\n",
    "            if isinstance(logits_mask, List):\n",
    "                logits_mask = logits_mask[0]\n",
    "\n",
    "        else:\n",
    "            logits_mask = self.forward(image)\n",
    "            if isinstance(logits_mask, List):\n",
    "                logits_mask = logits_mask[0]\n",
    "            prob_mask = logits_mask.sigmoid()\n",
    "            \n",
    "        if self.boundaryloss:\n",
    "            dist_map = batch[\"boundary\"].to(\"cuda\")\n",
    "            t_loss = self.t_loss(logits_mask, prob_mask, dist_map, mask, self.current_epoch)\n",
    "\n",
    "        else:\n",
    "            t_loss = self.t_loss(logits_mask, mask)\n",
    "            \n",
    "        loss = t_loss\n",
    "\n",
    "        return loss, prob_mask\n",
    "\n",
    "    def fcn_ffnet_step(self,stage, image, mask, batch):\n",
    "        # Ensure mask has the correct shape\n",
    "        mask = mask.data.cpu().numpy()  # Convert to numpy\n",
    "        mask = np.squeeze(mask, axis=1)  # Remove the singleton channel dimension (B, H, W)\n",
    "        \n",
    "        # Resize and prepare the masks at different scales\n",
    "        mask3 = np.stack([cv2.resize(m, dsize=(128, 128), interpolation=cv2.INTER_NEAREST) for m in mask], axis=0)\n",
    "        mask2 = np.stack([cv2.resize(m, dsize=(64, 64), interpolation=cv2.INTER_NEAREST) for m in mask], axis=0)\n",
    "        mask1 = np.stack([cv2.resize(m, dsize=(32, 32), interpolation=cv2.INTER_NEAREST) for m in mask], axis=0)\n",
    "        \n",
    "        # Convert resized masks back to tensors and adjust dimensions\n",
    "        mask3 = torch.from_numpy(mask3).unsqueeze(1).type(torch.LongTensor).cuda()  # (B, 1, 128, 128)\n",
    "        mask2 = torch.from_numpy(mask2).unsqueeze(1).type(torch.LongTensor).cuda()  # (B, 1, 64, 64)\n",
    "        mask1 = torch.from_numpy(mask1).unsqueeze(1).type(torch.LongTensor).cuda()  # (B, 1, 32, 32)\n",
    "        \n",
    "        # Prepare the original mask\n",
    "        mask = torch.from_numpy(mask).unsqueeze(1).type(torch.LongTensor).cuda()  # (B, 1, H, W)\n",
    "        \n",
    "        # Add weights for MLP loss\n",
    "        image = Variable(image.cuda())\n",
    "        mask = Variable(mask)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, out_fc, out_neigh = self.model(image)[:3]\n",
    "\n",
    "        # Compute losses\n",
    "        loss = self.t_loss(output, mask, weight=None)\n",
    "\n",
    "        loss_fc1 = CrossEntropy2d(out_fc[0], Variable(mask1), weight=compute_class_weight(mask1).cuda())\n",
    "        loss_fc2 = CrossEntropy2d(out_fc[1], Variable(mask2), weight=compute_class_weight(mask2).cuda())\n",
    "        loss_fc3 = CrossEntropy2d(out_fc[2], Variable(mask3), weight=compute_class_weight(mask3).cuda())\n",
    "        pairwise_loss = self.t_loss(out_neigh, mask, weight=None)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = (loss + loss_fc1 + loss_fc2 + loss_fc3) / 4 + pairwise_loss\n",
    "        \n",
    "        # Output probabilities\n",
    "        prob_mask = output.sigmoid()\n",
    "        \n",
    "        return loss, prob_mask\n",
    "\n",
    "\n",
    "            \n",
    "    def step(self, batch, batch_idx, stage, mode='base'):\n",
    "        \n",
    "        image = batch[\"image\"].to(\"cuda\")\n",
    "\n",
    "        assert image.ndim == 4\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask = batch[\"label\"].to(\"cuda\")\n",
    "        assert mask.ndim == 4\n",
    "\n",
    "        epsilon = 1e-6  # Define a small epsilon value for margin of error\n",
    "\n",
    "        assert mask.max() <= 1 + epsilon and mask.min() >= -epsilon\n",
    "\n",
    "        if mode == \"fcn_ffnet\":\n",
    "            loss, prob_mask= self.fcn_ffnet_step(stage, image, mask, batch)\n",
    "        else:\n",
    "            loss, prob_mask = self.base_step(stage, image, mask, batch)\n",
    "\n",
    "        pred_mask = (prob_mask > self.threshold).float()\n",
    "\n",
    "        if stage=='test' and self.augment_inference:\n",
    "            pred_mask = (prob_mask > self.threshold).int()\n",
    "            pred_mask = remove_far_masses_based_on_largest_mass(pred_mask, distance_threshold=10)\n",
    "            pred_mask = RemoveSmallObjects(connectivity=1, min_size=140)(pred_mask)\n",
    "            pred_mask = torch.Tensor(pred_mask).to(\"cuda\")\n",
    "\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode = \"binary\")\n",
    "\n",
    "        iou_per_image_mass = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 1,\n",
    "                                         reduction = \"micro_image_wise\")\n",
    "        iou_per_image_background = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 0,\n",
    "                                               reduction = \"micro_image_wise\")\n",
    "\n",
    "        iou_per_image_mass_no_empty = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 1,\n",
    "                                         reduction = \"micro_image_wise\", exclude_empty=True)\n",
    "        iou_per_image_background_no_empty = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 0,\n",
    "                                               reduction = \"micro_image_wise\",exclude_empty=True)\n",
    "\n",
    "        iou_per_dataset_mass = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 1, reduction = \"micro\")\n",
    "        iou_per_dataset_background = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 0, reduction = \"micro\")\n",
    "\n",
    "        iou_per_dataset_mass_no_empty = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 1, reduction = \"micro\",exclude_empty=True)\n",
    "        iou_per_dataset_background_no_empty = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 0, reduction = \"micro\",exclude_empty=True)\n",
    "\n",
    "        ###\n",
    "\n",
    "        dice_per_image_mass = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 1,\n",
    "                                         reduction = \"micro_image_wise\")\n",
    "        dice_per_image_background = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 0,\n",
    "                                               reduction = \"micro_image_wise\")\n",
    "\n",
    "        dice_per_image_mass_no_empty = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 1,\n",
    "                                         reduction = \"micro_image_wise\", exclude_empty=True)\n",
    "        dice_per_image_background_no_empty = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 0,\n",
    "                                               reduction = \"micro_image_wise\",exclude_empty=True)\n",
    "\n",
    "        dice_per_dataset_mass = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 1, reduction = \"micro\")\n",
    "        dice_per_dataset_background = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 0, reduction = \"micro\")\n",
    "\n",
    "        dice_per_dataset_mass_no_empty = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 1, reduction = \"micro\",exclude_empty=True)\n",
    "        dice_per_dataset_background_no_empty = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 0, reduction = \"micro\",exclude_empty=True)\n",
    "\n",
    "\n",
    "        ###\n",
    "\n",
    "        acc_per_image_mass = class_specific_accuracy_score(preds = mask, targets = pred_mask, class_id = 1,\n",
    "                                                           averaging = \"micro_image_wise\")\n",
    "        acc_per_image_background = class_specific_accuracy_score(preds = mask, targets = pred_mask, class_id = 0,\n",
    "                                                                 averaging = \"micro_image_wise\")\n",
    "\n",
    "        acc_per_dataset_mass = class_specific_accuracy_score(preds = mask, targets = pred_mask, class_id = 1,\n",
    "                                                              averaging = \"micro\")\n",
    "        acc_per_dataset_background = class_specific_accuracy_score(preds = mask, targets = pred_mask, class_id = 0,\n",
    "                                                                   averaging = \"micro\")\n",
    "\n",
    "        loss = loss.to('cpu')\n",
    "\n",
    "        output = {\n",
    "            \"loss\":                       loss,\n",
    "            \"tp\":                         tp,\n",
    "            \"fp\":                         fp,\n",
    "            \"fn\":                         fn,\n",
    "            \"tn\":                         tn,\n",
    "            \"iou_per_image_mass\":         iou_per_image_mass,\n",
    "            \"iou_per_image_background\":   iou_per_image_background,\n",
    "            \"iou_per_dataset_mass\":      iou_per_dataset_mass,\n",
    "            \"iou_per_dataset_background\": iou_per_dataset_background,\n",
    "            \"iou_per_image_mass_no_empty\":         iou_per_image_mass_no_empty,\n",
    "            \"iou_per_image_background_no_empty\":   iou_per_image_background_no_empty,\n",
    "            \"iou_per_dataset_mass_no_empty\":      iou_per_dataset_mass_no_empty,\n",
    "            \"iou_per_dataset_background_no_empty\": iou_per_dataset_background_no_empty, ###\n",
    "            \"dice_per_image_mass\":         dice_per_image_mass,\n",
    "            \"dice_per_image_background\":   dice_per_image_background,\n",
    "            \"dice_per_dataset_mass\":      dice_per_dataset_mass,\n",
    "            \"dice_per_dataset_background\": dice_per_dataset_background,\n",
    "            \"dice_per_image_mass_no_empty\":         dice_per_image_mass_no_empty,\n",
    "            \"dice_per_image_background_no_empty\":   dice_per_image_background_no_empty,\n",
    "            \"dice_per_dataset_mass_no_empty\":      dice_per_dataset_mass_no_empty,\n",
    "            \"dice_per_dataset_background_no_empty\": dice_per_dataset_background_no_empty,\n",
    "            \"acc_per_image_mass\":         acc_per_image_mass,\n",
    "            \"acc_per_image_background\":   acc_per_image_background,\n",
    "            \"acc_per_dataset_mass\":      acc_per_dataset_mass,\n",
    "            \"acc_per_dataset_background\": acc_per_dataset_background\n",
    "        }\n",
    "\n",
    "        if stage=='train':\n",
    "            self.train_outputs.append(output)\n",
    "        if stage=='valid':\n",
    "            self.val_outputs.append(output)\n",
    "        if stage=='test':\n",
    "            self.test_outputs.append(output)\n",
    "        self.log(f'{stage}_loss', loss, prog_bar=True, on_epoch=True, batch_size=self.batch_size)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if batch:\n",
    "            return self.step(batch, batch_idx, \"valid\",self.mode)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        if batch:\n",
    "            return self.step(batch, batch_idx, \"test\", self.mode)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if batch:\n",
    "            return self.step(batch, batch_idx, \"train\",self.mode)\n",
    "\n",
    "    def forward(self, image):\n",
    "        mask=self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def single_predict_sliding_window(self, image, roi_size=(128,128), sw_batch_size=128, overlap=0):\n",
    "        return sliding_window_inference(image, roi_size, sw_batch_size, self.model, overlap=0,\n",
    "                                              mode=\"gaussian\")\n",
    "\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        # aggregate step metics\n",
    "        if not outputs:\n",
    "            return\n",
    "\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "        \n",
    "        iou_per_image_mass = torch.nanmean(torch.Tensor([x[\"iou_per_image_mass\"] for x in outputs]))\n",
    "        iou_per_image_background = torch.nanmean(torch.Tensor([x[\"iou_per_image_background\"] for x in outputs]))\n",
    "        iou_per_dataset_image = torch.nanmean(torch.Tensor([x[\"iou_per_dataset_mass\"] for x in outputs]))\n",
    "        iou_per_dataset_background = torch.nanmean(torch.Tensor([x[\"iou_per_dataset_background\"] for x in outputs]))\n",
    "\n",
    "        iou_per_image_mass_no_empty = torch.nanmean(torch.Tensor([x[\"iou_per_image_mass_no_empty\"] for x in outputs]))\n",
    "        iou_per_image_background_no_empty = torch.nanmean(torch.Tensor([x[\"iou_per_image_background_no_empty\"] for x in outputs]))\n",
    "        iou_per_dataset_image_no_empty = torch.nanmean(torch.Tensor([x[\"iou_per_dataset_mass_no_empty\"] for x in outputs]))\n",
    "        iou_per_dataset_background_no_empty = torch.nanmean(torch.Tensor([x[\"iou_per_dataset_background_no_empty\"] for x in outputs]))\n",
    "\n",
    "\n",
    "        dice_per_image_mass = torch.nanmean(torch.Tensor([x[\"dice_per_image_mass\"] for x in outputs]))\n",
    "        dice_per_image_background = torch.nanmean(torch.Tensor([x[\"dice_per_image_background\"] for x in outputs]))\n",
    "        dice_per_dataset_image = torch.nanmean(torch.Tensor([x[\"dice_per_dataset_mass\"] for x in outputs]))\n",
    "        dice_per_dataset_background = torch.nanmean(torch.Tensor([x[\"dice_per_dataset_background\"] for x in outputs]))\n",
    "\n",
    "        dice_per_image_mass_no_empty = torch.nanmean(torch.Tensor([x[\"dice_per_image_mass_no_empty\"] for x in outputs]))\n",
    "        dice_per_image_background_no_empty = torch.nanmean(torch.Tensor([x[\"dice_per_image_background_no_empty\"] for x in outputs]))\n",
    "        dice_per_dataset_image_no_empty = torch.nanmean(torch.Tensor([x[\"dice_per_dataset_mass_no_empty\"] for x in outputs]))\n",
    "        dice_per_dataset_background_no_empty = torch.nanmean(torch.Tensor([x[\"dice_per_dataset_background_no_empty\"] for x in outputs]))\n",
    "\n",
    "\n",
    "        acc_per_image_mass = torch.nanmean(torch.Tensor([x[\"acc_per_image_mass\"] for x in outputs]))\n",
    "        acc_per_image_background = torch.nanmean(torch.Tensor([x[\"acc_per_image_background\"] for x in outputs]))\n",
    "        acc_per_dataset_image = torch.nanmean(torch.Tensor([x[\"acc_per_dataset_mass\"] for x in outputs]))\n",
    "        acc_per_dataset_background = torch.nanmean(torch.Tensor([x[\"acc_per_dataset_background\"] for x in outputs]))\n",
    "\n",
    "\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction = \"micro-imagewise\")\n",
    "        per_dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction = \"micro\")\n",
    "\n",
    "        per_image_dice = compute_dice_score_from_cm(tp, fp, fn, tn, reduction = \"micro-imagewise\")\n",
    "        per_dataset_dice = compute_dice_score_from_cm(tp, fp, fn, tn, reduction = \"micro\")\n",
    "\n",
    "\n",
    "        # MACRO AVG\n",
    "        precision = compute_mean_precision(tp, fp, fn, tn)\n",
    "        recall = compute_mean_recall(tp, fp, fn, tn)\n",
    "\n",
    "        accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction='micro-imagewise')\n",
    "\n",
    "        # MACRO IMAGEWISE MEAN DICE WITH EMPTY\n",
    "        dice1_per_image = compute_dice_from_metrics(tp, fp, tn, fn, reduction='none')\n",
    "        dice0_per_image = compute_dice_from_metrics(tn, fn, tp, fp,  reduction='none')\n",
    "        mean_dice_per_image = np.mean(np.nanmean(np.array([dice0_per_image.cpu().numpy(), dice1_per_image.cpu().numpy()]), axis=0))\n",
    "\n",
    "        # MACRO MEAN DICE WITH EMPTY\n",
    "        mean_dice1_per_dataset = compute_dice_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise')\n",
    "        mean_dice0_per_dataset = compute_dice_from_metrics(tn, fn, tp, fp,  reduction='micro-imagewise')\n",
    "        mean_dice_per_dataset = np.nanmean(np.array([mean_dice0_per_dataset.cpu().numpy(), mean_dice1_per_dataset.cpu().numpy()]))\n",
    "\n",
    "        # MACRO IMAGEWISE MEAN DICE NO EMPTY\n",
    "        dice1_per_image_no_empty = compute_dice_from_metrics(tp, fp, tn, fn, reduction='none',exclude_empty=True)\n",
    "        dice0_per_image_no_empty = compute_dice_from_metrics(tn, fn, tp, fp,  reduction='none',exclude_empty=True)\n",
    "        combined_dice_scores = np.hstack((dice0_per_image_no_empty, dice1_per_image_no_empty ))\n",
    "        valid_pairs = ~np.isnan(combined_dice_scores).any(axis=1)\n",
    "        mean_dice_per_image_no_empty = np.mean(np.nanmean(combined_dice_scores[valid_pairs], axis=1))\n",
    "\n",
    "        if mean_dice_per_image_no_empty.size == 0:\n",
    "             mean_dice_per_image_no_empty = float('nan')\n",
    "\n",
    "        # MACRO MEAN DICE NO EMPTY\n",
    "        mean_dice1_per_dataset_no_empty = compute_dice_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise',exclude_empty=True)\n",
    "        mean_dice0_per_dataset_no_empty = compute_dice_from_metrics(tn, fn, tp, fp,  reduction='micro-imagewise',exclude_empty=True)\n",
    "        mean_dice_per_dataset_no_empty = np.mean(np.array([mean_dice0_per_dataset_no_empty.cpu().numpy(), mean_dice1_per_dataset_no_empty.cpu().numpy()]))\n",
    "\n",
    "        # MACRO IMAGEWISE MEAN IOU WITH EMPTY\n",
    "\n",
    "        iou1_per_image = compute_iou_from_metrics(tp, fp, tn, fn, reduction='none')\n",
    "        iou0_per_image = compute_iou_from_metrics(tn, fn, tp, fp,  reduction='none')\n",
    "        mean_iou_per_image = np.mean(np.nanmean(np.array([iou0_per_image.cpu().numpy(), iou1_per_image.cpu().numpy()]), axis=0))\n",
    "\n",
    "        # MACRO MEAN IOU WITH EMPTY\n",
    "        mean_iou1_per_dataset = compute_iou_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise')\n",
    "        mean_iou0_per_dataset = compute_iou_from_metrics(tn, fn, tp, fp,  reduction='micro-imagewise')\n",
    "        mean_iou_per_dataset = np.nanmean(np.array([mean_iou0_per_dataset.cpu().numpy(), mean_iou1_per_dataset.cpu().numpy()]))\n",
    "\n",
    "        # MACRO IMAGEWISE MEAN IOU NO EMPTY\n",
    "        iou1_per_image_no_empty = compute_iou_from_metrics(tp, fp, tn, fn, reduction='none',exclude_empty=True)\n",
    "        iou0_per_image_no_empty = compute_iou_from_metrics(tn, fn, tp, fp,  reduction='none',exclude_empty=True)\n",
    "        \n",
    "        combined_iou_scores = np.hstack((iou0_per_image_no_empty, iou1_per_image_no_empty))\n",
    "        valid_pairs = ~np.isnan(combined_iou_scores).any(axis=1) #10, 2\n",
    "        mean_iou_per_image_no_empty = np.mean(np.nanmean(combined_iou_scores[valid_pairs], axis=1))\n",
    "\n",
    "        if mean_iou_per_image_no_empty.size == 0:\n",
    "             mean_iou_per_image_no_empty = float('nan')\n",
    "\n",
    "        # MACRO MEAN IOU NO EMPTY\n",
    "        mean_iou1_per_dataset_no_empty = compute_iou_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise',exclude_empty=True)\n",
    "        mean_iou0_per_dataset_no_empty = compute_iou_from_metrics(tn, fn, tp, fp,  reduction='micro-imagewise',exclude_empty=True)\n",
    "        mean_iou_per_dataset_no_empty = np.mean(np.array([mean_iou0_per_dataset_no_empty.cpu().numpy(), mean_iou1_per_dataset_no_empty.cpu().numpy()]))\n",
    "        \n",
    "\n",
    "       \n",
    "        self.log(f\"{stage}_per_image_iou\", per_image_iou, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_per_dataset_iou\", per_dataset_iou, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f\"{stage}_per_image_dice\", per_image_dice, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_per_dataset_dice\", per_dataset_dice, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.log(f\"{stage}_mean_iou_per_image\", mean_iou_per_image, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_mean_iou_per_dataset\", mean_iou_per_dataset, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f\"{stage}_mean_iou_per_image_no_empty\", mean_iou_per_image_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_mean_iou_per_dataset_no_empty\", mean_iou_per_dataset_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f\"{stage}_mean_dice_per_image\", mean_dice_per_image, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_mean_dice_per_dataset\", mean_dice_per_dataset, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f\"{stage}_mean_dice_per_image_no_empty\", mean_dice_per_image_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_mean_dice_per_dataset_no_empty\", mean_dice_per_dataset_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        \n",
    "        self.log(f\"{stage}_precision\", precision, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_recall\", recall, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_accuracy\", accuracy, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f'{stage}_iou_per_image_mass', iou_per_image_mass, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_image_background', iou_per_image_background, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_dataset_mass', iou_per_dataset_image, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_dataset_background', iou_per_dataset_background, sync_dist = True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f'{stage}_iou_per_image_mass_no_empty', iou_per_image_mass_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_image_background_no_empty', iou_per_image_background_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_dataset_mass_no_empty', iou_per_dataset_image_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_dataset_background_no_empty', iou_per_dataset_background_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f'{stage}_dice_per_image_mass', dice_per_image_mass, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_image_background', dice_per_image_background, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_dataset_mass', dice_per_dataset_image, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_dataset_background', dice_per_dataset_background, sync_dist = True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f'{stage}_dice_per_image_mass_no_empty', dice_per_image_mass_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_image_background_no_empty', dice_per_image_background_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_dataset_mass_no_empty', dice_per_dataset_image_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_dataset_background_no_empty', dice_per_dataset_background_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "        self.log(f'{stage}_acc_per_image_mass', acc_per_image_mass, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_acc_per_image_background', acc_per_image_background, sync_dist = True, prog_bar=True,batch_size=batch_size)\n",
    "        self.log(f'{stage}_acc_per_dataset_mass', acc_per_dataset_image, sync_dist = True, prog_bar=True,batch_size=batch_size)\n",
    "        self.log(f'{stage}_acc_per_dataset_background', acc_per_dataset_background, sync_dist = True, prog_bar=True,batch_size=batch_size)\n",
    "        self.log(f'{stage}_acc_per_dataset_background', acc_per_dataset_background, sync_dist = True, prog_bar=True,batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(outputs = self.train_outputs, stage = \"train\")\n",
    "        self.train_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(outputs = self.val_outputs, stage = \"valid\")\n",
    "        self.val_outputs.clear()\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.shared_epoch_end(outputs = self.test_outputs, stage = \"test\")\n",
    "        self.test_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-4, weight_decay=1e-4)\n",
    "\n",
    "        iterations_per_epoch = self.len_train_loader  # Number of iterations per epoch\n",
    "        step_size_up = iterations_per_epoch // 2  # Half an epoch for the increasing phase\n",
    "        gamma = 0.99 \n",
    "\n",
    "        base_lr = 3e-5  # Increased base learning rate\n",
    "        max_lr = 9e-4   # Increased maximum learning rate\n",
    "\n",
    "        #base_lr = 1e-4  # Increased base learning rate\n",
    "        #max_lr = 1e-3   # Increased maximum learning rate\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n",
    "                             base_lr=base_lr,  # Minimum learning rate\n",
    "                             max_lr=max_lr,   # Maximum learning rate\n",
    "                             step_size_up=step_size_up,\n",
    "                             mode='triangular',\n",
    "                             cycle_momentum=False)  # Set to True if using an optimizer with momentum\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    "class BreastModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 arch,\n",
    "                 encoder_name,\n",
    "                 in_channels,\n",
    "                 out_classes,\n",
    "                 batch_size,\n",
    "                 len_train_loader,\n",
    "                 threshold = 0.4,\n",
    "                 t_loss = AsymmetricUnifiedFocalLoss(delta=0.4, gamma=0.1),\n",
    "                 boundaryloss=False, use_decoder_attention=True,use_simple_fusion=False,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        aux_params = dict(\n",
    "                pooling = 'avg',  # one of 'avg', 'max'\n",
    "                dropout = 0.5,  # dropout ratio, default is None\n",
    "                activation = None,  # activation function, default is None\n",
    "                classes = out_classes,  # define number of output labels\n",
    "        )\n",
    "        self.model = MultiInputUNet(n_channels=in_channels, n_classes=out_classes, use_simple_fusion=use_simple_fusion,use_decoder_attention=use_decoder_attention)\n",
    "        \"\"\"self.model = smp.create_model(\n",
    "               arch, encoder_name=encoder_name,\n",
    "                aux_params = aux_params, \n",
    "               in_channels = in_channels, encoder_weights=None,**kwargs)\"\"\"\n",
    "\n",
    "        self.t_loss = t_loss\n",
    "    \n",
    "        self.train_outputs = []\n",
    "        self.val_outputs = []\n",
    "        self.test_outputs = []\n",
    "        self.save_hyperparameters()\n",
    "        self.threshold = threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.len_train_loader = len_train_loader\n",
    "        self.augment_inference=False\n",
    "        self.val=80\n",
    "        self.boundaryloss=boundaryloss\n",
    "\n",
    "    def ttaug(self, model, image):\n",
    "        transforms = tta.Compose(\n",
    "        [\n",
    "            tta.HorizontalFlip(),\n",
    "            tta.VerticalFlip(),\n",
    "            tta.Rotate90(angles=[0, 180]),\n",
    "            #tta.Scale(scales=[1, 2, 4]),\n",
    "            #tta.Multiply(factors=[0.8]),        \n",
    "        ]\n",
    "        )\n",
    "    \n",
    "        model.eval()\n",
    "        merger = Merger(type=\"tsharpen\", n=len(transforms))\n",
    "        for transformer in transforms: # custom transforms or e.g. tta.aliases.d4_transform() \n",
    "        \n",
    "            # augment image\n",
    "            augmented_image = transformer.augment_image(image)\n",
    "            augmented_image = augmented_image.to(\"cuda\")\n",
    "    \n",
    "            model_output = model(augmented_image)[0]\n",
    "            \n",
    "            # reverse augmentation for mask and label\n",
    "            deaug_mask = transformer.deaugment_mask(model_output)\n",
    "            #deaug_mask = deaug_mask.sigmoid()\n",
    "            \n",
    "            merger.append(deaug_mask)\n",
    "    \n",
    "        masks = merger.result\n",
    "        #masks = masks.sigmoid()\n",
    "\n",
    "        return masks\n",
    "\n",
    "\n",
    "    def step(self, batch, batch_idx, stage):\n",
    "        \n",
    "        image1 = batch[0][\"image\"].to(\"cuda\")\n",
    "        image2 = batch[1][\"image\"].to(\"cuda\")\n",
    "        image3 = batch[2][\"image\"].to(\"cuda\")\n",
    "\n",
    "        assert image1.ndim == 4\n",
    "        h, w = image1.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask = batch[0][\"label\"].to(\"cuda\")\n",
    "        assert mask.ndim == 4\n",
    "\n",
    "        mask = mask\n",
    "        assert mask.max() <= 1 and mask.min() >= 0\n",
    "\n",
    "        if stage=='test' and self.augment_inference:\n",
    "            prob_mask = self.ttaug(self.model, image)\n",
    "            logits_mask = self.forward(image)\n",
    "\n",
    "        else:\n",
    "            logits_mask = self.forward(image1,image2,image3)\n",
    "            prob_mask = logits_mask.sigmoid()\n",
    "\n",
    "        if self.boundaryloss:\n",
    "            dist_map = batch[0][\"boundary\"].to(\"cuda\")\n",
    "            t_loss = self.t_loss(logits_mask, prob_mask, dist_map, mask, self.current_epoch)\n",
    "\n",
    "        else:\n",
    "            t_loss = self.t_loss(logits_mask, mask)\n",
    "\n",
    "        loss = t_loss\n",
    "\n",
    "        pred_mask = (prob_mask > self.threshold).float()\n",
    "\n",
    "        if stage=='test' and self.augment_inference:\n",
    "            pred_mask = (prob_mask > self.threshold).int()\n",
    "            pred_mask = remove_far_masses_based_on_largest_mass(pred_mask, distance_threshold=10)\n",
    "            pred_mask = RemoveSmallObjects(connectivity=1, min_size=140)(pred_mask)\n",
    "            pred_mask = torch.Tensor(pred_mask).to(\"cuda\")\n",
    "\n",
    "\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode = \"binary\")\n",
    "\n",
    "        iou_per_image_mass = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 1,\n",
    "                                         reduction = \"micro_image_wise\")\n",
    "        iou_per_image_background = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 0,\n",
    "                                               reduction = \"micro_image_wise\")\n",
    "\n",
    "        iou_per_image_mass_no_empty = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 1,\n",
    "                                         reduction = \"micro_image_wise\", exclude_empty=True)\n",
    "        iou_per_image_background_no_empty = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 0,\n",
    "                                               reduction = \"micro_image_wise\",exclude_empty=True)\n",
    "\n",
    "        iou_per_dataset_mass = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 1, reduction = \"micro\")\n",
    "        iou_per_dataset_background = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 0, reduction = \"micro\")\n",
    "\n",
    "        iou_per_dataset_mass_no_empty = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 1, reduction = \"micro\",exclude_empty=True)\n",
    "        iou_per_dataset_background_no_empty = compute_iou(y_true = mask, y_pred = pred_mask, class_id = 0, reduction = \"micro\",exclude_empty=True)\n",
    "\n",
    "        ###\n",
    "\n",
    "        dice_per_image_mass = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 1,\n",
    "                                         reduction = \"micro_image_wise\")\n",
    "        dice_per_image_background = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 0,\n",
    "                                               reduction = \"micro_image_wise\")\n",
    "\n",
    "        dice_per_image_mass_no_empty = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 1,\n",
    "                                         reduction = \"micro_image_wise\", exclude_empty=True)\n",
    "        dice_per_image_background_no_empty = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 0,\n",
    "                                               reduction = \"micro_image_wise\",exclude_empty=True)\n",
    "\n",
    "        dice_per_dataset_mass = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 1, reduction = \"micro\")\n",
    "        dice_per_dataset_background = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 0, reduction = \"micro\")\n",
    "\n",
    "        dice_per_dataset_mass_no_empty = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 1, reduction = \"micro\",exclude_empty=True)\n",
    "        dice_per_dataset_background_no_empty = compute_dice_score(y_true = mask, y_pred = pred_mask, class_id = 0, reduction = \"micro\",exclude_empty=True)\n",
    "\n",
    "\n",
    "        ###\n",
    "\n",
    "        acc_per_image_mass = class_specific_accuracy_score(preds = mask, targets = pred_mask, class_id = 1,\n",
    "                                                           averaging = \"micro_image_wise\")\n",
    "        acc_per_image_background = class_specific_accuracy_score(preds = mask, targets = pred_mask, class_id = 0,\n",
    "                                                                 averaging = \"micro_image_wise\")\n",
    "\n",
    "        acc_per_dataset_mass = class_specific_accuracy_score(preds = mask, targets = pred_mask, class_id = 1,\n",
    "                                                              averaging = \"micro\")\n",
    "        acc_per_dataset_background = class_specific_accuracy_score(preds = mask, targets = pred_mask, class_id = 0,\n",
    "                                                                   averaging = \"micro\")\n",
    "\n",
    "        loss = loss.to('cpu')\n",
    "\n",
    "        output = {\n",
    "            \"loss\":                       loss,\n",
    "            \"tp\":                         tp,\n",
    "            \"fp\":                         fp,\n",
    "            \"fn\":                         fn,\n",
    "            \"tn\":                         tn,\n",
    "            \"iou_per_image_mass\":         iou_per_image_mass,\n",
    "            \"iou_per_image_background\":   iou_per_image_background,\n",
    "            \"iou_per_dataset_mass\":      iou_per_dataset_mass,\n",
    "            \"iou_per_dataset_background\": iou_per_dataset_background,\n",
    "            \"iou_per_image_mass_no_empty\":         iou_per_image_mass_no_empty,\n",
    "            \"iou_per_image_background_no_empty\":   iou_per_image_background_no_empty,\n",
    "            \"iou_per_dataset_mass_no_empty\":      iou_per_dataset_mass_no_empty,\n",
    "            \"iou_per_dataset_background_no_empty\": iou_per_dataset_background_no_empty, ###\n",
    "            \"dice_per_image_mass\":         dice_per_image_mass,\n",
    "            \"dice_per_image_background\":   dice_per_image_background,\n",
    "            \"dice_per_dataset_mass\":      dice_per_dataset_mass,\n",
    "            \"dice_per_dataset_background\": dice_per_dataset_background,\n",
    "            \"dice_per_image_mass_no_empty\":         dice_per_image_mass_no_empty,\n",
    "            \"dice_per_image_background_no_empty\":   dice_per_image_background_no_empty,\n",
    "            \"dice_per_dataset_mass_no_empty\":      dice_per_dataset_mass_no_empty,\n",
    "            \"dice_per_dataset_background_no_empty\": dice_per_dataset_background_no_empty,\n",
    "            \"acc_per_image_mass\":         acc_per_image_mass,\n",
    "            \"acc_per_image_background\":   acc_per_image_background,\n",
    "            \"acc_per_dataset_mass\":      acc_per_dataset_mass,\n",
    "            \"acc_per_dataset_background\": acc_per_dataset_background\n",
    "        }\n",
    "\n",
    "        if stage=='train':\n",
    "            self.train_outputs.append(output)\n",
    "        if stage=='valid':\n",
    "            self.val_outputs.append(output)\n",
    "        if stage=='test':\n",
    "            self.test_outputs.append(output)\n",
    "        self.log(f'{stage}_loss', loss, prog_bar=True, on_epoch=True, batch_size=self.batch_size)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if batch:\n",
    "            return self.step(batch, batch_idx, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        if batch:\n",
    "            return self.step(batch, batch_idx, \"test\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if batch:\n",
    "            return self.step(batch, batch_idx, \"train\")\n",
    "\n",
    "\n",
    "    def forward(self, image1,image2, image3):\n",
    "        mask=self.model(image1, image2, image3)\n",
    "        return mask\n",
    "\n",
    "    def single_predict_sliding_window(self, image, roi_size=(128,128), sw_batch_size=128, overlap=0):\n",
    "        return sliding_window_inference(image, roi_size, sw_batch_size, self.model, overlap=0,\n",
    "                                              mode=\"gaussian\")\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        # aggregate step metics\n",
    "        if not outputs:\n",
    "            return\n",
    "\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "        \n",
    "        iou_per_image_mass = torch.nanmean(torch.Tensor([x[\"iou_per_image_mass\"] for x in outputs]))\n",
    "        iou_per_image_background = torch.nanmean(torch.Tensor([x[\"iou_per_image_background\"] for x in outputs]))\n",
    "        iou_per_dataset_image = torch.nanmean(torch.Tensor([x[\"iou_per_dataset_mass\"] for x in outputs]))\n",
    "        iou_per_dataset_background = torch.nanmean(torch.Tensor([x[\"iou_per_dataset_background\"] for x in outputs]))\n",
    "\n",
    "        iou_per_image_mass_no_empty = torch.nanmean(torch.Tensor([x[\"iou_per_image_mass_no_empty\"] for x in outputs]))\n",
    "        iou_per_image_background_no_empty = torch.nanmean(torch.Tensor([x[\"iou_per_image_background_no_empty\"] for x in outputs]))\n",
    "        iou_per_dataset_image_no_empty = torch.nanmean(torch.Tensor([x[\"iou_per_dataset_mass_no_empty\"] for x in outputs]))\n",
    "        iou_per_dataset_background_no_empty = torch.nanmean(torch.Tensor([x[\"iou_per_dataset_background_no_empty\"] for x in outputs]))\n",
    "\n",
    "\n",
    "        dice_per_image_mass = torch.nanmean(torch.Tensor([x[\"dice_per_image_mass\"] for x in outputs]))\n",
    "        dice_per_image_background = torch.nanmean(torch.Tensor([x[\"dice_per_image_background\"] for x in outputs]))\n",
    "        dice_per_dataset_image = torch.nanmean(torch.Tensor([x[\"dice_per_dataset_mass\"] for x in outputs]))\n",
    "        dice_per_dataset_background = torch.nanmean(torch.Tensor([x[\"dice_per_dataset_background\"] for x in outputs]))\n",
    "\n",
    "        dice_per_image_mass_no_empty = torch.nanmean(torch.Tensor([x[\"dice_per_image_mass_no_empty\"] for x in outputs]))\n",
    "        dice_per_image_background_no_empty = torch.nanmean(torch.Tensor([x[\"dice_per_image_background_no_empty\"] for x in outputs]))\n",
    "        dice_per_dataset_image_no_empty = torch.nanmean(torch.Tensor([x[\"dice_per_dataset_mass_no_empty\"] for x in outputs]))\n",
    "        dice_per_dataset_background_no_empty = torch.nanmean(torch.Tensor([x[\"dice_per_dataset_background_no_empty\"] for x in outputs]))\n",
    "\n",
    "\n",
    "        acc_per_image_mass = torch.nanmean(torch.Tensor([x[\"acc_per_image_mass\"] for x in outputs]))\n",
    "        acc_per_image_background = torch.nanmean(torch.Tensor([x[\"acc_per_image_background\"] for x in outputs]))\n",
    "        acc_per_dataset_image = torch.nanmean(torch.Tensor([x[\"acc_per_dataset_mass\"] for x in outputs]))\n",
    "        acc_per_dataset_background = torch.nanmean(torch.Tensor([x[\"acc_per_dataset_background\"] for x in outputs]))\n",
    "\n",
    "\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction = \"micro-imagewise\")\n",
    "        per_dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction = \"micro\")\n",
    "\n",
    "        per_image_dice = compute_dice_score_from_cm(tp, fp, fn, tn, reduction = \"micro-imagewise\")\n",
    "        per_dataset_dice = compute_dice_score_from_cm(tp, fp, fn, tn, reduction = \"micro\")\n",
    "\n",
    "\n",
    "        # MACRO AVG\n",
    "        precision = compute_mean_precision(tp, fp, fn, tn)\n",
    "        recall = compute_mean_recall(tp, fp, fn, tn)\n",
    "\n",
    "        accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction='micro-imagewise')\n",
    "\n",
    "        \n",
    "        # MACRO IMAGEWISE MEAN DICE WITH EMPTY\n",
    "        dice1_per_image = compute_dice_from_metrics(tp, fp, tn, fn, reduction='none')\n",
    "        dice0_per_image = compute_dice_from_metrics(tn, fn, tp, fp,  reduction='none')\n",
    "        mean_dice_per_image = np.mean(np.nanmean(np.array([dice0_per_image.cpu().numpy(), dice1_per_image.cpu().numpy()]), axis=0))\n",
    "\n",
    "        # MACRO MEAN DICE WITH EMPTY\n",
    "        mean_dice1_per_dataset = compute_dice_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise')\n",
    "        mean_dice0_per_dataset = compute_dice_from_metrics(tn, fn, tp, fp,  reduction='micro-imagewise')\n",
    "        mean_dice_per_dataset = np.nanmean(np.array([mean_dice0_per_dataset.cpu().numpy(), mean_dice1_per_dataset.cpu().numpy()]))\n",
    "\n",
    "        # MACRO IMAGEWISE MEAN DICE NO EMPTY\n",
    "        dice1_per_image_no_empty = compute_dice_from_metrics(tp, fp, tn, fn, reduction='none',exclude_empty=True)\n",
    "        dice0_per_image_no_empty = compute_dice_from_metrics(tn, fn, tp, fp,  reduction='none',exclude_empty=True)\n",
    "        combined_dice_scores = np.hstack((dice0_per_image_no_empty, dice1_per_image_no_empty ))\n",
    "        valid_pairs = ~np.isnan(combined_dice_scores).any(axis=1)\n",
    "        mean_dice_per_image_no_empty = np.mean(np.nanmean(combined_dice_scores[valid_pairs], axis=1))\n",
    "\n",
    "        if mean_dice_per_image_no_empty.size == 0:\n",
    "             mean_dice_per_image_no_empty = float('nan')\n",
    "\n",
    "        # MACRO MEAN DICE NO EMPTY\n",
    "        mean_dice1_per_dataset_no_empty = compute_dice_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise',exclude_empty=True)\n",
    "        mean_dice0_per_dataset_no_empty = compute_dice_from_metrics(tn, fn, tp, fp,  reduction='micro-imagewise',exclude_empty=True)\n",
    "        mean_dice_per_dataset_no_empty = np.mean(np.array([mean_dice0_per_dataset_no_empty.cpu().numpy(), mean_dice1_per_dataset_no_empty.cpu().numpy()]))\n",
    "\n",
    "        # MACRO IMAGEWISE MEAN IOU WITH EMPTY\n",
    "\n",
    "        iou1_per_image = compute_iou_from_metrics(tp, fp, tn, fn, reduction='none')\n",
    "        iou0_per_image = compute_iou_from_metrics(tn, fn, tp, fp,  reduction='none')\n",
    "        mean_iou_per_image = np.mean(np.nanmean(np.array([iou0_per_image.cpu().numpy(), iou1_per_image.cpu().numpy()]), axis=0))\n",
    "\n",
    "        # MACRO MEAN IOU WITH EMPTY\n",
    "        mean_iou1_per_dataset = compute_iou_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise')\n",
    "        mean_iou0_per_dataset = compute_iou_from_metrics(tn, fn, tp, fp,  reduction='micro-imagewise')\n",
    "        mean_iou_per_dataset = np.nanmean(np.array([mean_iou0_per_dataset.cpu().numpy(), mean_iou1_per_dataset.cpu().numpy()]))\n",
    "\n",
    "        # MACRO IMAGEWISE MEAN IOU NO EMPTY\n",
    "        iou1_per_image_no_empty = compute_iou_from_metrics(tp, fp, tn, fn, reduction='none',exclude_empty=True)\n",
    "        iou0_per_image_no_empty = compute_iou_from_metrics(tn, fn, tp, fp,  reduction='none',exclude_empty=True)\n",
    "        \n",
    "        combined_iou_scores = np.hstack((iou0_per_image_no_empty, iou1_per_image_no_empty))\n",
    "        valid_pairs = ~np.isnan(combined_iou_scores).any(axis=1) #10, 2\n",
    "        mean_iou_per_image_no_empty = np.mean(np.nanmean(combined_iou_scores[valid_pairs], axis=1))\n",
    "\n",
    "        if mean_iou_per_image_no_empty.size == 0:\n",
    "             mean_iou_per_image_no_empty = float('nan')\n",
    "\n",
    "        # MACRO MEAN IOU NO EMPTY\n",
    "        mean_iou1_per_dataset_no_empty = compute_iou_from_metrics(tp, fp, tn, fn, reduction='micro-imagewise',exclude_empty=True)\n",
    "        mean_iou0_per_dataset_no_empty = compute_iou_from_metrics(tn, fn, tp, fp,  reduction='micro-imagewise',exclude_empty=True)\n",
    "        mean_iou_per_dataset_no_empty = np.mean(np.array([mean_iou0_per_dataset_no_empty.cpu().numpy(), mean_iou1_per_dataset_no_empty.cpu().numpy()]))\n",
    "    \n",
    "        self.log(f\"{stage}_per_image_iou\", per_image_iou, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.log(f\"{stage}_per_dataset_iou\", per_dataset_iou, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f\"{stage}_per_image_dice\", per_image_dice, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_per_dataset_dice\", per_dataset_dice, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.log(f\"{stage}_mean_iou_per_image\", mean_iou_per_image, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_mean_iou_per_dataset\", mean_iou_per_dataset, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f\"{stage}_mean_iou_per_image_no_empty\", mean_iou_per_image_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_mean_iou_per_dataset_no_empty\", mean_iou_per_dataset_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f\"{stage}_mean_dice_per_image\", mean_dice_per_image, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_mean_dice_per_dataset\", mean_dice_per_dataset, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f\"{stage}_mean_dice_per_image_no_empty\", mean_dice_per_image_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_mean_dice_per_dataset_no_empty\", mean_dice_per_dataset_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        \n",
    "        self.log(f\"{stage}_precision\", precision, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_recall\", recall, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f\"{stage}_accuracy\", accuracy, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f'{stage}_iou_per_image_mass', iou_per_image_mass, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_image_background', iou_per_image_background, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_dataset_mass', iou_per_dataset_image, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_dataset_background', iou_per_dataset_background, sync_dist = True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f'{stage}_iou_per_image_mass_no_empty', iou_per_image_mass_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_image_background_no_empty', iou_per_image_background_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_dataset_mass_no_empty', iou_per_dataset_image_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_iou_per_dataset_background_no_empty', iou_per_dataset_background_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f'{stage}_dice_per_image_mass', dice_per_image_mass, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_image_background', dice_per_image_background, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_dataset_mass', dice_per_dataset_image, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_dataset_background', dice_per_dataset_background, sync_dist = True, batch_size=batch_size)\n",
    "\n",
    "        self.log(f'{stage}_dice_per_image_mass_no_empty', dice_per_image_mass_no_empty, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_image_background_no_empty', dice_per_image_background_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_dataset_mass_no_empty', dice_per_dataset_image_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_dice_per_dataset_background_no_empty', dice_per_dataset_background_no_empty, sync_dist = True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "        self.log(f'{stage}_acc_per_image_mass', acc_per_image_mass, sync_dist = True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(f'{stage}_acc_per_image_background', acc_per_image_background, sync_dist = True, prog_bar=True,batch_size=batch_size)\n",
    "        self.log(f'{stage}_acc_per_dataset_mass', acc_per_dataset_image, sync_dist = True, prog_bar=True,batch_size=batch_size)\n",
    "        self.log(f'{stage}_acc_per_dataset_background', acc_per_dataset_background, sync_dist = True, prog_bar=True,batch_size=batch_size)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(outputs = self.train_outputs, stage = \"train\")\n",
    "        self.train_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(outputs = self.val_outputs, stage = \"valid\")\n",
    "        self.val_outputs.clear()\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.shared_epoch_end(outputs = self.test_outputs, stage = \"test\")\n",
    "        self.test_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-4, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "        iterations_per_epoch = self.len_train_loader  # Number of iterations per epoch\n",
    "        step_size_up = iterations_per_epoch // 2  # Half an epoch for the increasing phase\n",
    "        gamma = 0.99 \n",
    "\n",
    "        base_lr = 3e-5  # Increased base learning rate\n",
    "        max_lr = 9e-4   # Increased maximum learning rate\n",
    "\n",
    "        #base_lr = 1e-4  # Increased base learning rate\n",
    "        #max_lr = 1e-3   # Increased maximum learning rate\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n",
    "                             base_lr=base_lr,  # Minimum learning rate\n",
    "                             max_lr=max_lr,   # Maximum learning rate\n",
    "                             step_size_up=step_size_up,\n",
    "                             mode='triangular',\n",
    "                             cycle_momentum=False)  # Set to True if using an optimizer with momentum\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOFTDICELOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Soft Dice loss for binary (single-channel) segmentation.\n",
    "    Expects predictions of shape (B, 1, H, W) already in [0,1].\n",
    "    Expects ground truth of shape (B, 1, H, W), either 0/1 or float in [0,1].\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            probs: model outputs as probabilities in [0,1], (B, 1, H, W)\n",
    "            targets: ground truth masks, (B, 1, H, W) in {0,1} or floats\n",
    "        Returns:\n",
    "            Dice loss (scalar).\n",
    "        \"\"\"\n",
    "\n",
    "        probs = logits.sigmoid()\n",
    "        \n",
    "        # Flatten: (B, 1, H, W) -> (B, H*W)\n",
    "        probs_flat = probs.view(probs.size(0), -1)\n",
    "        targets_flat = targets.view(targets.size(0), -1)\n",
    "\n",
    "        # Numerator = 2 * Σ (p_i * t_i)\n",
    "        intersection = 2.0 * torch.sum(probs_flat * targets_flat, dim=1)\n",
    "\n",
    "        # Denominator = Σ (p_i^2) + Σ (t_i^2)\n",
    "        denominator = torch.sum(probs_flat * probs_flat, dim=1) + \\\n",
    "                      torch.sum(targets_flat * targets_flat, dim=1)\n",
    "\n",
    "        # Dice coefficient (per sample), then average over batch\n",
    "        dice_per_sample = (intersection + self.smooth) / (denominator + self.smooth)\n",
    "        dice = dice_per_sample.mean()\n",
    "\n",
    "        # Dice loss = 1 - dice coefficient\n",
    "        return 1.0 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8GmGyqMg1HS"
   },
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionGate, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, F_int, use_attention=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, mid_channels, kernel_size=2, stride=2)\n",
    "        # Adjust the attention gate to handle the combined skip connections\n",
    "        if use_attention:\n",
    "            self.attention = AttentionGate(F_g=mid_channels, F_l=mid_channels, F_int=F_int)\n",
    "        self.conv = ConvBlock(mid_channels + mid_channels, out_channels)  # Adjust for concatenated skip connection size\n",
    "        self.use_attention=use_attention\n",
    "\n",
    "    def forward(self, x, combined_skip):\n",
    "\n",
    "        x = self.up(x)\n",
    "        # Apply attention to the combined skip connections\n",
    "        if self.use_attention:\n",
    "            combined_skip = self.attention(g=x, x=combined_skip)\n",
    "        \n",
    "        x = torch.cat([x, combined_skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction_ratio, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction_ratio, in_channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_pooled = self.avg_pool(x).view(b, c)\n",
    "        max_pooled = self.max_pool(x).view(b, c)\n",
    "        avg_out = self.fc2(self.relu(self.fc1(avg_pooled)))\n",
    "        max_out = self.fc2(self.relu(self.fc1(max_pooled)))\n",
    "        out = self.sigmoid(avg_out + max_out).view(b, c, 1, 1)\n",
    "        return x * out\n",
    "        \n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([max_out, avg_out], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class FeatureFusionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, reduction=16):\n",
    "        super(FeatureFusionBlock, self).__init__()\n",
    "        self.channel_attention_local = ChannelAttention(in_channels)\n",
    "        self.spatial_attention_local = SpatialAttention()\n",
    "\n",
    "        self.channel_attention_global = ChannelAttention(in_channels)\n",
    "        self.spatial_attention_global = SpatialAttention()\n",
    "        \n",
    "        self.fusion_conv = nn.Conv2d(in_channels*3, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, global_feat, local_feat1, local_feat2):\n",
    "        # Apply channel attention to each feature map\n",
    "        global_ca = global_feat * self.channel_attention_global(global_feat)\n",
    "        local_ca1 = local_feat1 * self.channel_attention_local(local_feat1)\n",
    "        local_ca2 = local_feat2 * self.channel_attention_local(local_feat2)\n",
    "\n",
    "        # Apply spatial attention to each feature map\n",
    "        global_sa = global_ca * self.spatial_attention_global(global_ca)\n",
    "        local_sa1 = local_ca1 * self.spatial_attention_local(local_ca1)\n",
    "        local_sa2 = local_ca2 * self.spatial_attention_local(local_ca2)\n",
    "\n",
    "        # Concatenate the feature maps\n",
    "        fused_features = torch.cat((global_sa, local_sa1, local_sa2), dim=1)\n",
    "        \n",
    "        # Fuse them using a convolutional layer\n",
    "        fused_features = self.fusion_conv(fused_features)\n",
    "        fused_features = self.relu(fused_features)\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "class SimpleFeatureFusionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimpleFeatureFusionBlock, self).__init__()\n",
    "        # Since we're concatenating three feature maps, the input to the fusion_conv will be 3 times in_channels\n",
    "        self.fusion_conv = nn.Conv2d(in_channels * 3, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, global_feat, local_feat1, local_feat2):\n",
    "        # Concatenate the feature maps along the channel dimension\n",
    "        fused_features = torch.cat((global_feat, local_feat1, local_feat2), dim=1)\n",
    "        \n",
    "        # Apply a convolutional layer to reduce dimensions\n",
    "        fused_features = self.fusion_conv(fused_features)\n",
    "        fused_features = self.relu(fused_features)\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "\n",
    "\n",
    "class MultiInputUNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, use_simple_fusion=False, use_decoder_attention=True):\n",
    "        super(MultiInputUNet, self).__init__()\n",
    "\n",
    "        if use_simple_fusion:\n",
    "            self.fusion_skip1 = SimpleFeatureFusionBlock(64, 64)\n",
    "            self.fusion_skip2 = SimpleFeatureFusionBlock(128, 128)\n",
    "            self.fusion_skip3 = SimpleFeatureFusionBlock(256, 256)\n",
    "            self.fusion_skip4 = SimpleFeatureFusionBlock(512, 512)\n",
    "        else:\n",
    "          # Initialize fusion blocks with channel reduction\n",
    "            self.fusion_skip1 = FeatureFusionBlock(64, 64)\n",
    "            self.fusion_skip2 = FeatureFusionBlock(128, 128)\n",
    "            self.fusion_skip3 = FeatureFusionBlock(256, 256)\n",
    "            self.fusion_skip4 = FeatureFusionBlock(512,512)\n",
    "        \n",
    "        # Encoders for each input stream\n",
    "        self.encoder1 = nn.ModuleList([EncoderBlock(n_channels, 64), EncoderBlock(64, 128), EncoderBlock(128, 256), EncoderBlock(256,512)])\n",
    "        self.encoder2 = nn.ModuleList([EncoderBlock(n_channels, 64), EncoderBlock(64, 128), EncoderBlock(128, 256), EncoderBlock(256,512)])\n",
    "        self.encoder3 = nn.ModuleList([EncoderBlock(n_channels, 64), EncoderBlock(64, 128), EncoderBlock(128, 256), EncoderBlock(256,512)])\n",
    "\n",
    "        if use_simple_fusion:\n",
    "            self.deep_feature_fusion = SimpleFeatureFusionBlock(512, 512)\n",
    "        else:\n",
    "             self.deep_feature_fusion = FeatureFusionBlock(512,512)\n",
    "        \n",
    "        \n",
    "        # Decoder Blocks\n",
    "        self.decoder1 = DecoderBlock(512,512, 256,256, use_attention=use_decoder_attention)  # Input channels adjusted for merged features\n",
    "        self.decoder2 = DecoderBlock(256, 256, 128, 128,use_attention=use_decoder_attention)  # Input channels adjusted for merged features\n",
    "        self.decoder3 = DecoderBlock(128, 128, 64, 64,use_attention=use_decoder_attention)\n",
    "        self.decoder4 = DecoderBlock(64, 64, 32, 32,use_attention=use_decoder_attention)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        # Process each input through its respective encoders\n",
    "        skips1, p1 = self.process_through_encoders(x1, self.encoder1)\n",
    "        skips2, p2 = self.process_through_encoders(x2, self.encoder2)\n",
    "        skips3, p3 = self.process_through_encoders(x3, self.encoder3)\n",
    "\n",
    "        fused_skips1 = self.fusion_skip1(skips1[0], skips2[0], skips3[0])\n",
    "        fused_skips2 = self.fusion_skip2(skips1[1], skips2[1], skips3[1])\n",
    "        fused_skips3 = self.fusion_skip3(skips1[2], skips2[2], skips3[2])\n",
    "        fused_skips4 = self.fusion_skip4(skips1[3], skips2[3], skips3[3])\n",
    "\n",
    "        fused_features = self.deep_feature_fusion(p1, p2, p3)\n",
    "        \n",
    "        # Decode the combined features\n",
    "        # Note: Attention mechanism applies to concatenated skip connections from corresponding layers of each input\n",
    "        d1 = self.decoder1(fused_features, fused_skips4)\n",
    "        d2 = self.decoder2(d1, fused_skips3)\n",
    "        d3 = self.decoder3(d2, fused_skips2)\n",
    "        d4 = self.decoder4(d3, fused_skips1)\n",
    "\n",
    "        return self.final_conv(d4)\n",
    "\n",
    "    def process_through_encoders(self, x, encoders):\n",
    "        skips = []\n",
    "        p = x\n",
    "        for encoder in encoders:\n",
    "            x, p = encoder(p)\n",
    "            skips.append(x)\n",
    "        return skips, p  # Reverse skips for correct order in decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute volume metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(gt_mask,prediction_mask):\n",
    "    \"\"\"\n",
    "    Compute precision between prediction and ground truth masks, with consideration\n",
    "    for cases where no positives are retrieved and no relevant elements exist.\n",
    "    \n",
    "    :param prediction_mask: Binary mask of predictions (HxW).\n",
    "    :param gt_mask: Binary mask of ground truth (HxW).\n",
    "    :return: Precision as a float.\n",
    "    \"\"\"\n",
    "    prediction_mask = (prediction_mask > 0).astype(np.uint8)\n",
    "    gt_mask = (gt_mask > 0).astype(np.uint8)\n",
    "    \n",
    "    TP = np.logical_and(prediction_mask, gt_mask).sum()\n",
    "    FP = np.logical_and(prediction_mask, np.logical_not(gt_mask)).sum()\n",
    "    \n",
    "    if TP + FP == 0:\n",
    "        # If no predictions are made and the ground truth has no positives,\n",
    "        # precision is perfect if ground truth also has no positives.\n",
    "        return 1.0 if np.sum(gt_mask) == 0 else 0\n",
    "    else:\n",
    "        precision = TP / (TP + FP)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def compute_recall(gt_mask,prediction_mask):\n",
    "    \"\"\"\n",
    "    Compute recall between prediction and ground truth masks, with consideration\n",
    "    for cases where no positives are identified and no relevant elements exist.\n",
    "    \n",
    "    :param prediction_mask: Binary mask of predictions (HxW).\n",
    "    :param gt_mask: Binary mask of ground truth (HxW).\n",
    "    :return: Recall as a float.\n",
    "    \"\"\"\n",
    "    prediction_mask = (prediction_mask > 0).astype(np.uint8)\n",
    "    gt_mask = (gt_mask > 0).astype(np.uint8)\n",
    "    \n",
    "    TP = np.logical_and(prediction_mask, gt_mask).sum()\n",
    "    FN = np.logical_and(np.logical_not(prediction_mask), gt_mask).sum()\n",
    "    \n",
    "    if TP + FN == 0:\n",
    "        # If the ground truth has no positives, recall is perfect.\n",
    "        return 1.0\n",
    "    else:\n",
    "        recall = TP / (TP + FN)\n",
    "    \n",
    "    return recall\n",
    "\n",
    "\n",
    "def compute_precision_from_cumulator(TPs, FPs, FNs, TNs, exclude_empty=False, is_mean=True, return_std=False):\n",
    "    tp = torch.cat([tp for tp in TPs])\n",
    "    fp = torch.cat([fp for fp in FPs])\n",
    "    fn = torch.cat([fn for fn in FNs])\n",
    "    tn = torch.cat([tn for tn in TNs])\n",
    "    \n",
    "    # Denominators for precision\n",
    "    denom_class_1 = tp + fp\n",
    "    denom_class_0 = tn + fn\n",
    "    \n",
    "    # Precision for class 1 (per sample)\n",
    "    precision_class_1 = torch.where(\n",
    "        denom_class_1 > 0, tp / denom_class_1, torch.tensor(1.0 if not exclude_empty else float('nan'))\n",
    "    )\n",
    "    mean_precision_class_1 = torch.nanmean(precision_class_1).item()\n",
    "    stddev_class_1 = np.nanstd(precision_class_1.cpu().numpy())\n",
    "    \n",
    "    # Precision for class 0 (per sample)\n",
    "    precision_class_0 = torch.where(\n",
    "        denom_class_0 > 0, tn / denom_class_0, torch.tensor(1.0 if not exclude_empty else float('nan'))\n",
    "    )\n",
    "    mean_precision_class_0 = torch.nanmean(precision_class_0).item()\n",
    "    stddev_class_0 = np.nanstd(precision_class_0.cpu().numpy())\n",
    "    \n",
    "    if not is_mean:\n",
    "        if return_std:\n",
    "            return mean_precision_class_1, stddev_class_1\n",
    "        else:\n",
    "            return mean_precision_class_1\n",
    "    \n",
    "    # Overall mean precision (mean between the two classes)\n",
    "    overall_mean_precision = (mean_precision_class_1 + mean_precision_class_0) / 2\n",
    "\n",
    "    overall_std_precision = (stddev_class_1 + stddev_class_0) / 2\n",
    "\n",
    "    if return_std:\n",
    "        # Include std deviations when `return_std` is True\n",
    "        return overall_mean_precision, overall_std_precision\n",
    "    else:\n",
    "        # Default behavior\n",
    "        return overall_mean_precision\n",
    "\n",
    "\n",
    "\n",
    "def compute_recall_from_cumulator(TPs, FPs, FNs, TNs, exclude_empty=False, is_mean=True, return_std=False):\n",
    "    tp = torch.cat([tp for tp in TPs])\n",
    "    fp = torch.cat([fp for fp in FPs])\n",
    "    fn = torch.cat([fn for fn in FNs])\n",
    "    tn = torch.cat([tn for tn in TNs])\n",
    "    \n",
    "    # Denominators for recall\n",
    "    denom_class_1 = tp + fn\n",
    "    denom_class_0 = tn + fp\n",
    "    \n",
    "    # Recall for class 1 (per sample)\n",
    "    recall_class_1 = torch.where(\n",
    "        denom_class_1 > 0, tp / denom_class_1, torch.tensor(1.0 if not exclude_empty else float('nan'))\n",
    "    )\n",
    "    mean_recall_class_1 = torch.nanmean(recall_class_1).item()\n",
    "    stddev_class_1 = np.nanstd(recall_class_1.cpu().numpy())\n",
    "    \n",
    "    # Recall for class 0 (per sample)\n",
    "    recall_class_0 = torch.where(\n",
    "        denom_class_0 > 0, tn / denom_class_0, torch.tensor(1.0 if not exclude_empty else float('nan'))\n",
    "    )\n",
    "    mean_recall_class_0 = torch.nanmean(recall_class_0).item()\n",
    "    stddev_class_0 = np.nanstd(recall_class_0.cpu().numpy())\n",
    "    \n",
    "    if not is_mean:\n",
    "        if return_std:\n",
    "            # Return per-class recall and standard deviations\n",
    "            return mean_recall_class_1, stddev_class_1\n",
    "        else:\n",
    "            # Return per-class recall only\n",
    "            return mean_recall_class_1\n",
    "    \n",
    "    # Overall mean recall (mean between the two classes)\n",
    "    overall_mean_recall = (mean_recall_class_1 + mean_recall_class_0) / 2\n",
    "\n",
    "    overall_std_recall = (stddev_class_1 +  stddev_class_0) / 2\n",
    "\n",
    "    if return_std:\n",
    "        # Return overall mean recall and standard deviations\n",
    "        return overall_mean_recall, overall_std_recall\n",
    "    else:\n",
    "        # Return overall mean recall and class-wise mean recalls\n",
    "        return overall_mean_recall\n",
    "\n",
    "\n",
    "\n",
    "def compute_f1_from_cumulator(\n",
    "    TPs, FPs, FNs, TNs,\n",
    "    exclude_empty=False,\n",
    "    reduce_mean=True,\n",
    "    is_mean=True,\n",
    "    return_std=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the F1-score for both class 1 and class 0 from cumulative TPs, FPs, FNs, and TNs.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    TPs, FPs, FNs, TNs : list of torch.Tensor\n",
    "        Lists of True/False Positives/Negatives for each batch/segment.\n",
    "    exclude_empty : bool, optional\n",
    "        If True, denominators of zero yield NaN; if False, they fall back to 1.0.\n",
    "    reduce_mean : bool, optional\n",
    "        If False, returns per-image F1 array; if True, returns aggregated mean (and std).\n",
    "    is_mean : bool, optional\n",
    "        If True, averages class 0 and class 1 F1’s; if False, returns only class 1’s.\n",
    "    return_std : bool, optional\n",
    "        If True and reduce_mean=True, also returns the standard deviation.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    If reduce_mean == False:\n",
    "        - per_image_f1 (np.ndarray)\n",
    "          * If is_mean: overall (class-0+class-1)/2 per image\n",
    "          * Else: class-1 F1 per image\n",
    "    If reduce_mean == True:\n",
    "        If return_std:\n",
    "            - mean_f1 (float), std_f1 (float)\n",
    "        Else:\n",
    "            - mean_f1 (float)\n",
    "    \"\"\"\n",
    "    # concatenate\n",
    "    tp = torch.cat(TPs) if isinstance(TPs, (list, tuple)) else TPs\n",
    "    fp = torch.cat(FPs) if isinstance(FPs, (list, tuple)) else FPs\n",
    "    fn = torch.cat(FNs) if isinstance(FNs, (list, tuple)) else FNs\n",
    "    tn = torch.cat(TNs) if isinstance(TNs, (list, tuple)) else TNs\n",
    "\n",
    "    # class 1\n",
    "    denom1 = 2*tp + fp + fn\n",
    "    fallback = torch.tensor(float('nan') if exclude_empty else 1.0, device=tp.device)\n",
    "    f1_1 = torch.where(denom1 > 0, (2.0*tp)/denom1, fallback)\n",
    "\n",
    "    # class 0\n",
    "    denom0 = 2*tn + fn + fp\n",
    "    f1_0 = torch.where(denom0 > 0, (2.0*tn)/denom0, fallback)\n",
    "\n",
    "    # per-image arrays (CPU numpy)\n",
    "    f1_1_np = f1_1.cpu().numpy()\n",
    "    f1_0_np = f1_0.cpu().numpy()\n",
    "    if is_mean:\n",
    "        per_image = (f1_1_np + f1_0_np) / 2.0\n",
    "    else:\n",
    "        per_image = f1_1_np\n",
    "\n",
    "    # if user wants the raw per-image values…\n",
    "    if not reduce_mean:\n",
    "        return per_image\n",
    "\n",
    "    # otherwise aggregate across images\n",
    "    mean_f1 = np.nanmean(per_image)\n",
    "    if return_std:\n",
    "        std_f1 = np.nanstd(per_image)\n",
    "        return mean_f1, std_f1\n",
    "    return mean_f1\n",
    "\n",
    "\n",
    "\n",
    "def compute_accuracy_from_cumulator(TPs, FPs, FNs, TNs, exclude_empty=False, is_mean=True, return_std=False):\n",
    "    tp = torch.cat([tp for tp in TPs])\n",
    "    fp = torch.cat([fp for fp in FPs])\n",
    "    fn = torch.cat([fn for fn in FNs])\n",
    "    tn = torch.cat([tn for tn in TNs])\n",
    "    \n",
    "    # Denominators for accuracy\n",
    "    denom_class_1 = tp + fp + fn\n",
    "    denom_class_0 = tn + fp + fn\n",
    "    \n",
    "    # Accuracy for class 1 (foreground)\n",
    "    accuracy_class_1 = torch.where(\n",
    "        denom_class_1 > 0, tp / denom_class_1, torch.tensor(1.0 if not exclude_empty else float('nan'))\n",
    "    )\n",
    "    mean_accuracy_class_1 = torch.nanmean(accuracy_class_1).item()\n",
    "    stddev_class_1 = np.nanstd(accuracy_class_1.cpu().numpy())\n",
    "    \n",
    "    # Accuracy for class 0 (background)\n",
    "    accuracy_class_0 = torch.where(\n",
    "        denom_class_0 > 0, tn / denom_class_0, torch.tensor(1.0 if not exclude_empty else float('nan'))\n",
    "    )\n",
    "    mean_accuracy_class_0 = torch.nanmean(accuracy_class_0).item()\n",
    "    stddev_class_0 = np.nanstd(accuracy_class_0.cpu().numpy())\n",
    "    \n",
    "    if not is_mean:\n",
    "        if return_std:\n",
    "            # Return per-class accuracy and standard deviations\n",
    "            return mean_accuracy_class_1, stddev_class_1\n",
    "        else:\n",
    "            # Return per-class accuracy only\n",
    "            return mean_accuracy_class_1\n",
    "    \n",
    "    # Overall mean accuracy (mean between the two classes)\n",
    "    overall_mean_accuracy = (mean_accuracy_class_1 + mean_accuracy_class_0) / 2\n",
    "\n",
    "    overall_std_accuracy = (stddev_class_1 + stddev_class_0) / 2\n",
    "\n",
    "    if return_std:\n",
    "        # Return overall mean accuracy and standard deviations\n",
    "        return overall_mean_accuracy, overall_std_accuracy\n",
    "    else:\n",
    "        # Return overall mean accuracy and class-wise mean accuracies\n",
    "        return overall_mean_accuracy, mean_accuracy_class_0, mean_accuracy_class_1\n",
    "\n",
    "def compute_accuracy_excluding_cases(TPs, FPs, FNs, TNs, return_std=False, exclude_blank_case=False):\n",
    "    \"\"\"\n",
    "    Computes accuracy, excluding cases with a zero denominator or no ground truth positives.\n",
    "\n",
    "    Args:\n",
    "        TPs: List of tensors for true positives across batches.\n",
    "        FPs: List of tensors for false positives across batches.\n",
    "        FNs: List of tensors for false negatives across batches.\n",
    "        TNs: List of tensors for true negatives across batches.\n",
    "        return_std: Boolean indicating whether to return the standard deviation.\n",
    "\n",
    "    Returns:\n",
    "        mean_accuracy: Mean accuracy across valid cases.\n",
    "        (optional) stddev_accuracy: Standard deviation of accuracy across valid cases if return_std is True.\n",
    "    \"\"\"\n",
    "    tp = torch.cat([tp for tp in TPs])\n",
    "    fp = torch.cat([fp for fp in FPs])\n",
    "    fn = torch.cat([fn for fn in FNs])\n",
    "    tn = torch.cat([tn for tn in TNs])\n",
    "\n",
    "    # Compute the denominator for accuracy (tp + fp + fn + tn)\n",
    "    denominator = tp + fp + fn + tn\n",
    "\n",
    "\n",
    "    if exclude_blank_case:\n",
    "        valid_mask = ((tp + fp + fn) != 0)\n",
    "    else:\n",
    "        # Exclude cases with zero denominator or no ground truth positives\n",
    "        valid_mask = ((tp + fp + fn) != 0) & ((tp + fn) != 0)\n",
    "\n",
    "    # Compute accuracy only for valid cases\n",
    "    accuracy = torch.zeros_like(denominator, dtype=torch.float)\n",
    "    accuracy[valid_mask] = (tp[valid_mask] + tn[valid_mask]) / denominator[valid_mask]\n",
    "\n",
    "    # Exclude invalid cases by setting them to NaN\n",
    "    accuracy[~valid_mask] = torch.tensor(float('nan'))\n",
    "\n",
    "    # Compute mean accuracy\n",
    "    mean_accuracy = torch.nanmean(accuracy).item()\n",
    "\n",
    "    if return_std:\n",
    "        # Compute standard deviation, ignoring NaN values\n",
    "        stddev_accuracy = np.nanstd(accuracy)\n",
    "        return mean_accuracy, stddev_accuracy\n",
    "\n",
    "    return mean_accuracy\n",
    "\n",
    "\n",
    "def compute_precision_excluding_cases_from_cumulator(TPs, FPs, FNs, TNs, return_std=False, exclude_only_zero_denominator=False):\n",
    "    \"\"\"\n",
    "    Computes precision for cases with a non-zero denominator and excludes cases where there are no ground truth positives.\n",
    "\n",
    "    Args:\n",
    "        TPs: List of tensors for true positives across batches.\n",
    "        FPs: List of tensors for false positives across batches.\n",
    "        FNs: List of tensors for false negatives across batches.\n",
    "        TNs: List of tensors for true negatives across batches.\n",
    "        return_std: Boolean indicating whether to return the standard deviation.\n",
    "\n",
    "    Returns:\n",
    "        mean_precision: Mean precision across valid cases.\n",
    "        (optional) stddev_precision: Standard deviation of precision across valid cases if return_std is True.\n",
    "    \"\"\"\n",
    "    tp = torch.cat([tp for tp in TPs])\n",
    "    fp = torch.cat([fp for fp in FPs])\n",
    "    fn = torch.cat([fn for fn in FNs])\n",
    "    tn = torch.cat([tn for tn in TNs])\n",
    "\n",
    "    # Compute the denominator for precision (tp + fp)\n",
    "    denominator = tp + fp\n",
    "\n",
    "    if exclude_only_zero_denominator:\n",
    "        valid_mask = denominator != 0\n",
    "    else:\n",
    "        # Exclude cases with zero denominator or no ground truth positives\n",
    "        valid_mask = (denominator != 0) & ((tp + fn) != 0)\n",
    "\n",
    "    # Compute precision only for valid cases\n",
    "    precision = torch.zeros_like(denominator, dtype=torch.float)\n",
    "    precision[valid_mask] = tp[valid_mask] / denominator[valid_mask]\n",
    "\n",
    "    # Exclude invalid cases by setting them to NaN\n",
    "    precision[~valid_mask] = torch.tensor(float('nan'))\n",
    "\n",
    "    # Compute mean precision\n",
    "    mean_precision = torch.nanmean(precision).item()\n",
    "\n",
    "    if return_std:\n",
    "        # Compute standard deviation, ignoring NaN values\n",
    "        stddev_precision = np.nanstd(precision)\n",
    "        return mean_precision, stddev_precision\n",
    "\n",
    "    return mean_precision\n",
    "\n",
    "\n",
    "\n",
    "def compute_recall_excluding_cases_from_cumulator(TPs, FPs, FNs, TNs, return_std=False, exclude_only_zero_denominator=False):\n",
    "    \"\"\"\n",
    "    Computes recall for class 1, excluding cases with a zero denominator or no ground truth positives.\n",
    "\n",
    "    Args:\n",
    "        TPs: List of tensors for true positives across batches.\n",
    "        FPs: List of tensors for false positives across batches.\n",
    "        FNs: List of tensors for false negatives across batches.\n",
    "        TNs: List of tensors for true negatives across batches.\n",
    "        return_std: Boolean indicating whether to return the standard deviation.\n",
    "\n",
    "    Returns:\n",
    "        mean_recall: Mean recall across valid cases.\n",
    "        (optional) stddev_recall: Standard deviation of recall across valid cases if return_std is True.\n",
    "    \"\"\"\n",
    "    tp = torch.cat([tp for tp in TPs])\n",
    "    fp = torch.cat([fp for fp in FPs])\n",
    "    fn = torch.cat([fn for fn in FNs])\n",
    "    tn = torch.cat([tn for tn in TNs])\n",
    "\n",
    "    # Compute the denominator for recall (tp + fn)\n",
    "    denominator = tp + fn\n",
    "\n",
    "    if exclude_only_zero_denominator:\n",
    "        valid_mask = denominator != 0\n",
    "    else:\n",
    "        # Exclude cases with zero denominator or no ground truth positives\n",
    "        valid_mask = (denominator != 0) & ((tp + fn) != 0)\n",
    "\n",
    "    # Compute recall only for valid cases\n",
    "    recall = torch.zeros_like(denominator, dtype=torch.float)\n",
    "    recall[valid_mask] = tp[valid_mask] / denominator[valid_mask]\n",
    "\n",
    "    # Exclude invalid cases by setting them to NaN\n",
    "    recall[~valid_mask] = torch.tensor(float('nan'))\n",
    "\n",
    "    # Compute mean recall\n",
    "    mean_recall = torch.nanmean(recall).item()\n",
    "\n",
    "    if return_std:\n",
    "        # Compute standard deviation, ignoring NaN values\n",
    "        stddev_recall = np.nanstd(recall)\n",
    "        return mean_recall, stddev_recall\n",
    "\n",
    "    return mean_recall\n",
    "\n",
    "\n",
    "def compute_f1_excluding_cases_from_cumulator(TPs, FPs, FNs, TNs, return_std=False, exclude_only_zero_denominator=False):\n",
    "    \"\"\"\n",
    "    Computes F1 score for each 'case' (sample), excluding invalid cases.\n",
    "    Invalid cases may be:\n",
    "      - Those with zero denominator (tp + 0.5*(fp + fn) = 0).\n",
    "      - Those with no ground-truth positives (tp + fn = 0), depending on the 'exclude_only_zero_denominator' flag.\n",
    "\n",
    "    Args:\n",
    "        TPs: List of tensors for true positives across batches.\n",
    "        FPs: List of tensors for false positives across batches.\n",
    "        FNs: List of tensors for false negatives across batches.\n",
    "        TNs: List of tensors for true negatives across batches.\n",
    "        return_std: Boolean indicating whether to return standard deviation.\n",
    "        exclude_only_zero_denominator: If True, we only exclude cases where (2*tp + fp + fn) = 0.\n",
    "                                       If False, we also exclude cases with no ground-truth positives (tp + fn = 0).\n",
    "\n",
    "    Returns:\n",
    "        mean_f1: Mean F1 across valid cases (float).\n",
    "        (optional) stddev_f1: Standard deviation of F1 across valid cases (float),\n",
    "                              only returned if return_std is True.\n",
    "    \"\"\"\n",
    "    # 1. Concatenate all batches\n",
    "    tp = torch.cat([tp for tp in TPs])\n",
    "    fp = torch.cat([fp for fp in FPs])\n",
    "    fn = torch.cat([fn for fn in FNs])\n",
    "    tn = torch.cat([tn for tn in TNs])\n",
    "\n",
    "    # 2. Compute the per-case denominator for F1 = 2*TP / (2*TP + FP + FN)\n",
    "    denominator = tp + 0.5*(fp + fn)\n",
    "\n",
    "    # 3. Determine valid cases\n",
    "    #    If 'exclude_only_zero_denominator' is True, only exclude denominator == 0.\n",
    "    #    Otherwise, also exclude cases where there are no positives in ground truth (tp+fn=0).\n",
    "    if exclude_only_zero_denominator:\n",
    "        valid_mask = (denominator != 0)\n",
    "    else:\n",
    "        valid_mask = (denominator != 0) & ((tp + fn) != 0)\n",
    "\n",
    "    # 4. Allocate a tensor to hold the per-case F1\n",
    "    f1 = torch.zeros_like(denominator, dtype=torch.float)\n",
    "\n",
    "    # 5. Compute F1 only for valid cases\n",
    "    f1[valid_mask] = (tp[valid_mask]) / denominator[valid_mask]\n",
    "\n",
    "    # 6. Mark invalid cases as NaN for later exclusion in mean/std computations\n",
    "    f1[~valid_mask] = torch.tensor(float('nan'))\n",
    "\n",
    "    # 7. Compute mean F1 (ignoring NaNs)\n",
    "    mean_f1 = torch.nanmean(f1).item()\n",
    "\n",
    "    if return_std:\n",
    "        # 8. Compute standard deviation (ignoring NaNs)\n",
    "        stddev_f1 = np.nanstd(f1.cpu().numpy())\n",
    "        return mean_f1, stddev_f1\n",
    "\n",
    "    return mean_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "dataset_base_path = 'Dataset-arrays-4-FINAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed 200...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = reseed()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_mask = False\n",
    "get_boundaryloss=True\n",
    "num_workers = os.cpu_count()*0.2\n",
    "image_only = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATCHES AND SUBTRACTED 3RD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_third_images_path_prefixes = (\"Dataset-arrays-4\", \"Dataset-arrays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_patches_sub= 86.13536834716797\n",
    "std_patches_sub= 238.13461303710938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms_patches_sub = Compose(\n",
    "        [\n",
    "            LoadImaged(keys = [\"image\", \"label\"], image_only = image_only,reader=monai.data.NumpyReader()),\n",
    "            EnsureChannelFirstd(keys = [\"image\", \"label\"]),\n",
    "            Preprocess(has_mask =has_mask,keys=None, mode='test', get_boundaryloss=get_boundaryloss, subtracted_images_path_prefixes=sub_third_images_path_prefixes, subtrahend = mean_patches_sub, divisor = std_patches_sub,get_patches=True)\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NO THORAX AND SUBTRACTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_third_images_path_prefixes = (\"Dataset-arrays-4\", \"Dataset-arrays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_no_thorax_third_sub= 43.1498\n",
    "std_no_thorax_third_sub= 172.6704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms_no_thorax_third_sub  = Compose(\n",
    "        [\n",
    "            LoadImaged(keys = [\"image\", \"label\"], image_only = image_only,reader=monai.data.NumpyReader()),\n",
    "            EnsureChannelFirstd(keys = [\"image\", \"label\"]),\n",
    "            Preprocess(has_mask =has_mask,keys=None, mode='test', get_boundaryloss=get_boundaryloss, subtracted_images_path_prefixes=sub_third_images_path_prefixes, subtrahend = mean_no_thorax_third_sub, divisor = std_no_thorax_third_sub, get_patches=False)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import label as labell, find_objects\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "def calculate_iou_for_mass_detection(mask1, mask2):\n",
    "        # Calculate intersection over union for two masks\n",
    "        intersection = np.logical_and(mask1, mask2).sum()\n",
    "        union = np.logical_or(mask1, mask2).sum()\n",
    "        if union == 0:\n",
    "            return 0\n",
    "        return intersection / union\n",
    "\n",
    "\n",
    "def calculate_mass_detection_iou(y_pred,y_true):\n",
    "    structure = np.ones((3,3), dtype=np.bool_)  # 2D connectivity\n",
    "    labels_true, num_true = labell(y_true, structure=structure)\n",
    "    labels_pred, num_pred = labell(y_pred, structure=structure)\n",
    "\n",
    "    if num_true == 0:\n",
    "        return float('nan')\n",
    "    \n",
    "    detected_masses = 0\n",
    "    true_objects = find_objects(labels_true)\n",
    "    pred_objects = find_objects(labels_pred)\n",
    "    \n",
    "    for i, true_slice in enumerate(true_objects):\n",
    "        for j, pred_slice in enumerate(pred_objects):\n",
    "            if not check_overlap(true_slice, pred_slice):\n",
    "                continue  # Skip if bounding boxes don't overlap\n",
    "            # Calculate the IoU only for the overlapping region\n",
    "            overlap_region = tuple(\n",
    "                slice(max(t.start, p.start), min(t.stop, p.stop))\n",
    "                for t, p in zip(true_slice, pred_slice)\n",
    "            )\n",
    "            true_mass = labels_true == (i + 1)\n",
    "            pred_mass = labels_pred == (j + 1)\n",
    "            if np.any(true_mass[overlap_region]) and np.any(pred_mass[overlap_region]):\n",
    "                    detected_masses += 1\n",
    "                    break  # Found an overlapping mass, move to the next true mass\n",
    "    \n",
    "    detection_rate = detected_masses / num_true\n",
    "    return detection_rate\n",
    "\n",
    "def check_overlap(slice1, slice2):\n",
    "    # Check if two slices overlap\n",
    "    for dim1, dim2 in zip(slice1, slice2):\n",
    "        if dim1.stop <= dim2.start or dim2.stop <= dim1.start:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def calculate_mass_detection_imagewise_volume(y_pred, y_true):\n",
    "\n",
    "    detection_rates = []\n",
    "    for idx in range(0, y_pred.shape[-1]):\n",
    "        slice_pred = y_pred[:,:, idx]\n",
    "        slice_true = y_true[:,:, idx]\n",
    "\n",
    "    \n",
    "        structure = np.ones((3, 3), dtype=np.bool_)  # 3D connectivity\n",
    "        labels_true, num_true = labell(slice_true, structure=structure)\n",
    "        labels_pred, num_pred = labell(slice_pred, structure=structure)\n",
    "\n",
    "        if num_true != 0:\n",
    "            detected_masses = 0\n",
    "            true_objects = find_objects(labels_true)\n",
    "            pred_objects = find_objects(labels_pred)\n",
    "            \n",
    "            for i, true_slice in enumerate(true_objects):\n",
    "                for j, pred_slice in enumerate(pred_objects):\n",
    "                    if not check_overlap(true_slice, pred_slice):\n",
    "                        continue  # Skip if bounding boxes don't overlap\n",
    "                    # Calculate the IoU only for the overlapping region\n",
    "                    overlap_region = tuple(\n",
    "                        slice(max(t.start, p.start), min(t.stop, p.stop))\n",
    "                        for t, p in zip(true_slice, pred_slice)\n",
    "                    )\n",
    "                    true_mass = labels_true == (i + 1)\n",
    "                    pred_mass = labels_pred == (j + 1)\n",
    "                    if np.any(true_mass[overlap_region]) and np.any(pred_mass[overlap_region]):\n",
    "                            detected_masses += 1\n",
    "                            break  # Found an overlapping mass, move to the next true mass\n",
    "            \n",
    "            detection_rate = detected_masses / num_true\n",
    "            detection_rates.append(detection_rate)\n",
    "            \n",
    "        \n",
    "    return detection_rates\n",
    "\n",
    "def calculate_mass_detection_iou_volume_get_rates(y_pred, y_true):\n",
    "    structure = np.ones((3, 3, 3), dtype=np.bool_)  # 3D connectivity\n",
    "    labels_true, num_true = labell(y_true, structure=structure)\n",
    "    labels_pred, num_pred = labell(y_pred, structure=structure)\n",
    "    \n",
    "    detected_masses = 0\n",
    "    true_objects = find_objects(labels_true)\n",
    "    pred_objects = find_objects(labels_pred)\n",
    "    \n",
    "    for i, true_slice in enumerate(true_objects):\n",
    "        for j, pred_slice in enumerate(pred_objects):\n",
    "            if not check_overlap(true_slice, pred_slice):\n",
    "                continue  # Skip if bounding boxes don't overlap\n",
    "            # Calculate the IoU only for the overlapping region\n",
    "            overlap_region = tuple(\n",
    "                slice(max(t.start, p.start), min(t.stop, p.stop))\n",
    "                for t, p in zip(true_slice, pred_slice)\n",
    "            )\n",
    "            true_mass = labels_true == (i + 1)\n",
    "            pred_mass = labels_pred == (j + 1)\n",
    "            if np.any(true_mass[overlap_region]) and np.any(pred_mass[overlap_region]):\n",
    "                    detected_masses += 1\n",
    "                    break  # Found an overlapping mass, move to the next true mass\n",
    "    \n",
    "    return detected_masses, num_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = os.listdir(dataset_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train+val (80%) and test (20%)\n",
    "x_train_val, x_test= train_test_split(patient_ids, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Split the train+val dataset into train (60% of the original dataset) and val (20% of the original dataset)\n",
    "x_train, x_val = train_test_split(x_train_val, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 228/228 [00:16<00:00, 13.86it/s]\n",
      "Loading dataset: 100%|██████████| 228/228 [00:48<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 192/192 [00:14<00:00, 13.52it/s]\n",
      "Loading dataset: 100%|██████████| 192/192 [00:39<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 228/228 [00:16<00:00, 13.80it/s]\n",
      "Loading dataset: 100%|██████████| 228/228 [00:50<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 204/204 [00:15<00:00, 12.82it/s]\n",
      "Loading dataset: 100%|██████████| 204/204 [00:44<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 188/188 [00:13<00:00, 13.89it/s]\n",
      "Loading dataset: 100%|██████████| 188/188 [00:40<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 228/228 [00:17<00:00, 12.97it/s]\n",
      "Loading dataset: 100%|██████████| 228/228 [00:48<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 132/132 [00:10<00:00, 12.58it/s]\n",
      "Loading dataset: 100%|██████████| 132/132 [00:29<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 228/228 [00:16<00:00, 14.23it/s]\n",
      "Loading dataset: 100%|██████████| 228/228 [00:48<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 188/188 [00:12<00:00, 14.77it/s]\n",
      "Loading dataset: 100%|██████████| 188/188 [00:38<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 228/228 [00:18<00:00, 12.44it/s]\n",
      "Loading dataset: 100%|██████████| 228/228 [00:48<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 236/236 [00:17<00:00, 13.62it/s]\n",
      "Loading dataset: 100%|██████████| 236/236 [00:50<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 228/228 [00:18<00:00, 12.54it/s]\n",
      "Loading dataset: 100%|██████████| 228/228 [00:50<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 188/188 [00:15<00:00, 11.82it/s]\n",
      "Loading dataset: 100%|██████████| 188/188 [00:40<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 284/284 [00:22<00:00, 12.82it/s]\n",
      "Loading dataset: 100%|██████████| 284/284 [01:00<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 96/96 [00:08<00:00, 11.19it/s]\n",
      "Loading dataset: 100%|██████████| 96/96 [00:22<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 80/80 [00:08<00:00,  8.92it/s]\n",
      "Loading dataset: 100%|██████████| 80/80 [00:18<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 112/112 [00:09<00:00, 12.33it/s]\n",
      "Loading dataset: 100%|██████████| 112/112 [00:24<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 96/96 [00:08<00:00, 11.39it/s]\n",
      "Loading dataset: 100%|██████████| 96/96 [00:21<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 204/204 [00:16<00:00, 12.68it/s]\n",
      "Loading dataset: 100%|██████████| 204/204 [00:45<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 212/212 [00:18<00:00, 11.72it/s]\n",
      "Loading dataset: 100%|██████████| 212/212 [00:48<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 112/112 [00:10<00:00, 10.47it/s]\n",
      "Loading dataset: 100%|██████████| 112/112 [00:24<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for patient_id in x_test:\n",
    "    print(patient_id)\n",
    "\n",
    "\n",
    "    patient_id = [patient_id]\n",
    "    images_fnames, _ = get_filenames(suffix=\"images\",\n",
    "                                       base_path='Dataset-arrays-4-FINAL',\n",
    "                                       patient_ids=patient_id,\n",
    "                                       remove_black_samples=False,\n",
    "                                       get_random_samples_and_remove_black_samples=False,\n",
    "                                       random_samples_indexes_list=None)\n",
    "\n",
    "    labels_fnames, _ = get_filenames(suffix=\"masks\",\n",
    "                                      base_path='Dataset-arrays-4-FINAL',\n",
    "                                      patient_ids=patient_id,\n",
    "                                      remove_black_samples=False,\n",
    "                                      get_random_samples_and_remove_black_samples=False,\n",
    "                                      random_samples_indexes_list=None, remove_picked_samples=False)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    test_dicts = [{\"image\": image_name, \"label\":label_name} for image_name, label_name in zip(images_fnames,labels_fnames)]\n",
    "\n",
    "    #no_thorax_third_test_ds = CacheDataset(data=test_dicts, transform=test_transforms_no_thorax_third,num_workers)\n",
    "    no_thorax_sub_test_ds = CacheDataset(data=test_dicts, transform=test_transforms_no_thorax_third_sub,num_workers=32)\n",
    "    #patches_third_test_ds = CacheDataset(data=test_dicts, transform=test_transforms_patches_third, num_workers)\n",
    "    patches_sub_test_ds = CacheDataset(data=test_dicts, transform=test_transforms_patches_sub,num_workers=32)\n",
    "\n",
    "    datasets[patient_id[0]]={\n",
    "        #\"no_thorax_third_test_ds\":no_thorax_third_test_ds,\n",
    "        \"no_thorax_sub_test_ds\": no_thorax_sub_test_ds,\n",
    "        #\"patches_third_test_ds\": patches_third_test_ds,\n",
    "        \"patches_sub_test_ds\": patches_sub_test_ds\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LAXXX', 'AS0170', 'PR0760', 'GF0380', 'FP211261', 'D2MP3(VR)', 'MG0477', 'OL1062R', 'BV1252', 'D1AP7(VR)', 'SD080569', 'CC0167', 'RHCL031174', 'LA0248', 'RP271052', 'SL191251', 'LGM0159(1,5)', 'PA150139', 'HF230274', 'CF160366', 'GLA1074'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATIENT AWARE TEST FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_segmentations(model1_prob, model2_prob, prob_threshold=0.5, boost_factor=1.5, penalty_factor=0.5, kernel_size=3):\n",
    "\n",
    "    \n",
    "    model1_prob = np.squeeze(model1_prob)\n",
    "    model2_prob = np.squeeze(model2_prob)\n",
    "    # Step 1: Check where both models agree above the probability threshold\n",
    "    agreement = np.logical_and(model1_prob > prob_threshold, model2_prob > prob_threshold)# Step 2: Create a kernel for dilation\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    # Step 3: Dilate the agreement area to enlarge it\n",
    "    enlarged_agreement = cv2.dilate(agreement.astype(np.uint8), kernel)\n",
    "    \n",
    "    # Step 4: Sum the probabilities of both models\n",
    "    prob_sum = model1_prob + model2_prob\n",
    "    \n",
    "    # Step 5: Boost the probability sum where there is agreement\n",
    "    prob_sum[enlarged_agreement > 0] *= boost_factor\n",
    "    \n",
    "    # Step 6: Identify disagreement (where there is no enlarged agreement)\n",
    "    disagreement = enlarged_agreement == 0\n",
    "    \n",
    "    # Step 7: Apply penalty factor where there is disagreement\n",
    "    prob_sum[disagreement] *= penalty_factor\n",
    "    \n",
    "    # Step 8: Normalize the probability sum to get the fused probability, ensuring it's within [0, 1]\n",
    "    fused_prob = np.clip(prob_sum / 2.0, 0, 1)\n",
    "    \n",
    "    return fused_prob\n",
    "\n",
    "def filter_masses(volume, min_slices=7, window_size=3):\n",
    "    \"\"\"\n",
    "    Filters out masses in a 3D volume (HxWxB) that do not consecutively appear in at least 'min_slices' slices,\n",
    "    considering a window around each mass. This function uses cv2 for dilation and assumes binary masks as input.\n",
    "    \n",
    "    :param volume: 3D numpy array of shape (H, W, B) representing a volume of binary masks.\n",
    "    :param min_slices: Minimum number of consecutive slices a mass must appear in to be kept.\n",
    "    :param window_size: Diameter of the window used for dilation to connect varying shapes.\n",
    "    :return: Filtered 3D volume.\n",
    "    \"\"\"\n",
    "    volume_copy = np.copy(volume)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (window_size, window_size))\n",
    "    \n",
    "    for i in range(volume_copy.shape[2]):\n",
    "        volume_copy[:, :, i] = cv2.dilate(volume_copy[:, :, i].astype(np.uint8), kernel, iterations=1)\n",
    "    \n",
    "    volume_copy_transposed = np.transpose(volume_copy, (2, 0, 1))\n",
    "    volume_transposed = np.transpose(volume, (2, 0, 1))\n",
    "    \n",
    "    structure = generate_binary_structure(3, 1)\n",
    "    labeled_volume, num_features = labell(volume_copy_transposed, structure=structure)\n",
    "    print(f\"num features: {num_features}\")\n",
    "    for feature_id in range(1, num_features + 1):\n",
    "        print(feature_id)\n",
    "        feature_mask = labeled_volume == feature_id\n",
    "        slice_presence_count = np.sum(np.any(feature_mask, axis=(1, 2)))\n",
    "    \n",
    "        if slice_presence_count < min_slices:\n",
    "            volume_transposed[labeled_volume == feature_id] = 0  # Filter out the feature\n",
    "    \n",
    "    filtered_volume = np.transpose(volume_transposed, (1, 2, 0))\n",
    "    print(\"fine\")\n",
    "    return filtered_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_patient_aware_no_patches(model_path, patient_ids, datasets, dataset_key, filter=False):\n",
    "\n",
    "    model = BreastModel2.load_from_checkpoint(model_path, strict=False)\n",
    "\n",
    "    model_class_mean_iou = []\n",
    "    model_class_mean_dice = []\n",
    "    model_detection_iou = []\n",
    "    \n",
    "    model_iou_mass_volume = []\n",
    "    model_iou_mass_volume_no_empty = []\n",
    "    \n",
    "    model_dice_mass_volume = []\n",
    "    model_dice_mass_volume_no_empty = []\n",
    "\n",
    "    model_accuracy = []\n",
    "    model_precision = []\n",
    "    model_recall = []\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        \n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "    \n",
    "         TP = []\n",
    "         FP = []\n",
    "         FN = []\n",
    "         TN = []\n",
    "\n",
    "         detection_rates =  []\n",
    "    \n",
    "         print(patient_id)\n",
    "         dataset = datasets[patient_id][dataset_key]\n",
    "        \n",
    "         for idx, e in tqdm(enumerate(dataset), total = len(dataset)):\n",
    "            original_image = np.load(e['image_meta_dict']['filename_or_obj'])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "\n",
    "            gt_label = np.load(e['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "\n",
    "            if e['keep_sample']:\n",
    "                image = torch.unsqueeze(e['image'], 0)\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    model = model.to(\"cuda\")\n",
    "                    model.eval()\n",
    "                    masks = model(image.to(\"cuda\"))[0]\n",
    "                    masks = masks.sigmoid()\n",
    "                    \n",
    "                pred_label = masks[0]\n",
    "                pred_label = (pred_label > 0.4).int()\n",
    "                pred_label = reverse_transformations(dataset[idx], pred_label, mode='whole')\n",
    "                \n",
    "                pred_label = monai.transforms.Resize(spatial_size=(original_image.shape[1], original_image.shape[2]), mode='neares-exact')(pred_label)\n",
    "            else:\n",
    "                pred_label = torch.zeros(original_image.shape, dtype=torch.uint8)\n",
    "\n",
    "            if not filter:\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(np.expand_dims(pred_label,0).astype(int)), torch.tensor(np.expand_dims(gt_label,0).astype(int)), mode = \"binary\")\n",
    "                TP.append(tp)\n",
    "                FP.append(fp)\n",
    "                FN.append(fn)\n",
    "                TN.append(tn)\n",
    "                \n",
    "            \"\"\"plt.figure(figsize=(15, 10))\n",
    "    \n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.imshow(original_image.squeeze(),  cmap='gray')  # convert CHW -> HWC\n",
    "            plt.title(\"Image\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.imshow(label_whole, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Whole\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.imshow(label_patches, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Patch\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.imshow(gt_label, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"GT\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.show()\n",
    "    \n",
    "            plt.imshow(fusion , cmap='gray')\n",
    "            plt.show()\"\"\"\n",
    "    \n",
    "    \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    "                    \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "    \n",
    "         if filter:\n",
    "             predicted_label_volume = filter_masses(predicted_label_volume, min_slices=3, window_size=3) # H x W x N\n",
    "             # H x W x N -> N x H x W -> N x 1 x H x W\n",
    "             predicted_label_volume_for_stats = np.transpose(predicted_label_volume, (2, 0, 1))\n",
    "             predicted_label_volume_for_stats = np.expand_dims(predicted_label_volume_for_stats, 1)  # N x 1 x H x W\n",
    "\n",
    "             gt_label_volume_for_stats = np.transpose(gt_label_volume, (2, 0, 1))\n",
    "             gt_label_volume_for_stats = np.expand_dims(gt_label_volume_for_stats, 1)  # N x 1 x H x W             \n",
    "\n",
    "             \n",
    "             tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(predicted_label_volume_for_stats.astype(int)), torch.tensor(gt_label_volume_for_stats.astype(int)), mode = \"binary\")\n",
    "             TP =  [torch.tensor([[elem]]) for elem in tp.squeeze()]\n",
    "             FP =  [torch.tensor([[elem]]) for elem in fp.squeeze()]\n",
    "             FN =  [torch.tensor([[elem]]) for elem in fn.squeeze()]\n",
    "             TN =  [torch.tensor([[elem]]) for elem in tn.squeeze()]\n",
    "\n",
    "    \n",
    "         detection_iou =  np.array(calculate_mass_detection_imagewise_volume(predicted_label_volume.astype(int), gt_label_volume)).mean()\n",
    "         \n",
    "         mean_iou = compute_mean_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True)\n",
    "         mean_dice = compute_mean_dice_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True)\n",
    "\n",
    "         iou_mass_volume = compute_iou_npy(gt_label_volume, predicted_label_volume, class_id=1, reduction='micro-imagewise')\n",
    "         iou_mass_volume_no_empty = compute_iou_npy(gt_label_volume, predicted_label_volume, class_id=1, reduction='micro-imagewise', exclude_empty=True)\n",
    "         dice_mass_volume = compute_dice_score_npy(gt_label_volume, predicted_label_volume,reduction='micro-imagewise', class_id=1)\n",
    "         dice_mass_volume_no_empty = compute_dice_score_npy(gt_label_volume, predicted_label_volume, reduction='micro-imagewise',class_id=1, exclude_empty=True)\n",
    "        \n",
    "         accuracy = compute_accuracy_from_cumulator(TP, FP, FN, TN)\n",
    "         precision = compute_precision_from_cumulator(TP, FP, FN, TN)\n",
    "         recall = compute_recall_from_cumulator(TP, FP, FN, TN)\n",
    "        \n",
    "         print(\"CLASS MEAN IOU\", mean_iou)\n",
    "         print(\"CLASS MEAN DICE\", mean_dice)\n",
    "         print(\"DIOU\", detection_iou)\n",
    "         print(\"IOU MASS VOLUME\", iou_mass_volume)\n",
    "         print(\"IOU MASS VOLUME NO EMPTY\", iou_mass_volume_no_empty)\n",
    "         print(\"DICE MASS VOLUME \", dice_mass_volume)\n",
    "         print(\"DICE MASS VOLUME NO EMPTY \", dice_mass_volume_no_empty)\n",
    "\n",
    "         print(\"ACCURACY \", accuracy)\n",
    "         print(\"PRECISION \", precision)\n",
    "         print(\"RECALL\", recall)\n",
    "         \n",
    "         print()\n",
    "         model_class_mean_iou.append(mean_iou)\n",
    "         model_class_mean_dice.append(mean_dice)\n",
    "         model_detection_iou.append(detection_iou)\n",
    "        \n",
    "         model_iou_mass_volume.append(iou_mass_volume)\n",
    "         model_iou_mass_volume_no_empty.append(iou_mass_volume_no_empty)\n",
    "        \n",
    "         model_dice_mass_volume.append(dice_mass_volume)\n",
    "         model_dice_mass_volume_no_empty.append(dice_mass_volume_no_empty)\n",
    "\n",
    "         model_accuracy.append(accuracy)\n",
    "         model_precision.append(precision)\n",
    "         model_recall.append(recall)\n",
    "\n",
    "    model_class_mean_iou = np.array(model_class_mean_iou).mean()\n",
    "    model_class_mean_dice = np.array(model_class_mean_dice).mean()\n",
    "    model_detection_iou = np.array(model_detection_iou).mean()\n",
    "    \n",
    "    model_iou_mass_volume = np.array(model_iou_mass_volume).mean()\n",
    "    model_iou_mass_volume_no_empty = np.array(model_iou_mass_volume_no_empty).mean()\n",
    "    \n",
    "    model_dice_mass_volume = np.array(model_dice_mass_volume).mean()\n",
    "    model_dice_mass_volume_no_empty = np.array(model_dice_mass_volume_no_empty).mean()\n",
    "\n",
    "    model_accuracy = np.array(model_accuracy).mean()\n",
    "    model_precision = np.array(model_precision).mean()\n",
    "    model_recall = np.array(model_recall).mean()\n",
    "\n",
    "    print(\"MODEL CLASS MEAN IOU\", model_class_mean_iou)\n",
    "    print(\"MODEL CLASS MEAN DICE\", model_class_mean_dice)\n",
    "    print(\"MODEL DIOU\", model_detection_iou)\n",
    "        \n",
    "    print(\"MODEL IOU MASS VOLUME\", model_iou_mass_volume)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY\", model_iou_mass_volume_no_empty)\n",
    "    \n",
    "    print(\"MODEL DICE MASS VOLUME \", model_dice_mass_volume)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY \", model_dice_mass_volume_no_empty)\n",
    "\n",
    "    print(\"MODEL ACCURACY \", model_accuracy)\n",
    "    print(\"MODEL PRECISION \", model_precision)\n",
    "    print(\"MODEL RECALL\", model_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_aware_no_patches(model_path, patient_ids, datasets, dataset_key, filter=False, strict=False, get_scores_for_statistics=False,get_only_masses=False,arch_name=False):\n",
    "\n",
    "    if arch_name:\n",
    "        model = BreastModel2.load_from_checkpoint(model_path, strict=strict, arch=arch_name)\n",
    "\n",
    "    else:\n",
    "\n",
    "        model = BreastModel2.load_from_checkpoint(model_path, strict=strict)\n",
    "\n",
    "    model_class_mean_iou = []\n",
    "    model_class_mean_dice = []\n",
    "    model_detection_iou = []\n",
    "    \n",
    "    model_iou_mass_volume = []\n",
    "    model_iou_mass_volume_no_empty = []\n",
    "    \n",
    "    model_dice_mass_volume = []\n",
    "    model_dice_mass_volume_no_empty = []\n",
    "\n",
    "    model_accuracy = []\n",
    "    model_precision = []\n",
    "    model_recall = []\n",
    "\n",
    "    TP = []\n",
    "    FP = []\n",
    "    FN = []\n",
    "    TN = []\n",
    "    detection_iou=[]\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "        \n",
    "         print(patient_id)\n",
    "         dataset = datasets[patient_id][dataset_key]\n",
    "        \n",
    "         for idx, e in tqdm(enumerate(dataset), total = len(dataset)):\n",
    "            original_image = np.load(e['image_meta_dict']['filename_or_obj'])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "\n",
    "            gt_label = np.load(e['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "\n",
    "            if e['keep_sample']:\n",
    "                image = torch.unsqueeze(e['image'], 0)\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    model = model.to(\"cuda\")\n",
    "                    model.eval()\n",
    "                    if arch_name:\n",
    "                            masks = model(image.to(\"cuda\"))[0]\n",
    "                    else:\n",
    "                            masks = model(image.to(\"cuda\"))[0]\n",
    "                    masks = masks.sigmoid()\n",
    "                    \n",
    "                pred_label = masks[0]\n",
    "                pred_label = (pred_label > 0.4).int()\n",
    "                pred_label = torch.squeeze(pred_label)\n",
    "                pred_label = torch.unsqueeze(pred_label,0)\n",
    "                pred_label = reverse_transformations(dataset[idx], pred_label, mode='whole')\n",
    "                \n",
    "                pred_label = monai.transforms.Resize(spatial_size=(original_image.shape[1], original_image.shape[2]), mode='nearest-exact')(pred_label)\n",
    "            else:\n",
    "                pred_label = torch.zeros(original_image.shape, dtype=torch.uint8)\n",
    "\n",
    "            if not filter:\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(np.expand_dims(pred_label,0).astype(int)), torch.tensor(np.expand_dims(gt_label,0).astype(int)), mode = \"binary\")\n",
    "                TP.append(tp)\n",
    "                FP.append(fp)\n",
    "                FN.append(fn)\n",
    "                TN.append(tn)\n",
    "            \n",
    "            \"\"\"plt.figure(figsize=(15, 10))\n",
    "    \n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.imshow(original_image.squeeze(),  cmap='gray')  # convert CHW -> HWC\n",
    "            plt.title(\"Image\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.imshow(label_whole, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Whole\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.imshow(label_patches, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Patch\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.imshow(gt_label, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"GT\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.show()\n",
    "    \n",
    "            plt.imshow(fusion , cmap='gray')\n",
    "            plt.show()\"\"\"\n",
    "    \n",
    "    \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    "                    \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "    \n",
    "         if filter:\n",
    "             predicted_label_volume = filter_masses(predicted_label_volume, min_slices=3, window_size=3) # H x W x N\n",
    "             # H x W x N -> N x H x W -> N x 1 x H x W\n",
    "             predicted_label_volume_for_stats = np.transpose(predicted_label_volume, (2, 0, 1))\n",
    "             predicted_label_volume_for_stats = np.expand_dims(predicted_label_volume_for_stats, 1)  # N x 1 x H x W\n",
    "\n",
    "             gt_label_volume_for_stats = np.transpose(gt_label_volume, (2, 0, 1))\n",
    "             gt_label_volume_for_stats = np.expand_dims(gt_label_volume_for_stats, 1)  # N x 1 x H x W             \n",
    "            \n",
    "             \n",
    "             tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(predicted_label_volume_for_stats.astype(int)), torch.tensor(gt_label_volume_for_stats.astype(int)), mode = \"binary\")\n",
    "             TP +=  [torch.tensor([[elem]]) for elem in tp.squeeze()]\n",
    "             FP +=  [torch.tensor([[elem]]) for elem in fp.squeeze()]\n",
    "             FN +=  [torch.tensor([[elem]]) for elem in fn.squeeze()]\n",
    "             TN +=  [torch.tensor([[elem]]) for elem in tn.squeeze()]\n",
    "\n",
    "         detection_iou+=calculate_mass_detection_imagewise_volume(predicted_label_volume.astype(int), gt_label_volume)\n",
    "\n",
    "    model_detection_iou = np.array(detection_iou).mean()\n",
    "    model_detection_iou_std = np.array(detection_iou).std()\n",
    "    \n",
    "    model_class_mean_iou, model_class_std_iou = compute_mean_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "    model_class_mean_dice, model_class_std_dice = compute_mean_dice_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "\n",
    "    model_iou_mass_volume , model_iou_mass_volume_std = compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=False, return_std=True)\n",
    "    model_iou_mass_volume_no_empty, model_iou_mass_volume_no_empty_std =compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "    model_iou_mass_volume_no_empty_optimistic, model_iou_mass_volume_no_empty_optimistic_std =compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, exclude_empty_only_gt=True, return_std=True)\n",
    "    \n",
    "    \n",
    "    model_dice_mass_volume, model_dice_mass_volume_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=False, return_std=True)\n",
    "    model_dice_mass_volume_no_empty, model_dice_mass_volume_no_empty_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=True, return_std=True)\n",
    "    model_dice_mass_volume_no_empty_optimistic, model_dice_mass_volume_no_empty_optimistic_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=True, exclude_empty_only_gt=True,return_std=True)\n",
    "    \n",
    "    model_mean_accuracy_no_empty, model_mean_accuracy_no_empty_std = compute_accuracy_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_precision_no_empty,model_mean_precision_no_empty_std = compute_precision_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_recall_no_empty, model_mean_recall_no_empty_std = compute_recall_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_f1_no_empty, model_mean_f1_no_empty_std = compute_f1_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "        \n",
    "    model_accuracy_excluding_cases, model_accuracy_excluding_cases_std = compute_accuracy_excluding_cases(TP, FP, FN, TN, return_std=True)\n",
    "    model_precision_excluding_cases,model_precision_excluding_cases_std =compute_precision_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True)\n",
    "    model_recall_excluding_cases,model_recall_excluding_cases_std  =compute_recall_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True)\n",
    "\n",
    "    model_accuracy_no_empty, model_accuracy_no_empty_std = compute_accuracy_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=False, return_std=True)\n",
    "    model_precision_no_empty,model_precision_no_empty_std =compute_precision_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "    model_recall_no_empty,model_recall_no_empty_std  = compute_recall_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "\n",
    "    model_f1_no_empty,model_f1_no_empty_std = compute_f1_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "    \n",
    "    \n",
    "    print(\"MODEL CLASS MEAN IOU \", model_class_mean_iou)\n",
    "    print(\"MODEL CLASS STD IOU \", model_class_std_iou)\n",
    "    print()\n",
    "    print(\"MODEL CLASS MEAN DICE \", model_class_mean_dice)\n",
    "    print(\"MODEL CLASS STD DICE \", model_class_std_dice)\n",
    "    print()\n",
    "    print(\"MODEL DIOU\", model_detection_iou)\n",
    "    print(\"MODEL DIOU STD \", model_detection_iou_std) \n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME \", model_iou_mass_volume)\n",
    "    print(\"MODEL IOU MASS VOLUME STD \", model_iou_mass_volume_std)\n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY \", model_iou_mass_volume_no_empty)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY STD \", model_iou_mass_volume_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC \", model_iou_mass_volume_no_empty_optimistic)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD \", model_iou_mass_volume_no_empty_optimistic_std)\n",
    "    \n",
    "    print(\"MODEL DICE MASS VOLUME \", model_dice_mass_volume)\n",
    "    print(\"MODEL DICE MASS VOLUME STD \", model_dice_mass_volume_std)\n",
    "    print()\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY \", model_dice_mass_volume_no_empty)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY STD \", model_dice_mass_volume_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC \", model_dice_mass_volume_no_empty_optimistic)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD \", model_dice_mass_volume_no_empty_optimistic_std)\n",
    "    print() \n",
    "    print(\"MODEL MEAN ACCURACY NO EMPTY\", model_mean_accuracy_no_empty)\n",
    "    print(\"MODEL MEAN ACCURACY NO EMPTY STD\", model_mean_accuracy_no_empty_std)\n",
    "    print()                                                                              \n",
    "    print(\"MODEL MEAN PRECISION NO EMPTY\", model_mean_precision_no_empty)\n",
    "    print(\"MODEL MEAN PRECISION NO EMPTY STD\", model_mean_precision_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL MEAN RECALL NO EMPTY\", model_mean_recall_no_empty)\n",
    "    print(\"MODEL MEAN RECALL NO EMPTY STD\", model_mean_recall_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL MEAN F1 NO EMPTY\", model_mean_f1_no_empty)\n",
    "    print(\"MODEL MEAN F1 NO EMPTY STD\", model_mean_f1_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL ACCURACY EXCLUDING CASES \",  model_accuracy_excluding_cases)\n",
    "    print(\"MODEL ACCURACY EXCLUDING CASES STD \",  model_accuracy_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL PRECISION EXCLUDING CASES \",  model_precision_excluding_cases)\n",
    "    print(\"MODEL PRECISION EXCLUDING CASES STD \",  model_precision_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL RECALL EXCLUDING CASES \", model_recall_excluding_cases)\n",
    "    print(\"MODEL RECALL EXCLUDING CASES STD \", model_recall_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL ACCURACY NO EMPTY \",  model_accuracy_no_empty)\n",
    "    print(\"MODEL ACCURACY NO EMPTY STD \",  model_accuracy_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL PRECISION NO EMPTY\",  model_precision_no_empty)\n",
    "    print(\"MODEL PRECISION NO EMPTY STD \",  model_precision_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL RECALL NO EMPTY \", model_recall_no_empty)\n",
    "    print(\"MODEL RECALL NO EMPTY STD \", model_recall_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL F1 NO EMPTY \", model_f1_no_empty)\n",
    "    print(\"MODEL F1 NO EMPTY STD \", model_f1_no_empty_std)\n",
    "    print()\n",
    "    \n",
    "\n",
    "\n",
    "    if get_scores_for_statistics:\n",
    "            tp = torch.cat([tp for tp in TP])\n",
    "            fp = torch.cat([fp for fp in FP])\n",
    "            fn = torch.cat([fn for fn in FN])\n",
    "            tn = torch.cat([tn for tn in TN])\n",
    "\n",
    "\n",
    "            if get_only_masses:\n",
    "                # Create a mask where tp + fn is not equal to 0\n",
    "                mask = (tp + fn) != 0\n",
    "                \n",
    "                # Apply this mask to each tensor to filter out the desired values\n",
    "                tp = tp[mask]\n",
    "                fp = fp[mask]\n",
    "                fn = fn[mask]\n",
    "                tn = tn[mask]\n",
    "\n",
    "            miou_scores = compute_mean_iou_imagewise_from_cumulator(tp, fp, fn, tn, exclude_empty=False, return_std=False,reduce_mean=False)\n",
    "            mdice_scores = compute_mean_dice_imagewise_from_cumulator(tp, fp, fn, tn, exclude_empty=False, return_std=False,reduce_mean=False)\n",
    "            mf1_scores = compute_f1_from_cumulator(tp, fp, fn, tn, exclude_empty=False, is_mean=True, return_std=False,reduce_mean=False)\n",
    "\n",
    "\n",
    "            scores_dict = {\n",
    "                 'miou': miou_scores.squeeze().tolist(),\n",
    "                 'mdice': mdice_scores.squeeze().tolist(),\n",
    "                 \"mf1\": mf1_scores.squeeze().tolist(),\n",
    "            }\n",
    "            return scores_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_patient_aware_patches(model_path, patient_ids, datasets, dataset_key):\n",
    "\n",
    "    model = BreastModel2.load_from_checkpoint(model_path, strict=False)\n",
    "\n",
    "    model_class_mean_iou = []\n",
    "    model_class_mean_dice = []\n",
    "    model_detection_iou = []\n",
    "    \n",
    "    model_iou_mass_volume = []\n",
    "    model_iou_mass_volume_no_empty = []\n",
    "    \n",
    "    model_dice_mass_volume = []\n",
    "    model_dice_mass_volume_no_empty = []\n",
    "    \n",
    "    model_precision = []\n",
    "    model_recall = []\n",
    "    model_accuracy = []\n",
    "\n",
    "    \n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "    \n",
    "         TP = []\n",
    "         FP = []\n",
    "         FN =[]\n",
    "         TN = []\n",
    "         print(patient_id)\n",
    "         dataset = datasets[patient_id][dataset_key]\n",
    "        \n",
    "         for idx, e in tqdm(enumerate(dataset), total = len(dataset)):\n",
    "            original_image = np.load(e[0]['image_meta_dict']['filename_or_obj'])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "\n",
    "            pred_label = torch.zeros(original_image.shape, dtype=torch.uint8)\n",
    "            \n",
    "            gt_label = np.load(e[0]['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "\n",
    "\n",
    "            merged_label = torch.zeros(original_image.shape)\n",
    "            merged_label_for_fusion = torch.zeros(original_image.shape)\n",
    "\n",
    "            for elem in e:\n",
    "                if elem['keep_sample']:\n",
    "                    image = torch.unsqueeze(elem['image'], 0)\n",
    "                    with torch.no_grad():\n",
    "                        model = model.to(\"cuda\")\n",
    "                        model.eval()\n",
    "                        logits = model(image.to(\"cuda\"))[0]\n",
    "    \n",
    "                    pr_mask = logits.sigmoid()\n",
    "                    pr_mask = pr_mask[0]\n",
    "                    #pr_mask_to_viz = (pr_mask.cpu().numpy() > 0.4).astype(int)\n",
    "    \n",
    "                    if pr_mask.sum()>0:\n",
    "                        #label = pr_mask\n",
    "                        pr_mask = (pr_mask > 0.4).int()\n",
    "                        label = reverse_transformations(elem, pr_mask, mode='patches')\n",
    "                        merged_label += label\n",
    "                    \n",
    "\n",
    "                pred_label = merged_label\n",
    "                pred_label = monai.transforms.Resize(spatial_size=(original_image.shape[1], original_image.shape[2]), mode='nearest-exact')(pred_label)\n",
    "\n",
    "            tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(np.expand_dims(pred_label,0).astype(int)), torch.tensor(np.expand_dims(gt_label,0).astype(int)), mode = \"binary\")\n",
    "            TP.append(tp)\n",
    "            FP.append(fp)\n",
    "            FN.append(fn)\n",
    "            TN.append(tn)\n",
    "             \n",
    "            \"\"\"plt.figure(figsize=(15, 10))\n",
    "    \n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.imshow(original_image.squeeze(),  cmap='gray')  # convert CHW -> HWC\n",
    "            plt.title(\"Image\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.imshow(label_whole, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Whole\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.imshow(label_patches, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Patch\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.imshow(gt_label, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"GT\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.show()\n",
    "    \n",
    "            plt.imshow(fusion , cmap='gray')\n",
    "        \n",
    "            plt.show()\"\"\"\n",
    "    \n",
    "    \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    "       \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "    \n",
    "         detection_iou =  np.array(calculate_mass_detection_imagewise_volume(predicted_label_volume.astype(int), gt_label_volume)).mean()\n",
    "         \n",
    "         mean_iou = compute_mean_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True)\n",
    "         mean_dice = compute_mean_dice_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True)\n",
    "\n",
    "         iou_mass_volume = compute_iou_npy(gt_label_volume, predicted_label_volume, class_id=1, reduction='micro-imagewise')\n",
    "         iou_mass_volume_no_empty = compute_iou_npy(gt_label_volume, predicted_label_volume, class_id=1, reduction='micro-imagewise', exclude_empty=True)\n",
    "         dice_mass_volume = compute_dice_score_npy(gt_label_volume, predicted_label_volume,reduction='micro-imagewise', class_id=1)\n",
    "         dice_mass_volume_no_empty = compute_dice_score_npy(gt_label_volume, predicted_label_volume, reduction='micro-imagewise',class_id=1, exclude_empty=True)\n",
    "\n",
    "        \n",
    "         accuracy = compute_accuracy_from_cumulator(TP, FP, FN, TN)\n",
    "         precision = compute_precision_from_cumulator(TP, FP, FN, TN)\n",
    "         recall = compute_recall_from_cumulator(TP, FP, FN, TN)\n",
    "        \n",
    "         print(\"CLASS MEAN IOU\", mean_iou)\n",
    "         print(\"CLASS MEAN DICE\", mean_dice)\n",
    "         print(\"DIOU\", detection_iou)\n",
    "         print(\"IOU MASS VOLUME\", iou_mass_volume)\n",
    "         print(\"IOU MASS VOLUME NO EMPTY\", iou_mass_volume_no_empty)\n",
    "         print(\"DICE MASS VOLUME \", dice_mass_volume)\n",
    "         print(\"DICE MASS VOLUME NO EMPTY \", dice_mass_volume_no_empty)\n",
    "\n",
    "         print(\"ACCURACY \", accuracy)\n",
    "         print(\"PRECISION \", precision)\n",
    "         print(\"RECALL\", recall)\n",
    "         \n",
    "         print()\n",
    "         model_class_mean_iou.append(mean_iou)\n",
    "         model_class_mean_dice.append(mean_dice)\n",
    "         model_detection_iou.append(detection_iou)\n",
    "        \n",
    "         model_iou_mass_volume.append(iou_mass_volume)\n",
    "         model_iou_mass_volume_no_empty.append(iou_mass_volume_no_empty)\n",
    "        \n",
    "         model_dice_mass_volume.append(dice_mass_volume)\n",
    "         model_dice_mass_volume_no_empty.append(dice_mass_volume_no_empty)\n",
    "\n",
    "         model_accuracy.append(accuracy)\n",
    "         model_precision.append(precision)\n",
    "         model_recall.append(recall)\n",
    "\n",
    "    model_class_mean_iou = np.array(model_class_mean_iou).mean()\n",
    "    model_class_mean_dice = np.array(model_class_mean_dice).mean()\n",
    "    model_detection_iou = np.array(model_detection_iou).mean()\n",
    "    \n",
    "    model_iou_mass_volume = np.array(model_iou_mass_volume).mean()\n",
    "    model_iou_mass_volume_no_empty = np.array(model_iou_mass_volume_no_empty).mean()\n",
    "    \n",
    "    model_dice_mass_volume = np.array(model_dice_mass_volume).mean()\n",
    "    model_dice_mass_volume_no_empty = np.array(model_dice_mass_volume_no_empty).mean()\n",
    "\n",
    "    model_accuracy = np.array(model_accuracy).mean()\n",
    "    model_precision = np.array(model_precision).mean()\n",
    "    model_recall = np.array(model_recall).mean()\n",
    "\n",
    "    print(\"MODEL CLASS MEAN IOU\", model_class_mean_iou)\n",
    "    print(\"MODEL CLASS MEAN DICE\", model_class_mean_dice)\n",
    "    print(\"MODEL DIOU\", model_detection_iou)\n",
    "        \n",
    "    print(\"MODEL IOU MASS VOLUME\", model_iou_mass_volume)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY\", model_iou_mass_volume_no_empty)\n",
    "    \n",
    "    print(\"MODEL DICE MASS VOLUME \", model_dice_mass_volume)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY \", model_dice_mass_volume_no_empty)\n",
    "\n",
    "    print(\"MODEL ACCURACY \", model_accuracy)\n",
    "    print(\"MODEL PRECISION \", model_precision)\n",
    "    print(\"MODEL RECALL\", model_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sum every two consecutive elements in a tensor\n",
    "def sum_every_two(tensor):\n",
    "    # Ensure the tensor has an even number of elements\n",
    "    if tensor.numel() % 2 != 0:\n",
    "        raise ValueError(\"The number of elements in the tensor must be even\")\n",
    "    # Reshape to have pairs of elements in the last dimension, then sum along that dimension\n",
    "    return tensor.view(-1, 2).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_aware_patches(model_path, patient_ids, datasets, dataset_key, get_scores_for_statistics=False,get_only_masses=False, filter = False,  strict=False, arch_name=False):\n",
    "\n",
    "    \n",
    "    if arch_name:\n",
    "        model = BreastModel2.load_from_checkpoint(model_path, strict=strict, arch=arch_name)\n",
    "\n",
    "    else:\n",
    "\n",
    "        model = BreastModel2.load_from_checkpoint(model_path, strict=strict)\n",
    "    \n",
    "    model_class_mean_iou = []\n",
    "    model_class_mean_dice = []\n",
    "    model_detection_iou = []\n",
    "    \n",
    "    model_iou_mass_volume = []\n",
    "    model_iou_mass_volume_no_empty = []\n",
    "    \n",
    "    model_dice_mass_volume = []\n",
    "    model_dice_mass_volume_no_empty = []\n",
    "\n",
    "    model_accuracy = []\n",
    "    model_precision = []\n",
    "    model_recall = []\n",
    "    \n",
    "    TP = []\n",
    "    FP = []\n",
    "    FN = []\n",
    "    TN = []\n",
    "\n",
    "    \n",
    "    detection_iou=[]\n",
    "\n",
    "    for patient_id in patient_ids:\n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "         print(patient_id)\n",
    "         dataset = datasets[patient_id][dataset_key]\n",
    "        \n",
    "         for idx, e in tqdm(enumerate(dataset), total = len(dataset)):\n",
    "            original_image = np.load(e[0]['image_meta_dict']['filename_or_obj'])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "\n",
    "            pred_label = torch.zeros(original_image.shape, dtype=torch.uint8)\n",
    "            \n",
    "            gt_label = np.load(e[0]['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "\n",
    "\n",
    "            merged_label = torch.zeros(original_image.shape)\n",
    "            merged_label_for_fusion = torch.zeros(original_image.shape)\n",
    "\n",
    "            for elem in e:\n",
    "                if elem['keep_sample']:\n",
    "                    image = torch.unsqueeze(elem['image'], 0)\n",
    "                    with torch.no_grad():\n",
    "                        model = model.to(\"cuda\")\n",
    "                        model.eval()\n",
    "                        if arch_name:\n",
    "                            logits = model(image.to(\"cuda\"))[0]\n",
    "                        else:\n",
    "                            logits = model(image.to(\"cuda\"))[0]\n",
    "    \n",
    "                    pr_mask = logits.sigmoid()\n",
    "                    pr_mask = pr_mask[0]\n",
    "                    #pr_mask_to_viz = (pr_mask.cpu().numpy() > 0.4).astype(int)\n",
    "    \n",
    "                    if pr_mask.sum()>0:\n",
    "                        #label = pr_mask\n",
    "                        pr_mask = (pr_mask > 0.4).int()\n",
    "                        label = reverse_transformations(elem, pr_mask, mode='patches')\n",
    "                        merged_label += label\n",
    "                    \n",
    "\n",
    "                pred_label = merged_label\n",
    "                pred_label = monai.transforms.Resize(spatial_size=(original_image.shape[1], original_image.shape[2]), mode='nearest-exact')(pred_label)\n",
    "\n",
    "            tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(np.expand_dims(pred_label,0).astype(int)), torch.tensor(np.expand_dims(gt_label,0).astype(int)), mode = \"binary\")\n",
    "            TP.append(tp)\n",
    "            FP.append(fp)\n",
    "            FN.append(fn)\n",
    "            TN.append(tn)\n",
    "             \n",
    "            \"\"\"plt.figure(figsize=(15, 10))\n",
    "    \n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.imshow(original_image.squeeze(),  cmap='gray')  # convert CHW -> HWC\n",
    "            plt.title(\"Image\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.imshow(label_whole, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Whole\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.imshow(label_patches, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Patch\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.imshow(gt_label, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"GT\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.show()\n",
    "    \n",
    "            plt.imshow(fusion , cmap='gray')\n",
    "        \n",
    "            plt.show()\"\"\"\n",
    "    \n",
    "    \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    "                    \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "    \n",
    "         if filter:\n",
    "             predicted_label_volume = filter_masses(predicted_label_volume, min_slices=3, window_size=3) # H x W x N\n",
    "             # H x W x N -> N x H x W -> N x 1 x H x W\n",
    "             predicted_label_volume_for_stats = np.transpose(predicted_label_volume, (2, 0, 1))\n",
    "             predicted_label_volume_for_stats = np.expand_dims(predicted_label_volume_for_stats, 1)  # N x 1 x H x W\n",
    "\n",
    "             gt_label_volume_for_stats = np.transpose(gt_label_volume, (2, 0, 1))\n",
    "             gt_label_volume_for_stats = np.expand_dims(gt_label_volume_for_stats, 1)  # N x 1 x H x W             \n",
    "            \n",
    "             \n",
    "             tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(predicted_label_volume_for_stats.astype(int)), torch.tensor(gt_label_volume_for_stats.astype(int)), mode = \"binary\")\n",
    "             TP +=  [torch.tensor([[elem]]) for elem in tp.squeeze()]\n",
    "             FP +=  [torch.tensor([[elem]]) for elem in fp.squeeze()]\n",
    "             FN +=  [torch.tensor([[elem]]) for elem in fn.squeeze()]\n",
    "             TN +=  [torch.tensor([[elem]]) for elem in tn.squeeze()]\n",
    "\n",
    "    \n",
    "         detection_iou+=calculate_mass_detection_imagewise_volume(predicted_label_volume.astype(int), gt_label_volume)\n",
    "    \n",
    "    model_detection_iou = np.array(detection_iou).mean()\n",
    "    model_detection_iou_std = np.array(detection_iou).std()\n",
    "    \n",
    "    model_class_mean_iou, model_class_std_iou = compute_mean_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "    model_class_mean_dice, model_class_std_dice = compute_mean_dice_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "\n",
    "    model_iou_mass_volume , model_iou_mass_volume_std = compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=False, return_std=True)\n",
    "    model_iou_mass_volume_no_empty, model_iou_mass_volume_no_empty_std =compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "    model_iou_mass_volume_no_empty_optimistic, model_iou_mass_volume_no_empty_optimistic_std =compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, exclude_empty_only_gt=True, return_std=True)\n",
    "    \n",
    "    \n",
    "    model_dice_mass_volume, model_dice_mass_volume_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=False, return_std=True)\n",
    "    model_dice_mass_volume_no_empty, model_dice_mass_volume_no_empty_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=True, return_std=True)\n",
    "    model_dice_mass_volume_no_empty_optimistic, model_dice_mass_volume_no_empty_optimistic_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=True, exclude_empty_only_gt=True,return_std=True)\n",
    "    \n",
    "    model_mean_accuracy_no_empty, model_mean_accuracy_no_empty_std = compute_accuracy_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_precision_no_empty,model_mean_precision_no_empty_std = compute_precision_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_recall_no_empty, model_mean_recall_no_empty_std = compute_recall_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_f1_no_empty, model_mean_f1_no_empty_std = compute_f1_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "        \n",
    "    model_accuracy_excluding_cases, model_accuracy_excluding_cases_std = compute_accuracy_excluding_cases(TP, FP, FN, TN, return_std=True)\n",
    "    model_precision_excluding_cases,model_precision_excluding_cases_std =compute_precision_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True)\n",
    "    model_recall_excluding_cases,model_recall_excluding_cases_std  =compute_recall_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True)\n",
    "\n",
    "    model_accuracy_no_empty, model_accuracy_no_empty_std = compute_accuracy_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=False, return_std=True)\n",
    "    model_precision_no_empty,model_precision_no_empty_std =compute_precision_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "    model_recall_no_empty,model_recall_no_empty_std  = compute_recall_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "\n",
    "    model_f1_no_empty,model_f1_no_empty_std = compute_f1_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "    \n",
    "    \n",
    "    print(\"MODEL CLASS MEAN IOU \", model_class_mean_iou)\n",
    "    print(\"MODEL CLASS STD IOU \", model_class_std_iou)\n",
    "    print()\n",
    "    print(\"MODEL CLASS MEAN DICE \", model_class_mean_dice)\n",
    "    print(\"MODEL CLASS STD DICE \", model_class_std_dice)\n",
    "    print()\n",
    "    print(\"MODEL DIOU\", model_detection_iou)\n",
    "    print(\"MODEL DIOU STD \", model_detection_iou_std) \n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME \", model_iou_mass_volume)\n",
    "    print(\"MODEL IOU MASS VOLUME STD \", model_iou_mass_volume_std)\n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY \", model_iou_mass_volume_no_empty)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY STD \", model_iou_mass_volume_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC \", model_iou_mass_volume_no_empty_optimistic)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD \", model_iou_mass_volume_no_empty_optimistic_std)\n",
    "    \n",
    "    print(\"MODEL DICE MASS VOLUME \", model_dice_mass_volume)\n",
    "    print(\"MODEL DICE MASS VOLUME STD \", model_dice_mass_volume_std)\n",
    "    print()\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY \", model_dice_mass_volume_no_empty)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY STD \", model_dice_mass_volume_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC \", model_dice_mass_volume_no_empty_optimistic)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD \", model_dice_mass_volume_no_empty_optimistic_std)\n",
    "    print() \n",
    "    print(\"MODEL MEAN ACCURACY NO EMPTY\", model_mean_accuracy_no_empty)\n",
    "    print(\"MODEL MEAN ACCURACY NO EMPTY STD\", model_mean_accuracy_no_empty_std)\n",
    "    print()                                                                              \n",
    "    print(\"MODEL MEAN PRECISION NO EMPTY\", model_mean_precision_no_empty)\n",
    "    print(\"MODEL MEAN PRECISION NO EMPTY STD\", model_mean_precision_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL MEAN RECALL NO EMPTY\", model_mean_recall_no_empty)\n",
    "    print(\"MODEL MEAN RECALL NO EMPTY STD\", model_mean_recall_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL MEAN F1 NO EMPTY\", model_mean_f1_no_empty)\n",
    "    print(\"MODEL MEAN F1 NO EMPTY STD\", model_mean_f1_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL ACCURACY EXCLUDING CASES \",  model_accuracy_excluding_cases)\n",
    "    print(\"MODEL ACCURACY EXCLUDING CASES STD \",  model_accuracy_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL PRECISION EXCLUDING CASES \",  model_precision_excluding_cases)\n",
    "    print(\"MODEL PRECISION EXCLUDING CASES STD \",  model_precision_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL RECALL EXCLUDING CASES \", model_recall_excluding_cases)\n",
    "    print(\"MODEL RECALL EXCLUDING CASES STD \", model_recall_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL ACCURACY NO EMPTY \",  model_accuracy_no_empty)\n",
    "    print(\"MODEL ACCURACY NO EMPTY STD \",  model_accuracy_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL PRECISION NO EMPTY\",  model_precision_no_empty)\n",
    "    print(\"MODEL PRECISION NO EMPTY STD \",  model_precision_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL RECALL NO EMPTY \", model_recall_no_empty)\n",
    "    print(\"MODEL RECALL NO EMPTY STD \", model_recall_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL F1 NO EMPTY \", model_f1_no_empty)\n",
    "    print(\"MODEL F1 NO EMPTY STD \", model_f1_no_empty_std)\n",
    "    print()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if get_scores_for_statistics:\n",
    "            tp = torch.cat([tp for tp in TP])\n",
    "            fp = torch.cat([fp for fp in FP])\n",
    "            fn = torch.cat([fn for fn in FN])\n",
    "            tn = torch.cat([tn for tn in TN])\n",
    "\n",
    "            tp =  sum_every_two(tp.squeeze())\n",
    "            fp =  sum_every_two(fp.squeeze())\n",
    "            fn =  sum_every_two(fn.squeeze())\n",
    "            tn = sum_every_two(tn.squeeze())\n",
    "\n",
    "            if get_only_masses:\n",
    "                # Create a mask where tp + fn is not equal to 0\n",
    "                mask = (tp + fn) != 0\n",
    "                \n",
    "                # Apply this mask to each tensor to filter out the desired values\n",
    "                tp = tp[mask]\n",
    "                fp = fp[mask]\n",
    "                fn = fn[mask]\n",
    "                tn = tn[mask]\n",
    "\n",
    "            miou_scores = compute_mean_iou_imagewise_from_cumulator(tp, fp, fn, tn, exclude_empty=False, return_std=False,reduce_mean=False)\n",
    "            mdice_scores = compute_mean_dice_imagewise_from_cumulator(tp, fp, fn, tn, exclude_empty=False, return_std=False,reduce_mean=False)\n",
    "            mf1_scores = compute_f1_from_cumulator(tp, fp, fn, tn, exclude_empty=False, is_mean=True, return_std=False,reduce_mean=False)\n",
    "\n",
    "\n",
    "            scores_dict = {\n",
    "                 'miou': miou_scores.squeeze().tolist(),\n",
    "                 'mdice': mdice_scores.squeeze().tolist(),\n",
    "                 \"mf1\": mf1_scores.squeeze().tolist(),\n",
    "            }\n",
    "            return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_patient_aware_fusion(model_path, patient_ids, datasets, whole_dataset_key, patches_dataset_key, use_simple_fusion=False, use_decoder_attention=True, strict=False, filter=False, get_scores_for_statistics=False,get_only_masses=False):\n",
    "\n",
    "    print(use_decoder_attention)\n",
    "    print(use_simple_fusion)\n",
    "    model = BreastModel.load_from_checkpoint(model_path, strict=strict, use_simple_fusion=use_simple_fusion, use_decoder_attention=use_decoder_attention)\n",
    "    print(model)\n",
    "    \n",
    "    model_class_mean_iou = []\n",
    "    model_class_mean_dice = []\n",
    "    model_detection_iou = []\n",
    "    \n",
    "    model_iou_mass_volume = []\n",
    "    model_iou_mass_volume_no_empty = []\n",
    "    \n",
    "    model_dice_mass_volume = []\n",
    "    model_dice_mass_volume_no_empty = []\n",
    "    \n",
    "    model_precision = []\n",
    "    model_recall = []\n",
    "    model_accuracy = []\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "\n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "    \n",
    "         TP = []\n",
    "         FP = []\n",
    "         FN =[]\n",
    "         TN = []\n",
    "         print(patient_id)\n",
    "         patches_ds = datasets[patient_id][patches_dataset_key]\n",
    "         whole_image_ds = datasets[patient_id][whole_dataset_key]\n",
    "\n",
    "         fusion_dataset = PairedDataset(whole_image_ds, patches_ds, augment=False)\n",
    "        \n",
    "         prev_had_mask=False\n",
    "\n",
    "         for idx, e in tqdm(enumerate(fusion_dataset), total = len(patches_ds)):\n",
    "            original_image = np.load(e[0]['image_meta_dict']['filename_or_obj'])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "    \n",
    "            \n",
    "            pred_label = torch.zeros(original_image.shape, dtype=torch.uint8)\n",
    "    \n",
    "            gt_label = np.load(e[0]['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "\n",
    "            if fusion_dataset[idx][0]['keep_sample']:\n",
    "    \n",
    "                whole_image = torch.unsqueeze(fusion_dataset[idx][0]['image'], 0)\n",
    "                patch_image2 = torch.unsqueeze(fusion_dataset[idx][1]['image'], 0)\n",
    "                patch_image3 = torch.unsqueeze(fusion_dataset[idx][2]['image'], 0)\n",
    "                    \n",
    "        \n",
    "                with torch.no_grad():\n",
    "                    masks = []\n",
    "                    # pass to model\n",
    "                    model = model.to(\"cuda\")\n",
    "                    model.eval()\n",
    "                    \n",
    "                    masks = model(whole_image.to(\"cuda\"),patch_image2.to(\"cuda\"),patch_image3.to(\"cuda\"))\n",
    "                    masks = masks.sigmoid()\n",
    "                    \n",
    "    \n",
    "                pred_label = masks[0]\n",
    "                pred_label = (pred_label > 0.4).int()\n",
    "                pred_label = reverse_transformations(fusion_dataset[idx][0], pred_label, mode='whole')\n",
    "                \n",
    "                \n",
    "            pred_label = monai.transforms.Resize(spatial_size=(original_image.shape[1], original_image.shape[2]), mode='nearest-exact')(pred_label)\n",
    "             \n",
    "            if not filter:\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(np.expand_dims(pred_label,0).astype(int)), torch.tensor(np.expand_dims(gt_label,0).astype(int)), mode = \"binary\")\n",
    "                TP.append(tp)\n",
    "                FP.append(fp)\n",
    "                FN.append(fn)\n",
    "                TN.append(tn)\n",
    "             \n",
    "            \"\"\"plt.figure(figsize=(15, 10))\n",
    "    \n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.imshow(original_image.squeeze(),  cmap='gray')  # convert CHW -> HWC\n",
    "            plt.title(\"Image\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.imshow(label_whole, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Whole\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.imshow(label_patches, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Patch\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.imshow(gt_label, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"GT\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.show()\n",
    "    \n",
    "            plt.imshow(fusion , cmap='gray')\n",
    "            plt.show()\"\"\"\n",
    "    \n",
    "    \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    "                    \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "    \n",
    "         if filter:\n",
    "             predicted_label_volume = filter_masses(predicted_label_volume, min_slices=3, window_size=3) # H x W x N\n",
    "             # H x W x N -> N x H x W -> N x 1 x H x W\n",
    "             predicted_label_volume_for_stats = np.transpose(predicted_label_volume, (2, 0, 1))\n",
    "             predicted_label_volume_for_stats = np.expand_dims(predicted_label_volume_for_stats, 1)  # N x 1 x H x W\n",
    "\n",
    "             gt_label_volume_for_stats = np.transpose(gt_label_volume, (2, 0, 1))\n",
    "             gt_label_volume_for_stats = np.expand_dims(gt_label_volume_for_stats, 1)  # N x 1 x H x W             \n",
    "\n",
    "             \n",
    "             tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(predicted_label_volume_for_stats.astype(int)), torch.tensor(gt_label_volume_for_stats.astype(int)), mode = \"binary\")\n",
    "             TP =  [torch.tensor([[elem]]) for elem in tp.squeeze()]\n",
    "             FP =  [torch.tensor([[elem]]) for elem in fp.squeeze()]\n",
    "             FN =  [torch.tensor([[elem]]) for elem in fn.squeeze()]\n",
    "             TN =  [torch.tensor([[elem]]) for elem in tn.squeeze()]\n",
    "\n",
    "         detection_iou =  np.array(calculate_mass_detection_imagewise_volume(predicted_label_volume.astype(int), gt_label_volume)).mean()\n",
    "         \n",
    "         mean_iou = compute_mean_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True)\n",
    "         mean_dice = compute_mean_dice_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True)\n",
    "\n",
    "         iou_mass_volume = compute_iou_npy(gt_label_volume, predicted_label_volume, class_id=1, reduction='micro-imagewise')\n",
    "         iou_mass_volume_no_empty = compute_iou_npy(gt_label_volume, predicted_label_volume, class_id=1, reduction='micro-imagewise', exclude_empty=True)\n",
    "         dice_mass_volume = compute_dice_score_npy(gt_label_volume, predicted_label_volume,reduction='micro-imagewise', class_id=1)\n",
    "         dice_mass_volume_no_empty = compute_dice_score_npy(gt_label_volume, predicted_label_volume, reduction='micro-imagewise',class_id=1, exclude_empty=True)\n",
    "\n",
    "        \n",
    "         accuracy = compute_accuracy_from_cumulator(TP, FP, FN, TN)\n",
    "         precision = compute_precision_from_cumulator(TP, FP, FN, TN)\n",
    "         recall = compute_recall_from_cumulator(TP, FP, FN, TN)\n",
    "        \n",
    "         print(\"CLASS MEAN IOU\", mean_iou)\n",
    "         print(\"CLASS MEAN DICE\", mean_dice)\n",
    "         print(\"DIOU\", detection_iou)\n",
    "         print(\"IOU MASS VOLUME\", iou_mass_volume)\n",
    "         print(\"IOU MASS VOLUME NO EMPTY\", iou_mass_volume_no_empty)\n",
    "         print(\"DICE MASS VOLUME \", dice_mass_volume)\n",
    "         print(\"DICE MASS VOLUME NO EMPTY \", dice_mass_volume_no_empty)\n",
    "\n",
    "         print(\"ACCURACY \", accuracy)\n",
    "         print(\"PRECISION \", precision)\n",
    "         print(\"RECALL\", recall)\n",
    "         \n",
    "         print()\n",
    "         model_class_mean_iou.append(mean_iou)\n",
    "         model_class_mean_dice.append(mean_dice)\n",
    "         model_detection_iou.append(detection_iou)\n",
    "        \n",
    "         model_iou_mass_volume.append(iou_mass_volume)\n",
    "         model_iou_mass_volume_no_empty.append(iou_mass_volume_no_empty)\n",
    "        \n",
    "         model_dice_mass_volume.append(dice_mass_volume)\n",
    "         model_dice_mass_volume_no_empty.append(dice_mass_volume_no_empty)\n",
    "\n",
    "         model_accuracy.append(accuracy)\n",
    "         model_precision.append(precision)\n",
    "         model_recall.append(recall)\n",
    "\n",
    "    model_class_mean_iou = np.array(model_class_mean_iou).mean()\n",
    "    model_class_mean_dice = np.array(model_class_mean_dice).mean()\n",
    "    model_detection_iou = np.array(model_detection_iou).mean()\n",
    "    \n",
    "    model_iou_mass_volume = np.array(model_iou_mass_volume).mean()\n",
    "    model_iou_mass_volume_no_empty = np.array(model_iou_mass_volume_no_empty).mean()\n",
    "    \n",
    "    model_dice_mass_volume = np.array(model_dice_mass_volume).mean()\n",
    "    model_dice_mass_volume_no_empty = np.array(model_dice_mass_volume_no_empty).mean()\n",
    "\n",
    "    model_accuracy = np.array(model_accuracy).mean()\n",
    "    model_precision = np.array(model_precision).mean()\n",
    "    model_recall = np.array(model_recall).mean()\n",
    "\n",
    "    print(\"MODEL CLASS MEAN IOU\", model_class_mean_iou)\n",
    "    print(\"MODEL CLASS MEAN DICE\", model_class_mean_dice)\n",
    "    print(\"MODEL DIOU\", model_detection_iou)\n",
    "        \n",
    "    print(\"MODEL IOU MASS VOLUME\", model_iou_mass_volume)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY\", model_iou_mass_volume_no_empty)\n",
    "    \n",
    "    print(\"MODEL DICE MASS VOLUME \", model_dice_mass_volume)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY \", model_dice_mass_volume_no_empty)\n",
    "\n",
    "    print(\"MODEL ACCURACY \", model_accuracy)\n",
    "    print(\"MODEL PRECISION \", model_precision)\n",
    "    print(\"MODEL RECALL\", model_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_aware_fusion(model_path, patient_ids, datasets, whole_dataset_key, patches_dataset_key, use_simple_fusion=False, use_decoder_attention=True, strict=False, filter=False, get_scores_for_statistics=False,get_only_masses=False):\n",
    "\n",
    "    model = BreastModel.load_from_checkpoint(model_path, strict=strict, use_simple_fusion=use_simple_fusion, use_decoder_attention=use_decoder_attention)\n",
    "    model_class_mean_iou = []\n",
    "    model_class_mean_dice = []\n",
    "    model_detection_iou = []\n",
    "    \n",
    "    model_iou_mass_volume = []\n",
    "    model_iou_mass_volume_no_empty = []\n",
    "    \n",
    "    model_dice_mass_volume = []\n",
    "    model_dice_mass_volume_no_empty = []\n",
    "\n",
    "    model_accuracy = []\n",
    "    model_precision = []\n",
    "    model_recall = []\n",
    "    \n",
    "    TP = []\n",
    "    FP = []\n",
    "    FN = []\n",
    "    TN = []\n",
    "\n",
    "\n",
    "    detection_iou=[]\n",
    "\n",
    "\n",
    "    for patient_id in patient_ids:\n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "         print(patient_id)\n",
    "         patches_ds = datasets[patient_id][patches_dataset_key]\n",
    "         whole_image_ds = datasets[patient_id][whole_dataset_key]\n",
    "\n",
    "         fusion_dataset = PairedDataset(whole_image_ds, patches_ds, augment=False)\n",
    "        \n",
    "         prev_had_mask=False\n",
    "\n",
    "         for idx, e in tqdm(enumerate(fusion_dataset), total = len(patches_ds)):\n",
    "            original_image = np.load(e[0]['image_meta_dict']['filename_or_obj'])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "    \n",
    "            \n",
    "            pred_label = torch.zeros(original_image.shape, dtype=torch.uint8)\n",
    "    \n",
    "            gt_label = np.load(e[0]['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "\n",
    "            if fusion_dataset[idx][0]['keep_sample']:\n",
    "    \n",
    "                whole_image = torch.unsqueeze(fusion_dataset[idx][0]['image'], 0)\n",
    "                patch_image2 = torch.unsqueeze(fusion_dataset[idx][1]['image'], 0)\n",
    "                patch_image3 = torch.unsqueeze(fusion_dataset[idx][2]['image'], 0)\n",
    "                    \n",
    "        \n",
    "                with torch.no_grad():\n",
    "                    masks = []\n",
    "                    # pass to model\n",
    "                    model = model.to(\"cuda\")\n",
    "                    model.eval()\n",
    "                    \n",
    "                    masks = model(whole_image.to(\"cuda\"),patch_image2.to(\"cuda\"),patch_image3.to(\"cuda\"))\n",
    "                    masks = masks.sigmoid()\n",
    "                    \n",
    "    \n",
    "                pred_label = masks[0]\n",
    "                pred_label = (pred_label > 0.4).int()\n",
    "                pred_label = reverse_transformations(fusion_dataset[idx][0], pred_label, mode='whole')\n",
    "                \n",
    "\n",
    "                \n",
    "            pred_label = monai.transforms.Resize(spatial_size=(original_image.shape[1], original_image.shape[2]), mode='nearest-exact')(pred_label)\n",
    "             \n",
    "            if not filter:\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(np.expand_dims(pred_label,0).astype(int)), torch.tensor(np.expand_dims(gt_label,0).astype(int)), mode = \"binary\")\n",
    "                TP.append(tp)\n",
    "                FP.append(fp)\n",
    "                FN.append(fn)\n",
    "                TN.append(tn)\n",
    "             \n",
    "            \"\"\"plt.figure(figsize=(15, 10))\n",
    "    \n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.imshow(original_image.squeeze(),  cmap='gray')  # convert CHW -> HWC\n",
    "            plt.title(\"Image\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.imshow(label_whole, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Whole\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.imshow(label_patches, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Patch\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.imshow(gt_label, cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"GT\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.show()\n",
    "    \n",
    "            plt.imshow(fusion , cmap='gray')\n",
    "            plt.show()\"\"\"\n",
    "    \n",
    "    \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    "                    \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "    \n",
    "         if filter:\n",
    "             predicted_label_volume = filter_masses(predicted_label_volume, min_slices=3, window_size=3) # H x W x N\n",
    "             # H x W x N -> N x H x W -> N x 1 x H x W\n",
    "             predicted_label_volume_for_stats = np.transpose(predicted_label_volume, (2, 0, 1))\n",
    "             predicted_label_volume_for_stats = np.expand_dims(predicted_label_volume_for_stats, 1)  # N x 1 x H x W\n",
    "\n",
    "             gt_label_volume_for_stats = np.transpose(gt_label_volume, (2, 0, 1))\n",
    "             gt_label_volume_for_stats = np.expand_dims(gt_label_volume_for_stats, 1)  # N x 1 x H x W             \n",
    "            \n",
    "             \n",
    "             tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(predicted_label_volume_for_stats.astype(int)), torch.tensor(gt_label_volume_for_stats.astype(int)), mode = \"binary\")\n",
    "             TP +=  [torch.tensor([[elem]]) for elem in tp.squeeze()]\n",
    "             FP +=  [torch.tensor([[elem]]) for elem in fp.squeeze()]\n",
    "             FN +=  [torch.tensor([[elem]]) for elem in fn.squeeze()]\n",
    "             TN +=  [torch.tensor([[elem]]) for elem in tn.squeeze()]\n",
    "\n",
    "\n",
    "\n",
    "         detection_iou+=calculate_mass_detection_imagewise_volume(predicted_label_volume.astype(int), gt_label_volume)\n",
    "    \n",
    "    model_detection_iou = np.array(detection_iou).mean()\n",
    "    model_detection_iou_std = np.array(detection_iou).std()\n",
    "    \n",
    "    model_class_mean_iou, model_class_std_iou = compute_mean_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "    model_class_mean_dice, model_class_std_dice = compute_mean_dice_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "\n",
    "    model_iou_mass_volume , model_iou_mass_volume_std = compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=False, return_std=True)\n",
    "    model_iou_mass_volume_no_empty, model_iou_mass_volume_no_empty_std =compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "    model_iou_mass_volume_no_empty_optimistic, model_iou_mass_volume_no_empty_optimistic_std =compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, exclude_empty_only_gt=True, return_std=True)\n",
    "    \n",
    "    \n",
    "    model_dice_mass_volume, model_dice_mass_volume_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=False, return_std=True)\n",
    "    model_dice_mass_volume_no_empty, model_dice_mass_volume_no_empty_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=True, return_std=True)\n",
    "    model_dice_mass_volume_no_empty_optimistic, model_dice_mass_volume_no_empty_optimistic_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=True, exclude_empty_only_gt=True,return_std=True)\n",
    "    \n",
    "    model_mean_accuracy_no_empty, model_mean_accuracy_no_empty_std = compute_accuracy_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_precision_no_empty,model_mean_precision_no_empty_std = compute_precision_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_recall_no_empty, model_mean_recall_no_empty_std = compute_recall_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_f1_no_empty, model_mean_f1_no_empty_std = compute_f1_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "        \n",
    "    model_accuracy_excluding_cases, model_accuracy_excluding_cases_std = compute_accuracy_excluding_cases(TP, FP, FN, TN, return_std=True)\n",
    "    model_precision_excluding_cases,model_precision_excluding_cases_std =compute_precision_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True)\n",
    "    model_recall_excluding_cases,model_recall_excluding_cases_std  =compute_recall_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True)\n",
    "\n",
    "    model_accuracy_no_empty, model_accuracy_no_empty_std = compute_accuracy_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=False, return_std=True)\n",
    "    model_precision_no_empty,model_precision_no_empty_std =compute_precision_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "    model_recall_no_empty,model_recall_no_empty_std  = compute_recall_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "\n",
    "    model_f1_no_empty,model_f1_no_empty_std = compute_f1_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "    \n",
    "    \n",
    "    print(\"MODEL CLASS MEAN IOU \", model_class_mean_iou)\n",
    "    print(\"MODEL CLASS STD IOU \", model_class_std_iou)\n",
    "    print()\n",
    "    print(\"MODEL CLASS MEAN DICE \", model_class_mean_dice)\n",
    "    print(\"MODEL CLASS STD DICE \", model_class_std_dice)\n",
    "    print()\n",
    "    print(\"MODEL DIOU\", model_detection_iou)\n",
    "    print(\"MODEL DIOU STD \", model_detection_iou_std) \n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME \", model_iou_mass_volume)\n",
    "    print(\"MODEL IOU MASS VOLUME STD \", model_iou_mass_volume_std)\n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY \", model_iou_mass_volume_no_empty)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY STD \", model_iou_mass_volume_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC \", model_iou_mass_volume_no_empty_optimistic)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD \", model_iou_mass_volume_no_empty_optimistic_std)\n",
    "    \n",
    "    print(\"MODEL DICE MASS VOLUME \", model_dice_mass_volume)\n",
    "    print(\"MODEL DICE MASS VOLUME STD \", model_dice_mass_volume_std)\n",
    "    print()\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY \", model_dice_mass_volume_no_empty)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY STD \", model_dice_mass_volume_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC \", model_dice_mass_volume_no_empty_optimistic)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD \", model_dice_mass_volume_no_empty_optimistic_std)\n",
    "    print() \n",
    "    print(\"MODEL MEAN ACCURACY NO EMPTY\", model_mean_accuracy_no_empty)\n",
    "    print(\"MODEL MEAN ACCURACY NO EMPTY STD\", model_mean_accuracy_no_empty_std)\n",
    "    print()                                                                              \n",
    "    print(\"MODEL MEAN PRECISION NO EMPTY\", model_mean_precision_no_empty)\n",
    "    print(\"MODEL MEAN PRECISION NO EMPTY STD\", model_mean_precision_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL MEAN RECALL NO EMPTY\", model_mean_recall_no_empty)\n",
    "    print(\"MODEL MEAN RECALL NO EMPTY STD\", model_mean_recall_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL MEAN F1 NO EMPTY\", model_mean_f1_no_empty)\n",
    "    print(\"MODEL MEAN F1 NO EMPTY STD\", model_mean_f1_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL ACCURACY EXCLUDING CASES \",  model_accuracy_excluding_cases)\n",
    "    print(\"MODEL ACCURACY EXCLUDING CASES STD \",  model_accuracy_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL PRECISION EXCLUDING CASES \",  model_precision_excluding_cases)\n",
    "    print(\"MODEL PRECISION EXCLUDING CASES STD \",  model_precision_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL RECALL EXCLUDING CASES \", model_recall_excluding_cases)\n",
    "    print(\"MODEL RECALL EXCLUDING CASES STD \", model_recall_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL ACCURACY NO EMPTY \",  model_accuracy_no_empty)\n",
    "    print(\"MODEL ACCURACY NO EMPTY STD \",  model_accuracy_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL PRECISION NO EMPTY\",  model_precision_no_empty)\n",
    "    print(\"MODEL PRECISION NO EMPTY STD \",  model_precision_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL RECALL NO EMPTY \", model_recall_no_empty)\n",
    "    print(\"MODEL RECALL NO EMPTY STD \", model_recall_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL F1 NO EMPTY \", model_f1_no_empty)\n",
    "    print(\"MODEL F1 NO EMPTY STD \", model_f1_no_empty_std)\n",
    "    print()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if get_scores_for_statistics:\n",
    "            tp = torch.cat([tp for tp in TP])\n",
    "            fp = torch.cat([fp for fp in FP])\n",
    "            fn = torch.cat([fn for fn in FN])\n",
    "            tn = torch.cat([tn for tn in TN])\n",
    "\n",
    "\n",
    "            if get_only_masses:\n",
    "                # Create a mask where tp + fn is not equal to 0\n",
    "                mask = (tp + fn) != 0\n",
    "                \n",
    "                # Apply this mask to each tensor to filter out the desired values\n",
    "                tp = tp[mask]\n",
    "                fp = fp[mask]\n",
    "                fn = fn[mask]\n",
    "                tn = tn[mask]\n",
    "\n",
    "            miou_scores = compute_mean_iou_imagewise_from_cumulator(tp, fp, fn, tn, exclude_empty=False, return_std=False,reduce_mean=False)\n",
    "            mdice_scores = compute_mean_dice_imagewise_from_cumulator(tp, fp, fn, tn, exclude_empty=False, return_std=False,reduce_mean=False)\n",
    "            mf1_scores = compute_f1_from_cumulator(tp, fp, fn, tn, exclude_empty=False, is_mean=True, return_std=False,reduce_mean=False)\n",
    "\n",
    "\n",
    "            scores_dict = {\n",
    "                 'miou': miou_scores.squeeze().tolist(),\n",
    "                 'mdice': mdice_scores.squeeze().tolist(),\n",
    "                 \"mf1\": mf1_scores.squeeze().tolist(),\n",
    "            }\n",
    "            return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_patient_aware_ensemble(model_whole_path, model_patches_path, patient_ids, datasets, whole_dataset_key, patches_dataset_key, filter=False, get_scores_for_statistics=False,get_only_masses=False):\n",
    "\n",
    "    model_whole = BreastModel.load_from_checkpoint(model_whole_path, strict=False)\n",
    "    model_patches = BreastModel2.load_from_checkpoint(model_patches_path, strict=False)\n",
    "    model_class_mean_iou = []\n",
    "    model_class_mean_dice = []\n",
    "    model_detection_iou = []\n",
    "    \n",
    "    model_iou_mass_volume = []\n",
    "    model_iou_mass_volume_no_empty = []\n",
    "    \n",
    "    model_dice_mass_volume = []\n",
    "    model_dice_mass_volume_no_empty = []\n",
    "\n",
    "    model_accuracy  = []\n",
    "    model_precision = []\n",
    "    model_recall = []\n",
    "\n",
    "    for patient_id in patient_ids:\n",
    "\n",
    "         cum_iou=[]\n",
    "         cum_precision=[]\n",
    "         cum_recall=[]\n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "    \n",
    "         TP = []\n",
    "         FP = []\n",
    "         FN =[]\n",
    "         TN = []\n",
    "         print(patient_id)\n",
    "         patches_ds = datasets[patient_id][patches_dataset_key]\n",
    "         whole_image_ds = datasets[patient_id][whole_dataset_key]\n",
    "\n",
    "         fusion_dataset = PairedDataset(whole_image_ds, patches_ds, augment=False)\n",
    "        \n",
    "         prev_had_mask=False\n",
    "\n",
    "        \n",
    "\n",
    "         for idx, e in tqdm(enumerate(patches_ds), total = len(patches_ds)):\n",
    "            original_image = np.load(e[0]['image_meta_dict']['filename_or_obj'])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "    \n",
    "            merged_label_for_fusion = torch.zeros(original_image.shape)\n",
    "    \n",
    "            gt_label = np.load(e[0]['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "            \n",
    "            ## FIRST MODEL\n",
    "            for elem in e:\n",
    "                if elem['keep_sample']:\n",
    "                    image = torch.unsqueeze(elem['image'], 0)\n",
    "                    with torch.no_grad():\n",
    "                        model_patches = model_patches.to(\"cuda\")\n",
    "                        model_patches.eval()\n",
    "                        logits = model_patches(image.to(\"cuda\"))[0]\n",
    "    \n",
    "                    pr_mask = logits.sigmoid()\n",
    "                    pr_mask = pr_mask[0]\n",
    "                    #pr_mask_to_viz = (pr_mask.cpu().numpy() > 0.4).astype(int)\n",
    "    \n",
    "                    if pr_mask.sum()>0:\n",
    "                        #label = pr_mask\n",
    "                        label = reverse_transformations(elem, pr_mask, mode='patches')\n",
    "                        merged_label_for_fusion += label\n",
    "    \n",
    "    \n",
    "            original_image = np.transpose(original_image, (1,2,0))\n",
    "    \n",
    "            label_patches_for_fusion = merged_label_for_fusion[0]\n",
    "    \n",
    "            # SECOND MODEL\n",
    "\n",
    "            if fusion_dataset[idx][0]['keep_sample'] or fusion_dataset[idx][1]['keep_sample'] or fusion_dataset[idx][2]['keep_sample']:\n",
    "    \n",
    "                whole_image = torch.unsqueeze(fusion_dataset[idx][0]['image'], 0)\n",
    "                patch_image2 = torch.unsqueeze(fusion_dataset[idx][1]['image'], 0)\n",
    "                patch_image3 = torch.unsqueeze(fusion_dataset[idx][2]['image'], 0)\n",
    "                    \n",
    "        \n",
    "                with torch.no_grad():\n",
    "                    masks = []\n",
    "                    # pass to model\n",
    "                    model_whole = model_whole.to(\"cuda\")\n",
    "                    model_whole.eval()\n",
    "                    \n",
    "                    masks = model_whole(whole_image.to(\"cuda\"),patch_image2.to(\"cuda\"),patch_image3.to(\"cuda\"))\n",
    "                    masks = masks.sigmoid()\n",
    "                    \n",
    "    \n",
    "                label_whole = masks[0]\n",
    "                label_whole = (label_whole > 0.4).int()\n",
    "                label_whole = reverse_transformations(whole_image_ds[idx], label_whole, mode='whole')\n",
    "                label_whole = label_whole.squeeze()\n",
    "        \n",
    "                label_whole_for_fusion= masks[0]\n",
    "                label_whole_for_fusion = reverse_transformations(whole_image_ds[idx], label_whole_for_fusion, mode='whole')\n",
    "                \n",
    "                # Plot the first image\n",
    "            else:\n",
    "                label_whole_for_fusion = torch.zeros(original_image.shape)\n",
    "                \n",
    "            original_image_squeeze = np.load(e[0]['image_meta_dict']['filename_or_obj'])\n",
    "\n",
    "            fusion = fuse_segmentations(label_whole_for_fusion.numpy(), label_patches_for_fusion.numpy(), prob_threshold=0.4, boost_factor=3, penalty_factor=0.5, kernel_size=150)\n",
    "            \n",
    "            fusion = (fusion > 0.4).astype(int)\n",
    "    \n",
    "            fusion = np.expand_dims(fusion, 0)\n",
    "            pred_label=fusion\n",
    "\n",
    "            if not filter:\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(np.expand_dims(fusion,0).astype(int)), torch.tensor(np.expand_dims(gt_label,0).astype(int)), mode = \"binary\")\n",
    "                TP.append(tp)\n",
    "                FP.append(fp)\n",
    "                FN.append(fn)\n",
    "                TN.append(tn)\n",
    "             \n",
    "            \"\"\"plt.figure(figsize=(15, 10))\n",
    "    \n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.imshow(original_image.squeeze(),  cmap='gray')  # convert CHW -> HWC\n",
    "            plt.title(\"Image\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.imshow(((label_whole_for_fusion > 0.4).numpy().astype(int)).squeeze(), cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Whole\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.imshow(((label_patches_for_fusion > 0.4).numpy().astype(int)).squeeze(), cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Patch\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.imshow(gt_label.squeeze(), cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"GT\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.show()\n",
    "    \n",
    "            plt.imshow(fusion.squeeze() , cmap='gray')\n",
    "            plt.show()\"\"\"\n",
    "    \n",
    "    \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "            \n",
    "\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    " \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "\n",
    "\n",
    "         if filter:\n",
    "             predicted_label_volume = filter_masses(predicted_label_volume, min_slices=3, window_size=3) # H x W x N\n",
    "             # H x W x N -> N x H x W -> N x 1 x H x W\n",
    "             predicted_label_volume_for_stats = np.transpose(predicted_label_volume, (2, 0, 1))\n",
    "             predicted_label_volume_for_stats = np.expand_dims(predicted_label_volume_for_stats, 1)  # N x 1 x H x W\n",
    "\n",
    "             gt_label_volume_for_stats = np.transpose(gt_label_volume, (2, 0, 1))\n",
    "             gt_label_volume_for_stats = np.expand_dims(gt_label_volume_for_stats, 1)  # N x 1 x H x W             \n",
    "\n",
    "             \n",
    "             tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(predicted_label_volume_for_stats.astype(int)), torch.tensor(gt_label_volume_for_stats.astype(int)), mode = \"binary\")\n",
    "             TP =  [torch.tensor([[elem]]) for elem in tp.squeeze()]\n",
    "             FP =  [torch.tensor([[elem]]) for elem in fp.squeeze()]\n",
    "             FN =  [torch.tensor([[elem]]) for elem in fn.squeeze()]\n",
    "             TN =  [torch.tensor([[elem]]) for elem in tn.squeeze()]\n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "         detection_iou =  np.array(calculate_mass_detection_imagewise_volume(predicted_label_volume.astype(int), gt_label_volume)).mean()\n",
    "\n",
    "         mean_iou = compute_mean_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True)\n",
    "         mean_dice = compute_mean_dice_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True)\n",
    "\n",
    "         iou_mass_volume = compute_iou_npy(gt_label_volume, predicted_label_volume, class_id=1, reduction='micro-imagewise')\n",
    "         iou_mass_volume_no_empty = compute_iou_npy(gt_label_volume, predicted_label_volume, class_id=1, reduction='micro-imagewise', exclude_empty=True)\n",
    "         dice_mass_volume = compute_dice_score_npy(gt_label_volume, predicted_label_volume,reduction='micro-imagewise', class_id=1)\n",
    "         dice_mass_volume_no_empty = compute_dice_score_npy(gt_label_volume, predicted_label_volume, reduction='micro-imagewise',class_id=1, exclude_empty=True)\n",
    "\n",
    "        \n",
    "         accuracy = compute_accuracy_from_cumulator(TP, FP, FN, TN)\n",
    "         precision = compute_precision_from_cumulator(TP, FP, FN, TN)\n",
    "         recall = compute_recall_from_cumulator(TP, FP, FN, TN)\n",
    "        \n",
    "         print(\"CLASS MEAN IOU\", mean_iou)\n",
    "         print(\"CLASS MEAN DICE\", mean_dice)\n",
    "         print(\"DIOU\", detection_iou)\n",
    "         print(\"IOU MASS VOLUME\", iou_mass_volume)\n",
    "         print(\"IOU MASS VOLUME NO EMPTY\", iou_mass_volume_no_empty)\n",
    "         print(\"DICE MASS VOLUME \", dice_mass_volume)\n",
    "         print(\"DICE MASS VOLUME NO EMPTY \", dice_mass_volume_no_empty)\n",
    "\n",
    "         print(\"ACCURACY \", accuracy)\n",
    "         print(\"PRECISION \", precision)\n",
    "         print(\"RECALL\", recall)\n",
    "         \n",
    "         print()\n",
    "         model_class_mean_iou.append(mean_iou)\n",
    "         model_class_mean_dice.append(mean_dice)\n",
    "         model_detection_iou.append(detection_iou)\n",
    "        \n",
    "         model_iou_mass_volume.append(iou_mass_volume)\n",
    "         model_iou_mass_volume_no_empty.append(iou_mass_volume_no_empty)\n",
    "        \n",
    "         model_dice_mass_volume.append(dice_mass_volume)\n",
    "         model_dice_mass_volume_no_empty.append(dice_mass_volume_no_empty)\n",
    "\n",
    "         model_accuracy.append(accuracy)\n",
    "         model_precision.append(precision)\n",
    "         model_recall.append(recall)\n",
    "\n",
    "    model_class_mean_iou = np.array(model_class_mean_iou).mean()\n",
    "    model_class_mean_dice = np.array(model_class_mean_dice).mean()\n",
    "    model_detection_iou = np.array(model_detection_iou).mean()\n",
    "    \n",
    "    model_iou_mass_volume = np.array(model_iou_mass_volume).mean()\n",
    "    model_iou_mass_volume_no_empty = np.array(model_iou_mass_volume_no_empty).mean()\n",
    "    \n",
    "    model_dice_mass_volume = np.array(model_dice_mass_volume).mean()\n",
    "    model_dice_mass_volume_no_empty = np.array(model_dice_mass_volume_no_empty).mean()\n",
    "\n",
    "    model_accuracy = np.array(model_accuracy).mean()\n",
    "    model_precision = np.array(model_precision).mean()\n",
    "    model_recall = np.array(model_recall).mean()\n",
    "\n",
    "    print(\"MODEL CLASS MEAN IOU\", model_class_mean_iou)\n",
    "    print(\"MODEL CLASS MEAN DICE\", model_class_mean_dice)\n",
    "    print(\"MODEL DIOU\", model_detection_iou)\n",
    "        \n",
    "    print(\"MODEL IOU MASS VOLUME\", model_iou_mass_volume)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY\", model_iou_mass_volume_no_empty)\n",
    "    \n",
    "    print(\"MODEL DICE MASS VOLUME \", model_dice_mass_volume)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY \", model_dice_mass_volume_no_empty)\n",
    "\n",
    "    print(\"MODEL ACCURACY \", model_accuracy)\n",
    "    print(\"MODEL PRECISION \", model_precision)\n",
    "    print(\"MODEL RECALL\", model_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def print_model_params_and_memory(model):\n",
    "    # Function to calculate memory size in MB\n",
    "    def get_model_memory(model):\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        param_size = next(model.parameters()).element_size()  # Size of one parameter (in bytes)\n",
    "        total_memory = total_params * param_size / (1024 ** 2)  # Memory in MB\n",
    "        return total_memory\n",
    "    \n",
    "    # Calculate total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Calculate trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Calculate non-trainable parameters\n",
    "    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    \n",
    "    # Get model memory size\n",
    "    model_memory = get_model_memory(model)\n",
    "    \n",
    "    # Print the results in millions\n",
    "    print(f\"Total parameters: {total_params / 1e6:.2f} million\")\n",
    "    print(f\"Trainable parameters: {trainable_params / 1e6:.2f} million\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params / 1e6:.2f} million\")\n",
    "    print(f\"Memory required (in MB): {model_memory:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_aware_ensemble(model_whole_path, model_patches_path, patient_ids, datasets, whole_dataset_key, patches_dataset_key, filter=False, use_decoder_attention=True, use_simple_fusion=False,get_scores_for_statistics=False,get_only_masses=False):\n",
    "\n",
    "    model_whole = BreastModel.load_from_checkpoint(model_whole_path, strict=False, use_simple_fusion=use_simple_fusion, use_decoder_attention=use_decoder_attention)\n",
    "    model_patches = BreastModel2.load_from_checkpoint(model_patches_path, strict=False)\n",
    "\n",
    "\n",
    "    print_model_params_and_memory(model_whole)\n",
    "\n",
    "\n",
    "    print(\"aa\")\n",
    "\n",
    "    print_model_params_and_memory(model_patches)\n",
    "\n",
    "    print(\"aa\")\n",
    "\n",
    "    \n",
    "    model_class_mean_iou = []\n",
    "    model_class_mean_dice = []\n",
    "    model_detection_iou = []\n",
    "    \n",
    "    model_iou_mass_volume = []\n",
    "    model_iou_mass_volume_no_empty = []\n",
    "    \n",
    "    model_dice_mass_volume = []\n",
    "    model_dice_mass_volume_no_empty = []\n",
    "\n",
    "    model_accuracy = []\n",
    "    model_precision = []\n",
    "    model_recall = []\n",
    "\n",
    "\n",
    "    \n",
    "    TP = []\n",
    "    FP = []\n",
    "    FN = []\n",
    "    TN = []\n",
    "\n",
    "    detection_iou =  []\n",
    "\n",
    "    # Initialize performance metrics\n",
    "    inference_times = []  # To store the time for each volume inference\n",
    "    inference_times_slice = []  # To store the time for each volume inference\n",
    "    memory_usage = []  # To store the memory usage for each volume\n",
    "\n",
    "    for patient_id in patient_ids:\n",
    "\n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "\n",
    "         print(patient_id)\n",
    "         patches_ds = datasets[patient_id][patches_dataset_key]\n",
    "         whole_image_ds = datasets[patient_id][whole_dataset_key]\n",
    "\n",
    "         fusion_dataset = PairedDataset(whole_image_ds, patches_ds, augment=False)\n",
    "        \n",
    "         prev_had_mask=False\n",
    "\n",
    "         # Measure inference time per slice\n",
    "         start_time = time.time()\n",
    "\n",
    "         \n",
    "\n",
    "         \n",
    "         for idx, e in tqdm(enumerate(patches_ds), total = len(patches_ds)):\n",
    "\n",
    "            start_time_slice = time.time()\n",
    "\n",
    "\n",
    "             \n",
    "            original_image = np.load(e[0]['image_meta_dict']['filename_or_obj'])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "    \n",
    "            merged_label_for_fusion = torch.zeros(original_image.shape)\n",
    "    \n",
    "            gt_label = np.load(e[0]['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "            \n",
    "            ## FIRST MODEL\n",
    "            for elem in e:\n",
    "                if elem['keep_sample']:\n",
    "                    image = torch.unsqueeze(elem['image'], 0)\n",
    "                    with torch.no_grad():\n",
    "                        model_patches = model_patches.to(\"cuda\")\n",
    "                        model_patches.eval()\n",
    "                        logits = model_patches(image.to(\"cuda\"))[0]\n",
    "    \n",
    "                    pr_mask = logits.sigmoid()\n",
    "                    pr_mask = pr_mask[0]\n",
    "                    #pr_mask_to_viz = (pr_mask.cpu().numpy() > 0.4).astype(int)\n",
    "    \n",
    "                    if pr_mask.sum()>0:\n",
    "                        #label = pr_mask\n",
    "                        label = reverse_transformations(elem, pr_mask, mode='patches')\n",
    "                        merged_label_for_fusion += label\n",
    "    \n",
    "    \n",
    "            original_image = np.transpose(original_image, (1,2,0))\n",
    "    \n",
    "            label_patches_for_fusion = merged_label_for_fusion[0]\n",
    "    \n",
    "            # SECOND MODEL\n",
    "\n",
    "            if fusion_dataset[idx][0]['keep_sample'] or fusion_dataset[idx][1]['keep_sample'] or fusion_dataset[idx][2]['keep_sample']:\n",
    "    \n",
    "                whole_image = torch.unsqueeze(fusion_dataset[idx][0]['image'], 0)\n",
    "                patch_image2 = torch.unsqueeze(fusion_dataset[idx][1]['image'], 0)\n",
    "                patch_image3 = torch.unsqueeze(fusion_dataset[idx][2]['image'], 0)\n",
    "                    \n",
    "        \n",
    "                with torch.no_grad():\n",
    "                    masks = []\n",
    "                    # pass to model\n",
    "                    model_whole = model_whole.to(\"cuda\")\n",
    "                    model_whole.eval()\n",
    "                    \n",
    "                    masks = model_whole(whole_image.to(\"cuda\"),patch_image2.to(\"cuda\"),patch_image3.to(\"cuda\"))\n",
    "                    masks = masks.sigmoid()\n",
    "                    \n",
    "                    \n",
    "    \n",
    "                label_whole = masks[0]\n",
    "                label_whole = (label_whole > 0.4).int()\n",
    "                label_whole = reverse_transformations(whole_image_ds[idx], label_whole, mode='whole')\n",
    "                label_whole = label_whole.squeeze()\n",
    "        \n",
    "                label_whole_for_fusion= masks[0]\n",
    "                label_whole_for_fusion = reverse_transformations(whole_image_ds[idx], label_whole_for_fusion, mode='whole')\n",
    "                \n",
    "                # Plot the first image\n",
    "            else:\n",
    "                label_whole_for_fusion = torch.zeros(original_image.shape)\n",
    "                \n",
    "            original_image_squeeze = np.load(e[0]['image_meta_dict']['filename_or_obj'])\n",
    "\n",
    "            fusion = fuse_segmentations(label_whole_for_fusion.numpy(), label_patches_for_fusion.numpy(), prob_threshold=0.4, boost_factor=3, penalty_factor=0.5, kernel_size=150)\n",
    "            \n",
    "            fusion = (fusion > 0.4).astype(int)\n",
    "    \n",
    "            fusion = np.expand_dims(fusion, 0)\n",
    "            pred_label=fusion\n",
    "\n",
    "            if not filter:\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(np.expand_dims(fusion,0).astype(int)), torch.tensor(np.expand_dims(gt_label,0).astype(int)), mode = \"binary\")\n",
    "                TP.append(tp)\n",
    "                FP.append(fp)\n",
    "                FN.append(fn)\n",
    "                TN.append(tn)\n",
    "             \n",
    "            \"\"\"plt.figure(figsize=(15, 10))\n",
    "    \n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.imshow(original_image.squeeze(),  cmap='gray')  # convert CHW -> HWC\n",
    "            plt.title(\"Image\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.imshow(((label_whole_for_fusion > 0.4).numpy().astype(int)).squeeze(), cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Whole\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.imshow(((label_patches_for_fusion > 0.4).numpy().astype(int)).squeeze(), cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"Patch\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.imshow(gt_label.squeeze(), cmap='gray') # just squeeze classes dim, because we have only one class\n",
    "            plt.title(\"GT\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.show()\n",
    "    \n",
    "            plt.imshow(fusion.squeeze() , cmap='gray')\n",
    "            plt.show()\"\"\"\n",
    "    \n",
    "    \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "\n",
    "            # Measure inference time after processing  the voluyme\n",
    "            end_time_slice = time.time()\n",
    "            inference_times_slice.append(end_time_slice - start_time_slice)\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    "                    \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "\n",
    "        \n",
    "         # Measure inference time after processing  the volume\n",
    "         end_time = time.time()\n",
    "         inference_times.append(end_time - start_time)\n",
    "         memory_allocated = torch.cuda.memory_allocated()\n",
    "         memory_usage.append(memory_allocated)\n",
    "    \n",
    "         if filter:\n",
    "             print(\"filtering\")\n",
    "             predicted_label_volume = filter_masses(predicted_label_volume, min_slices=3, window_size=3) # H x W x N\n",
    "             # H x W x N -> N x H x W -> N x 1 x H x W\n",
    "             predicted_label_volume_for_stats = np.transpose(predicted_label_volume, (2, 0, 1))\n",
    "             predicted_label_volume_for_stats = np.expand_dims(predicted_label_volume_for_stats, 1)  # N x 1 x H x W\n",
    "\n",
    "             gt_label_volume_for_stats = np.transpose(gt_label_volume, (2, 0, 1))\n",
    "             gt_label_volume_for_stats = np.expand_dims(gt_label_volume_for_stats, 1)  # N x 1 x H x W             \n",
    "            \n",
    "             \n",
    "             tp, fp, fn, tn = smp.metrics.get_stats(torch.tensor(predicted_label_volume_for_stats.astype(int)), torch.tensor(gt_label_volume_for_stats.astype(int)), mode = \"binary\")\n",
    "             TP +=  [torch.tensor([[elem]]) for elem in tp.squeeze()]\n",
    "             FP +=  [torch.tensor([[elem]]) for elem in fp.squeeze()]\n",
    "             FN +=  [torch.tensor([[elem]]) for elem in fn.squeeze()]\n",
    "             TN +=  [torch.tensor([[elem]]) for elem in tn.squeeze()]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "         detection_iou+=calculate_mass_detection_imagewise_volume(predicted_label_volume.astype(int), gt_label_volume)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate mean and standard deviation for inference time and memory usage\n",
    "    mean_inference_time = np.mean(inference_times)\n",
    "    std_inference_time = np.std(inference_times)\n",
    "\n",
    "    mean_inference_time_slice = np.mean(inference_times_slice)\n",
    "    std_inference_time_slice = np.std(inference_times_slice)\n",
    "    \n",
    "    mean_memory_usage = np.mean(memory_usage)\n",
    "    std_memory_usage = np.std(memory_usage)\n",
    "        \n",
    "    # Frames per second (inference speed)\n",
    "    fps = 1 / mean_inference_time_slice\n",
    "        \n",
    "    # Final outputs\n",
    "    print(f\"Mean Inference Time per Volume: {mean_inference_time:.4f} seconds\")\n",
    "    print(f\"Standard Deviation of Inference Time per Volume: {std_inference_time:.4f} seconds\")\n",
    "\n",
    "    print(f\"Mean Inference Time per Slice: {mean_inference_time_slice:.4f} seconds\")\n",
    "    print(f\"Standard Deviation of Inference Time per Slice: {std_inference_time_slice:.4f} seconds\")\n",
    "\n",
    "    print(f\"Frames per second (FPS): {fps:.2f}\")\n",
    "    print(f\"Mean Memory Usage per Volume: {mean_memory_usage / (1024**2):.2f} MB\")  # Convert to MB\n",
    "    print(f\"Standard Deviation of Memory Usage: {std_memory_usage / (1024**2):.2f} MB\")  # Convert to MB\n",
    "\n",
    "    model_detection_iou = np.array(detection_iou).mean()\n",
    "    model_detection_iou_std = np.array(detection_iou).std()\n",
    "    \n",
    "    model_class_mean_iou, model_class_std_iou = compute_mean_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "    model_class_mean_dice, model_class_std_dice = compute_mean_dice_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "\n",
    "    model_iou_mass_volume , model_iou_mass_volume_std = compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=False, return_std=True)\n",
    "    model_iou_mass_volume_no_empty, model_iou_mass_volume_no_empty_std =compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, return_std=True)\n",
    "    model_iou_mass_volume_no_empty_optimistic, model_iou_mass_volume_no_empty_optimistic_std =compute_iou_imagewise_from_cumulator(TP, FP, FN, TN, exclude_empty=True, exclude_empty_only_gt=True, return_std=True)\n",
    "    \n",
    "    \n",
    "    model_dice_mass_volume, model_dice_mass_volume_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=False, return_std=True)\n",
    "    model_dice_mass_volume_no_empty, model_dice_mass_volume_no_empty_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=True, return_std=True)\n",
    "    model_dice_mass_volume_no_empty_optimistic, model_dice_mass_volume_no_empty_optimistic_std = compute_dice_imagewise_from_cumulator(TP, FP, FN, TN,exclude_empty=True, exclude_empty_only_gt=True,return_std=True)\n",
    "    \n",
    "    model_mean_accuracy_no_empty, model_mean_accuracy_no_empty_std = compute_accuracy_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_precision_no_empty,model_mean_precision_no_empty_std = compute_precision_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_recall_no_empty, model_mean_recall_no_empty_std = compute_recall_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "    model_mean_f1_no_empty, model_mean_f1_no_empty_std = compute_f1_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=True, return_std=True)\n",
    "        \n",
    "    model_accuracy_excluding_cases, model_accuracy_excluding_cases_std = compute_accuracy_excluding_cases(TP, FP, FN, TN, return_std=True)\n",
    "    model_precision_excluding_cases,model_precision_excluding_cases_std =compute_precision_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True)\n",
    "    model_recall_excluding_cases,model_recall_excluding_cases_std  =compute_recall_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True)\n",
    "\n",
    "    model_accuracy_no_empty, model_accuracy_no_empty_std = compute_accuracy_from_cumulator(TP, FP, FN, TN, exclude_empty=True, is_mean=False, return_std=True)\n",
    "    model_precision_no_empty,model_precision_no_empty_std =compute_precision_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "    model_recall_no_empty,model_recall_no_empty_std  = compute_recall_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "\n",
    "    model_f1_no_empty,model_f1_no_empty_std = compute_f1_excluding_cases_from_cumulator(TP, FP, FN, TN, return_std=True,exclude_only_zero_denominator=True)\n",
    "    \n",
    "    \n",
    "    print(\"MODEL CLASS MEAN IOU \", model_class_mean_iou)\n",
    "    print(\"MODEL CLASS STD IOU \", model_class_std_iou)\n",
    "    print()\n",
    "    print(\"MODEL CLASS MEAN DICE \", model_class_mean_dice)\n",
    "    print(\"MODEL CLASS STD DICE \", model_class_std_dice)\n",
    "    print()\n",
    "    print(\"MODEL DIOU\", model_detection_iou)\n",
    "    print(\"MODEL DIOU STD \", model_detection_iou_std) \n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME \", model_iou_mass_volume)\n",
    "    print(\"MODEL IOU MASS VOLUME STD \", model_iou_mass_volume_std)\n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY \", model_iou_mass_volume_no_empty)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY STD \", model_iou_mass_volume_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC \", model_iou_mass_volume_no_empty_optimistic)\n",
    "    print(\"MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD \", model_iou_mass_volume_no_empty_optimistic_std)\n",
    "    \n",
    "    print(\"MODEL DICE MASS VOLUME \", model_dice_mass_volume)\n",
    "    print(\"MODEL DICE MASS VOLUME STD \", model_dice_mass_volume_std)\n",
    "    print()\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY \", model_dice_mass_volume_no_empty)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY STD \", model_dice_mass_volume_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC \", model_dice_mass_volume_no_empty_optimistic)\n",
    "    print(\"MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD \", model_dice_mass_volume_no_empty_optimistic_std)\n",
    "    print() \n",
    "    print(\"MODEL MEAN ACCURACY NO EMPTY\", model_mean_accuracy_no_empty)\n",
    "    print(\"MODEL MEAN ACCURACY NO EMPTY STD\", model_mean_accuracy_no_empty_std)\n",
    "    print()                                                                              \n",
    "    print(\"MODEL MEAN PRECISION NO EMPTY\", model_mean_precision_no_empty)\n",
    "    print(\"MODEL MEAN PRECISION NO EMPTY STD\", model_mean_precision_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL MEAN RECALL NO EMPTY\", model_mean_recall_no_empty)\n",
    "    print(\"MODEL MEAN RECALL NO EMPTY STD\", model_mean_recall_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL MEAN F1 NO EMPTY\", model_mean_f1_no_empty)\n",
    "    print(\"MODEL MEAN F1 NO EMPTY STD\", model_mean_f1_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL ACCURACY EXCLUDING CASES \",  model_accuracy_excluding_cases)\n",
    "    print(\"MODEL ACCURACY EXCLUDING CASES STD \",  model_accuracy_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL PRECISION EXCLUDING CASES \",  model_precision_excluding_cases)\n",
    "    print(\"MODEL PRECISION EXCLUDING CASES STD \",  model_precision_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL RECALL EXCLUDING CASES \", model_recall_excluding_cases)\n",
    "    print(\"MODEL RECALL EXCLUDING CASES STD \", model_recall_excluding_cases_std)\n",
    "    print()\n",
    "    print(\"MODEL ACCURACY NO EMPTY \",  model_accuracy_no_empty)\n",
    "    print(\"MODEL ACCURACY NO EMPTY STD \",  model_accuracy_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL PRECISION NO EMPTY\",  model_precision_no_empty)\n",
    "    print(\"MODEL PRECISION NO EMPTY STD \",  model_precision_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL RECALL NO EMPTY \", model_recall_no_empty)\n",
    "    print(\"MODEL RECALL NO EMPTY STD \", model_recall_no_empty_std)\n",
    "    print()\n",
    "    print(\"MODEL F1 NO EMPTY \", model_f1_no_empty)\n",
    "    print(\"MODEL F1 NO EMPTY STD \", model_f1_no_empty_std)\n",
    "    print()\n",
    "        \n",
    "\n",
    "\n",
    "    if get_scores_for_statistics:\n",
    "            tp = torch.cat([tp for tp in TP])\n",
    "            fp = torch.cat([fp for fp in FP])\n",
    "            fn = torch.cat([fn for fn in FN])\n",
    "            tn = torch.cat([tn for tn in TN])\n",
    "\n",
    "\n",
    "            if get_only_masses:\n",
    "                # Create a mask where tp + fn is not equal to 0\n",
    "                mask = (tp + fn) != 0\n",
    "                \n",
    "                # Apply this mask to each tensor to filter out the desired values\n",
    "                tp = tp[mask]\n",
    "                fp = fp[mask]\n",
    "                fn = fn[mask]\n",
    "                tn = tn[mask]\n",
    "\n",
    "\n",
    "            miou_scores = compute_mean_iou_imagewise_from_cumulator(tp, fp, fn, tn, exclude_empty=False, return_std=False,reduce_mean=False)\n",
    "            mdice_scores = compute_mean_dice_imagewise_from_cumulator(tp, fp, fn, tn, exclude_empty=False, return_std=False,reduce_mean=False)\n",
    "            mf1_scores = compute_f1_from_cumulator(tp, fp, fn, tn, exclude_empty=False, is_mean=True, return_std=False,reduce_mean=False)\n",
    "\n",
    "\n",
    "            scores_dict = {\n",
    "                 'miou': miou_scores.squeeze().tolist(),\n",
    "                 'mdice': mdice_scores.squeeze().tolist(),\n",
    "                 \"mf1\": mf1_scores.squeeze().tolist(),\n",
    "            }\n",
    "            return scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATASET AWARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet, SwinUNETR, BasicUNetPlusPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TEST MULTI-UNET SUB CABFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d53ff2b42c4400b08d7924f636e590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ac7c802abb48b49e89bfa362656aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317c208f7f914bb2ad35a64579989018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b27e1ddd6a449492ba5d8fdd2c9775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607202033a2b4713b08854e7ee128978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9fece09d6b4e5084dd222ef8b9cbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba219d0c078d4768b4a1e74713963852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c10195ed2ac4ea994b09df91ca6991b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5041c0568634e04b3d598ce18144a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255361889d0f46f5b3460c2702422f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf875af193842aa9db9bca105d479db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bceffc646dd34962ad5f522568b2288a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d06245f7e574689852cab1af53a451e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da832a153174ad98063627c88315213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c5b0c093aa4f849ee88e18a959f8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51354f006ca14a2a9072026ac432308e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34f0a61a49348db86dcfb57b2ddf9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d40af4172a47fd8a95b653616bf4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c581b0366648b5973a0b187f7dc513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d423d8d4b7e04d31aaad4bd0c0efb64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ce701888684a2f8c523d9d457b2e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.58887863\n",
      "MODEL CLASS STD IOU  0.15059951\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.60679895\n",
      "MODEL CLASS STD DICE  0.1754792\n",
      "\n",
      "MODEL DIOU 0.8334518909549894\n",
      "MODEL DIOU STD  0.29091198719192896\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.5803412795066833\n",
      "MODEL IOU MASS VOLUME STD  0.4636002779006958\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.1788276880979538\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.3012268543243408\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.5764801502227783\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.2515328526496887\n",
      "MODEL DICE MASS VOLUME  0.5983843207359314\n",
      "MODEL DICE MASS VOLUME STD  0.4661327004432678\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.21413369476795197\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.35099151730537415\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.6902948617935181\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.2616250813007355\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.5891403257846832\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.15114951133728027\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.6082805916666985\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.17821946740150452\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.8810059130191803\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.14072570204734802\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.6067989\n",
      "MODEL MEAN F1 NO EMPTY STD 0.17547919\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9986722469329834\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.001595282\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.7104867100715637\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.25359994\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.762446939945221\n",
      "MODEL RECALL EXCLUDING CASES STD  0.28058168\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.178827702999115\n",
      "MODEL ACCURACY NO EMPTY STD  0.30122685\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.21667324006557465\n",
      "MODEL PRECISION NO EMPTY STD  0.35582206\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.762446939945221\n",
      "MODEL RECALL NO EMPTY STD  0.28058168\n",
      "\n",
      "MODEL F1 NO EMPTY  0.21413369476795197\n",
      "MODEL F1 NO EMPTY STD  0.35099152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_fusion = test_dataset_aware_fusion(model_path=\"PRIVATE-FUSION-SUB-CABL-FINAL.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", strict=True, get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['miou', 'mdice', 'mf1'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_statistics_fusion.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_fcn.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_fusion, \"scores_for_statistics_fcn.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao\n"
     ]
    }
   ],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TEST MULTI-UNET SUB CABFL NO DECODER ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8350c83a6148bc9d9aaa98d5e12cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8e9ee4f9f24e75881f5b336c454eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c01ec613b6148c6a2cda34983bef272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0acef2582740c094852205df6e2667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e38486a658443387022e183ee46bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad555a2158684f82b6f0e7719e8708f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7987bfdd49154b1a8a1a9b9dd77462d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f16c90398f45d884f0c9dd28dbaaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf2459cca3e4988a67a35587a20afb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbecc89caec44af08697ae78ec193dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bf822f2617463a8582bde4d09fb420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81b57139bf94a30b03721f1228159ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d0403d4fe349698c97da64de0a1e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b92052f973c4aa18d783261ed2946c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04919985d51a469a9123ccd77d3b0c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72833073beb488781f16468c465f09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94408bffc1947f4a7bde7c02a224aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c047e8bb89d74e4f84660a4fec376308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800f28f394ed4e4abd2560c613e196c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37224a0c32d4447961dcb6f20d6b8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eae845583444b21a3431c24ff1ed726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.58709514\n",
      "MODEL CLASS STD IOU  0.15093568\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.60390264\n",
      "MODEL CLASS STD DICE  0.17522915\n",
      "\n",
      "MODEL DIOU 0.8214560608616285\n",
      "MODEL DIOU STD  0.3010394225147611\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.5586973428726196\n",
      "MODEL IOU MASS VOLUME STD  0.46707409620285034\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.17504799365997314\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.30202382802963257\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.5906805992126465\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.24960079789161682\n",
      "MODEL DICE MASS VOLUME  0.5764503479003906\n",
      "MODEL DICE MASS VOLUME STD  0.4708462059497833\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.20823468267917633\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.3505549132823944\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.7026655077934265\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.25933638215065\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.5872945711016655\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.1514940857887268\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.6070786863565445\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.18066361546516418\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.878357857465744\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.13775591552257538\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.604002483189106\n",
      "MODEL MEAN F1 NO EMPTY STD 0.17551904916763306\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9987033009529114\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0015986821\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.7322289347648621\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.25597906\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.7570728063583374\n",
      "MODEL RECALL EXCLUDING CASES STD  0.2747556\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.17504797875881195\n",
      "MODEL ACCURACY NO EMPTY STD  0.30202383\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.21425917744636536\n",
      "MODEL PRECISION NO EMPTY STD  0.3607676\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.7570728063583374\n",
      "MODEL RECALL NO EMPTY STD  0.2747556\n",
      "\n",
      "MODEL F1 NO EMPTY  0.20823468267917633\n",
      "MODEL F1 NO EMPTY STD  0.3505549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_fusion_nda = test_dataset_aware_fusion(model_path=\"PRIVATE-FUSION-SUB-CABL-NO-DECODER-ATTENTION-FINAL.ckpt\", \n",
    "                              patient_ids=x_test,\n",
    "                              datasets=datasets, \n",
    "                              use_decoder_attention=False,\n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", strict=True, get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_fusion_final_nda.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_fusion_nda, \"scores_for_statistics_fusion_final_nda.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TEST MULTI-UNET SUB CABFL USE SIMPLE FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5d37116b9b4f1bad04f89ec6dc132d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afaffea9e374053bc91b31ef5dfac32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce8f690894d423085bdf9da163903a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a015d4dfa54051b433836f9f424f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5a5e7a2afa46fabfca81b7b38d950d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52078a19d02041eb8a09cd8af2c9f672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b930106bbd844482820d5591a3a3a3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a8139ebc714e6eb8725f7b3031197f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ba4930b4f64daca583a306ca5d9d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be3ec16132a4b949b1b1b483bae1537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a965f7eac6ce445b9c532ad58ed9e312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6a868ee3274e9abc4ed7e82712ce10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc77d315eaa34c5cb85a39f43fe1ac6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d969a8eb064894b53460bb47b1781f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7bec9f4832402f82a776d0399cb9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2d5c5409f3432b98372becb83f49a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f32ced6b18a4f7388bf091df83492ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edced9bef7a4bcab434d84cb32113de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea0e49d4d6f4361be5dfeb54dbd7901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd280dbf3964149a10430c2ff8c1a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956d4a1cd36f4b8cb2a2dfc42c95a69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.6434665\n",
      "MODEL CLASS STD IOU  0.1672613\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.67304754\n",
      "MODEL CLASS STD DICE  0.19436733\n",
      "\n",
      "MODEL DIOU 0.7120013631668699\n",
      "MODEL DIOU STD  0.36402384669588006\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.7919582724571228\n",
      "MODEL IOU MASS VOLUME STD  0.37096554040908813\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.2878640294075012\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.33460259437561035\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.5304722785949707\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.2786027789115906\n",
      "MODEL DICE MASS VOLUME  0.8091059327125549\n",
      "MODEL DICE MASS VOLUME STD  0.36394983530044556\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.3465612828731537\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.38880589604377747\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.6386389136314392\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.303384393453598\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.6437960863113403\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.16775724291801453\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.716367319226265\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.22229346632957458\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.805164247751236\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.15493907034397125\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.6730475\n",
      "MODEL MEAN F1 NO EMPTY STD 0.19436733\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9986828565597534\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0018505327\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.835610032081604\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.2088396\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.6104287505149841\n",
      "MODEL RECALL EXCLUDING CASES STD  0.30956167\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.2878640294075012\n",
      "MODEL ACCURACY NO EMPTY STD  0.33460256\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.4329064190387726\n",
      "MODEL PRECISION NO EMPTY STD  0.4437659\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.6104287505149841\n",
      "MODEL RECALL NO EMPTY STD  0.30956167\n",
      "\n",
      "MODEL F1 NO EMPTY  0.3465612828731537\n",
      "MODEL F1 NO EMPTY STD  0.3888059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_fusion_sf = test_dataset_aware_fusion(model_path=\"PRIVATE-FUSION-SUB-CABL-NO-FUSION-FINAL.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              use_simple_fusion=True,\n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", strict=True, get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['miou', 'mdice', 'mf1'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_statistics_fusion_sf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_fusion_final_sf.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_fusion_sf, \"scores_for_statistics_fusion_final_sf.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TEST MULTI-UNET SUB CABFL SIMPLE FUSION + NO DECODER ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47460eea947e4c7a89ec98be4acb3ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a5b3035de643ef9cb84afe15f53505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70821aa732574910b8f9c31f68abc011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5563c78753e94506ab11ea2a1c7decf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6990367a64fc4c19889bcfd1afeb2208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8e2af00605405481dcc61c02423e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb7aff620a74041b6ba869e343993fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421f5f3aa33e48c2b4307a459ba9770f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f62d5b789aa48fabe2bcbe0b9cc5557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38105b321d8e44369fed10f6acbd0022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a43e4f403c1469a91d65166757ce7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b05f8f3a2124fd3bff59d2ff43b133b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6c4a15956f4a97853050b9577eec77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e928fc534eb458ebd4ca7145a654d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f0bc2f4f9b45239e56436ca811f208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e663f032e52a461c8eafa7ee1b90346f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c335905f948c42d48881952a59577b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6453181a1a42aeb97a22a9f9832320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056514f967b449c09afc1b41dcf7094c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328ed3bb68eb4ab1b8c40576c66d6cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69e1df3f8e54986976674f0218c67d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.5779559\n",
      "MODEL CLASS STD IOU  0.13250133\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.5989\n",
      "MODEL CLASS STD DICE  0.16153026\n",
      "\n",
      "MODEL DIOU 0.8688531439746999\n",
      "MODEL DIOU STD  0.27509447742095167\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.582230269908905\n",
      "MODEL IOU MASS VOLUME STD  0.4607909023761749\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.15753380954265594\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.26509344577789307\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.4927718937397003\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.23372112214565277\n",
      "MODEL DICE MASS VOLUME  0.6026008725166321\n",
      "MODEL DICE MASS VOLUME STD  0.4607812464237213\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.198612779378891\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.323122501373291\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.6212685108184814\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.2530020475387573\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.5783647522330284\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.13333997130393982\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.5893523544073105\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.1507631242275238\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.9111670553684235\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.132219135761261\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.5989001\n",
      "MODEL MEAN F1 NO EMPTY STD 0.16153026\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9981210827827454\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0017983903\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.5680516958236694\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.2587704\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.8230670690536499\n",
      "MODEL RECALL EXCLUDING CASES STD  0.2629169\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.15753380954265594\n",
      "MODEL ACCURACY NO EMPTY STD  0.26509342\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.17877617478370667\n",
      "MODEL PRECISION NO EMPTY STD  0.30111024\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.8230670690536499\n",
      "MODEL RECALL NO EMPTY STD  0.2629169\n",
      "\n",
      "MODEL F1 NO EMPTY  0.198612779378891\n",
      "MODEL F1 NO EMPTY STD  0.3231225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_fusion_sf_nda = test_dataset_aware_fusion(model_path=\"PRIVATE-FUSION-SUB-CABL-NO-DECODER-ATTENTION-NO-FUSION.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                                                         use_simple_fusion=True,\n",
    "                                                         use_decoder_attention=False,\n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", strict=True, get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['miou', 'mdice', 'mf1'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_statistics_fusion_sf_nda.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_fusion_sf_nda.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_fusion_sf_nda, \"scores_for_statistics_fusion_sf_nda.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ENSEMBLE MULTI-UNET CABFL + CABFL PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ec002811c94d3b879620c73a202d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eaeb260efff47fab1780ddb104d6dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7169e226e2b42a4b22daa8e285ae9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5c7d00a2da4df7b267552835dcfe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d255ec1ce0414c40901aeb5625fd4f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a540e6401b41288e744fff6eed4db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7c4446d16149b2bf08d97ef804a576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de264645f8b4daaa393f2540f13e5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63f380b80d74ea79dedccf565941d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155c04852358495e85a091277a56a787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287d485b254c40b2b3124a13cfcbee01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb280e358e98474d91a06fb20bbcd9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed69f507fd694e548b5d56d6f7a10b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819b0e41cc1b47978e7c2d27bb1545b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291bfd994f6141bcad2cf88fd8b982c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf30fdf1d9e42208249d7d3dfc2ca76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3355177f9c4e40dea380c5349c906bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c644d5ba02c24f4183065ee3990b105d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dbc33f395d44139e07636099af5db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1ef4f4fdf94868995a54064ce38836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8e6a005246496a90d2fb7af876d63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.62867385\n",
      "MODEL CLASS STD IOU  0.16370061\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.65572965\n",
      "MODEL CLASS STD DICE  0.19129984\n",
      "\n",
      "MODEL DIOU 0.8682983429003069\n",
      "MODEL DIOU STD  0.271822030176516\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.7384881377220154\n",
      "MODEL IOU MASS VOLUME STD  0.40412038564682007\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.2587006390094757\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.32734376192092896\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.5756822824478149\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.23657383024692535\n",
      "MODEL DICE MASS VOLUME  0.7573391199111938\n",
      "MODEL DICE MASS VOLUME STD  0.3995882570743561\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.31213682889938354\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.38259050250053406\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.6945930123329163\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.24510321021080017\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.6291116625070572\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.16422304511070251\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.6473107486963272\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.1833566427230835\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.9222642183303833\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.13463719189167023\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.6559488773345947\n",
      "MODEL MEAN F1 NO EMPTY STD 0.19157151877880096\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9986129999160767\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.001642388\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.6703987121582031\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.2306953\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.8449301719665527\n",
      "MODEL RECALL EXCLUDING CASES STD  0.26830244\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.2587006390094757\n",
      "MODEL ACCURACY NO EMPTY STD  0.32734376\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.2946971356868744\n",
      "MODEL PRECISION NO EMPTY STD  0.36621448\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.8449301719665527\n",
      "MODEL RECALL NO EMPTY STD  0.26830244\n",
      "\n",
      "MODEL F1 NO EMPTY  0.31213682889938354\n",
      "MODEL F1 NO EMPTY STD  0.3825905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_ensemble = test_dataset_aware_ensemble(\n",
    "                              model_whole_path=\"PRIVATE-FUSION-SUB-CABL-FINAL.ckpt\", \n",
    "                              model_patches_path=\"RESNET-PATCHES-PRIVATE.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\",\n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_ensemble.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_ensemble, \"scores_for_statistics_ensemble.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ENSEMBLE MULTI-UNET CABFL + CABFL PATCHES -filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fecfeb8bb84c0393ad234050f374f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 82\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "fine\n",
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8f8662c3d3484da1cca3d8b14bc9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 290\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "fine\n",
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b163612222e455bb3a4ee321cfa3f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 56\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "fine\n",
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7a9354ceea416597016c6e4248310f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 31\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "fine\n",
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9beec04ffdd4cb28263a110a8408a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 12\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "fine\n",
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955739d475f3419683ff151d5834d045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 151\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "fine\n",
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cace88e34e524dc18714e337a98efe4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 34\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "fine\n",
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7841e37321b146c1abf11a6db6e1fb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 84\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "fine\n",
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31771a77b7bd48c0ba032ec77ccf6727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "fine\n",
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce7fd5888554f4ea7e5cff408269861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 138\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "fine\n",
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31318476c7f476683b04c189b4b1131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 25\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "fine\n",
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02162bc065a344ed8cb216268ef32124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 21\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "fine\n",
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1de919912a4513b573f9eebd44b211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 66\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "fine\n",
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ef67b386b34782966ac0334071aefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 36\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "fine\n",
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f6f1836d0147bebd09e30a03ea8eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "fine\n",
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb2c18d97dd43b48a903c73c7f36d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "fine\n",
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a1297be5ed4b3ca8b451d03851c2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 20\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "fine\n",
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4dfd79f4794c5fbed2b22dec4bdd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 23\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "fine\n",
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75558c7fb0043d882b013cbc63972f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 124\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "fine\n",
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b44e022f30b498e81768f1a1aca4608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 41\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "fine\n",
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f03c4380cb4dd7a6c2de45490b3ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 66\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "fine\n",
      "MODEL CLASS MEAN IOU  0.64619195\n",
      "MODEL CLASS STD IOU  0.16802692\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.67626256\n",
      "MODEL CLASS STD DICE  0.19542588\n",
      "\n",
      "MODEL DIOU 0.8661127920923898\n",
      "MODEL DIOU STD  0.2729700412436063\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.777884840965271\n",
      "MODEL IOU MASS VOLUME STD  0.3782494366168976\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.29373177886009216\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.33596840500831604\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.5827028751373291\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.23566047847270966\n",
      "MODEL DICE MASS VOLUME  0.7965869903564453\n",
      "MODEL DICE MASS VOLUME STD  0.37179121375083923\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.3532000184059143\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.39083045721054077\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.7006756067276001\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.2440359890460968\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.6466539651155472\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.16851016879081726\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.6682574599981308\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.18779051303863525\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.921658456325531\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.13569508492946625\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.676493912935257\n",
      "MODEL MEAN F1 NO EMPTY STD 0.19567885994911194\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9986417889595032\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0016348846\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.6822951436042786\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.22204499\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.8436644077301025\n",
      "MODEL RECALL EXCLUDING CASES STD  0.27048147\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.29373177886009216\n",
      "MODEL ACCURACY NO EMPTY STD  0.33596843\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.33659133315086365\n",
      "MODEL PRECISION NO EMPTY STD  0.37507817\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.8436644077301025\n",
      "MODEL RECALL NO EMPTY STD  0.27048147\n",
      "\n",
      "MODEL F1 NO EMPTY  0.3532000184059143\n",
      "MODEL F1 NO EMPTY STD  0.39083046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_ensemble_filtered = test_dataset_aware_ensemble(\n",
    "                               model_whole_path=\"PRIVATE-FUSION-SUB-CABL-FINAL.ckpt\", \n",
    "                              model_patches_path=\"RESNET-PATCHES-PRIVATE.ckpt\", \n",
    "                              patient_ids=x_test,\n",
    "                              datasets=datasets, \n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", get_scores_for_statistics=True, filter=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_ensemble_filtered.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_ensemble_filtered, \"scores_for_statistics_ensemble_filtered.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ENSEMBLE MULTI-UNET CABFL + CABFL PATCHES NO FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5cfda17a534faf87421f066ad82a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e716e2111d8943d59000030194058b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f871f69bc3cc42f6a217423aad64fcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d47f89e38e40a98ff6aa8d6dd19717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5830431e0aef4dc88f792b03e3eeba29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c82b35644845088134520a66b4d86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4c7531f99041b681f020077a639f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9b1b198e5f46409de291e024dc8299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb385d96f4d94ccb97773278b9e59529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d4eb025a3c4aa8ac6ee6bacebe90a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe285ab706d74e42ae94de6d9fc9b578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938ea6b867f644bda9d9f766ccc5dcd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cf3f72da32487d945adb1e3b155a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e73ee17a3c4f43be3568e672e9880f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330bf4530afb4d99bd2c086f3a440ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059e6518a6f54b419c78156970552c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06af9993ceb7400e8f491fbbdf141364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bb7181c1de4823918592c3497cb8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8434cc58574e948b2f4581c3c1a1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c3824f258149b8b95dc7558a22e5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3bed0be7694163b7fbebf6563191ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.67957485\n",
      "MODEL CLASS STD IOU  0.17598999\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.71292734\n",
      "MODEL CLASS STD DICE  0.20100383\n",
      "\n",
      "MODEL DIOU 0.8016295954445445\n",
      "MODEL DIOU STD  0.32977545214953796\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.8356460332870483\n",
      "MODEL IOU MASS VOLUME STD  0.3316093385219574\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.36033472418785095\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.35189858078956604\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.584010899066925\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.26471084356307983\n",
      "MODEL DICE MASS VOLUME  0.8526331186294556\n",
      "MODEL DICE MASS VOLUME STD  0.3229965269565582\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.4264479875564575\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.4019955098628998\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.6911637187004089\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.28098264336586\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.6800151616334915\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.17638441920280457\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.7182077467441559\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.19697409868240356\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.893989235162735\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.15965613722801208\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.713147759437561\n",
      "MODEL MEAN F1 NO EMPTY STD 0.20121577382087708\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9987841248512268\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0015209324\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.7350437641143799\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.2028694\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.7881931662559509\n",
      "MODEL RECALL EXCLUDING CASES STD  0.31867835\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.36033472418785095\n",
      "MODEL ACCURACY NO EMPTY STD  0.35189858\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.4365053176879883\n",
      "MODEL PRECISION NO EMPTY STD  0.39338797\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.7881931662559509\n",
      "MODEL RECALL NO EMPTY STD  0.31867835\n",
      "\n",
      "MODEL F1 NO EMPTY  0.4264479875564575\n",
      "MODEL F1 NO EMPTY STD  0.4019955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_ensemble_nf = test_dataset_aware_ensemble(\n",
    "                              model_whole_path=\"PRIVATE-FUSION-SUB-CABL-NO-FUSION-FINAL.ckpt\", \n",
    "                              model_patches_path=\"RESNET-PATCHES-PRIVATE.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              use_simple_fusion=True,\n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\",\n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_statistics_ensemble_nf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_ensemble_nf.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_ensemble_nf, \"scores_for_statistics_ensemble_nf.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENSEMBLE MULTI-UNET CABFL + CABFL PATCHES -filtered NO FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 21.56 million\n",
      "Trainable parameters: 21.56 million\n",
      "Non-trainable parameters: 0.00 million\n",
      "Memory required (in MB): 82.25 MB\n",
      "aa\n",
      "Total parameters: 32.52 million\n",
      "Trainable parameters: 32.52 million\n",
      "Non-trainable parameters: 0.00 million\n",
      "Memory required (in MB): 124.04 MB\n",
      "aa\n",
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ef44ade8af4b489e5aa7e2b6507e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores_for_statistics_ensemble_sf_filtered \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset_aware_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmodel_whole_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPRIVATE-FUSION-SUB-CABL-NO-FUSION-FINAL.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmodel_patches_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRESNET-PATCHES-PRIVATE.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mpatient_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                              \u001b[49m\u001b[43muse_simple_fusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mwhole_dataset_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_thorax_sub_test_ds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mpatches_dataset_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatches_sub_test_ds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_scores_for_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[118], line 117\u001b[0m, in \u001b[0;36mtest_dataset_aware_ensemble\u001b[0;34m(model_whole_path, model_patches_path, patient_ids, datasets, whole_dataset_key, patches_dataset_key, filter, use_decoder_attention, use_simple_fusion, get_scores_for_statistics, get_only_masses)\u001b[0m\n\u001b[1;32m    114\u001b[0m     model_whole \u001b[38;5;241m=\u001b[39m model_whole\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m     model_whole\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 117\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhole_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpatch_image2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpatch_image3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39msigmoid()\n\u001b[1;32m    122\u001b[0m label_whole \u001b[38;5;241m=\u001b[39m masks[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 714\u001b[0m, in \u001b[0;36mBreastModel.forward\u001b[0;34m(self, image1, image2, image3)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image1,image2, image3):\n\u001b[0;32m--> 714\u001b[0m     mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 216\u001b[0m, in \u001b[0;36mMultiInputUNet.forward\u001b[0;34m(self, x1, x2, x3)\u001b[0m\n\u001b[1;32m    214\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder1(fused_features, fused_skips4)\n\u001b[1;32m    215\u001b[0m d2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder2(d1, fused_skips3)\n\u001b[0;32m--> 216\u001b[0m d3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder3\u001b[49m\u001b[43m(\u001b[49m\u001b[43md2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfused_skips2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder4(d3, fused_skips1)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(d4)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 77\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, combined_skip)\u001b[0m\n\u001b[1;32m     74\u001b[0m     combined_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(g\u001b[38;5;241m=\u001b[39mx, x\u001b[38;5;241m=\u001b[39mcombined_skip)\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, combined_skip], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/monai/data/meta_tensor.py:303\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     unpack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mMetaTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m unpack \u001b[38;5;28;01melse\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/monai/data/meta_tensor.py:218\u001b[0m, in \u001b[0;36mMetaTensor.update_meta\u001b[0;34m(rets, func, args, kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m meta_args \u001b[38;5;241m=\u001b[39m MetaObj\u001b[38;5;241m.\u001b[39mflatten_meta_objs(args, kwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    217\u001b[0m ret\u001b[38;5;241m.\u001b[39mis_batch \u001b[38;5;241m=\u001b[39m is_batch\n\u001b[0;32m--> 218\u001b[0m \u001b[43mret\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_meta_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the following is not implemented but the network arch may run into this case:\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# if func == torch.cat and any(m.is_batch if hasattr(m, \"is_batch\") else False for m in meta_args):\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m#     raise NotImplementedError(\"torch.cat is not implemented for batch of MetaTensors.\")\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batch:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/monai/data/meta_obj.py:136\u001b[0m, in \u001b[0;36mMetaObj.copy_meta_from\u001b[0;34m(self, input_objs, copy_attr, keys)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m {a: first_meta[a] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m first_meta}  \u001b[38;5;66;03m# shallow copy for performance\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate({a: MetaObj\u001b[38;5;241m.\u001b[39mcopy_items(first_meta[a]) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m first_meta})\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/monai/data/meta_obj.py:136\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m {a: first_meta[a] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m first_meta}  \u001b[38;5;66;03m# shallow copy for performance\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate({a: \u001b[43mMetaObj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_meta\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m first_meta})\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/monai/data/meta_obj.py:109\u001b[0m, in \u001b[0;36mMetaObj.copy_items\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy_items\u001b[39m(data):\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"returns a copy of the data. list and dict are shallow copied for efficiency purposes.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_immutable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/monai/utils/misc.py:159\u001b[0m, in \u001b[0;36mis_immutable\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_immutable\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    Determine if the object is an immutable object.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    see also https://github.com/python/cpython/blob/3.11/Lib/copy.py#L109\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcomplex\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores_for_statistics_ensemble_sf_filtered = test_dataset_aware_ensemble(\n",
    "                              model_whole_path=\"PRIVATE-FUSION-SUB-CABL-NO-FUSION-FINAL.ckpt\", \n",
    "                              model_patches_path=\"RESNET-PATCHES-PRIVATE.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              use_simple_fusion=True,\n",
    "                              filter=True,\n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\",\n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['miou', 'mdice', 'mf1'])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_statistics_ensemble_nf_filtered.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_ensemble_sf_filtered.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_ensemble_nf_filtered, \"scores_for_statistics_ensemble_sf_filtered.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENSEMBLE MULTI-UNET CABFL + CABFL PATCHES NO DECODER ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a130269f63164809a099c402b49394d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd42e729415845f384c8c84b67935eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3267b4b2774b6e8353ed678b41fbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbfeb4e2cab4b3cbc4acd0aec1e8bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c104081e51494372a91bd0f1676693b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2da57d87944b32a50c2a625a6683a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e98392bbce140be820888c879c76e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ea1577d7c9463e9a8c9378fcb860d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fd01b1cb644248adce68f1f7bf6cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91d56ef3aa340108a07598b2c04a5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819fc8efeca548dc8d62ae5557bd0ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b17e90bd72540b7aed03f1d09cc9f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e887d00b0b47b8a7c4babc3baf4b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff03f45a95a4428915770d41fc9e613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7972d5fced774025b3cc71e6f6024674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ff94469cb745589fde9607c8f380fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dcd0eff9794d009ac263f887bf104c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9c1868876d48cda3b9ea426fda1a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e9c6a16fcf41e589cc05fd60c5935f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0263a48f76bd414387f74aa69f244419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7905fadc472c43bf9e09c25c7e4a51a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Inference Time per Volume: 47.6399 seconds\n",
      "Standard Deviation of Inference Time per Volume: 15.6155 seconds\n",
      "Mean Inference Time per Slice: 0.2495 seconds\n",
      "Standard Deviation of Inference Time per Slice: 0.1094 seconds\n",
      "Frames per second (FPS): 4.01\n",
      "Mean Memory Usage per Volume: 542.38 MB\n",
      "Standard Deviation of Memory Usage: 254.26 MB\n",
      "MODEL CLASS MEAN IOU  0.62876403\n",
      "MODEL CLASS STD IOU  0.16610433\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.6545451\n",
      "MODEL CLASS STD DICE  0.19311067\n",
      "\n",
      "MODEL DIOU 0.8522614071450948\n",
      "MODEL DIOU STD  0.2834916392194654\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.732399046421051\n",
      "MODEL IOU MASS VOLUME STD  0.40822914242744446\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.25871676206588745\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.3322497010231018\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.5891362428665161\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.23814257979393005\n",
      "MODEL DICE MASS VOLUME  0.7507985830307007\n",
      "MODEL DICE MASS VOLUME STD  0.4047059118747711\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.30968543887138367\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.3862646222114563\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.7051994204521179\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.24664755165576935\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.6291438192129135\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.16663658618927002\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.6475036442279816\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.1866370439529419\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.9183567464351654\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.1362234205007553\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.6545451\n",
      "MODEL MEAN F1 NO EMPTY STD 0.19311067\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9986384510993958\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0016482882\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.6852236390113831\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.23534954\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.8370680809020996\n",
      "MODEL RECALL EXCLUDING CASES STD  0.2715634\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.25871679186820984\n",
      "MODEL ACCURACY NO EMPTY STD  0.3322497\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.29508185386657715\n",
      "MODEL PRECISION NO EMPTY STD  0.37279537\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.8370680809020996\n",
      "MODEL RECALL NO EMPTY STD  0.2715634\n",
      "\n",
      "MODEL F1 NO EMPTY  0.30968543887138367\n",
      "MODEL F1 NO EMPTY STD  0.38626462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_ensemble_nda = test_dataset_aware_ensemble(\n",
    "                              model_whole_path=\"PRIVATE-FUSION-SUB-CABL-NO-DECODER-ATTENTION-FINAL.ckpt\", \n",
    "                              model_patches_path=\"RESNET-PATCHES-PRIVATE.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              use_simple_fusion=False,\n",
    "                              use_decoder_attention=False,\n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\",\n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264774144"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['miou', 'mdice', 'mf1'])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_statistics_ensemble_nda.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_ensemble_nda.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_ensemble_nda, \"scores_for_statistics_ensemble_nda.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENSEMBLE MULTI-UNET CABFL + CABFL PATCHES -filtered NO DECODER ATTENTION (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da36150a4fbc484682cf5553dc8244d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_ensemble_nda_filtered = test_dataset_aware_ensemble(\n",
    "                              model_whole_path=\"PRIVATE-FUSION-SUB-CABL-NO-DECODER-ATTENTION-FINAL.ckpt\", \n",
    "                              model_patches_path=\"RESNET-PATCHES-PRIVATE.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              use_simple_fusion=False,\n",
    "                              use_decoder_attention=False,\n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\",\n",
    "                              filter=True,\n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_statistics_ensemble_nda_filtered.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(scores_for_statistics_ensemble_nda_filtered, \"scores_for_statistics_ensemble_nda_filtered.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ENSEMBLE MULTI-UNET CABFL + CABFL PATCHES NO DECODER ATTENTION NO FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f62fd64b0e84c7fa2d2151ade12329c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028c27284a6c4709b3a54d5487b64c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743984c9e1ff486bbae9d5b838a62d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf29ab14571408ca042ee8b8f9519cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6119822a2404f62bae0405d048f4b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a97cb9fa374cce86628511f99f8542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5852d630bb8b496f8016d4c0dbda89e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0eff9ea8c9346aba61f9d3c85224869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450a0fefaea5470abc98340af8e50b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a0933c75e14840817cc235fdbc5090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3a64a5bdcc4db4b7ab6d7c1a4246bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ab663f143a4fcbbc298b586c0fb308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1081371f16334ab9a5b41b45b4402ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc860802d8f410c9a7a0077fbc6e8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0b7a91c1c24406b0fbc61aeb7f1a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5834ad78a646199347a4c4a6549406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030596f60c6f495bbf9a0b5f1c4f3b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3224ab461df74906bd757c2ff818e90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b688471aaf486e83f586a0bc915b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0998431ab9b44a286bfefdeb88fb114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cfb6579a2449b4b65c554ce2699df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.61095583\n",
      "MODEL CLASS STD IOU  0.14788763\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.6396286\n",
      "MODEL CLASS STD DICE  0.17811231\n",
      "\n",
      "MODEL DIOU 0.8842068610301999\n",
      "MODEL DIOU STD  0.26653173263055463\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.7201789617538452\n",
      "MODEL IOU MASS VOLUME STD  0.4128887951374054\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.22376088798046112\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.2957236170768738\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.5088112354278564\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.2319953292608261\n",
      "MODEL DICE MASS VOLUME  0.7405183911323547\n",
      "MODEL DICE MASS VOLUME STD  0.40643733739852905\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.28018370270729065\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.35622110962867737\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.6371114253997803\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.24726735055446625\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.6115471422672272\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.14861223101615906\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.6238586157560349\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.16377970576286316\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.9323506653308868\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.13007614016532898\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.6399248689413071\n",
      "MODEL MEAN F1 NO EMPTY STD 0.17848694324493408\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9981030821800232\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0019284366\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.573447585105896\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.24694735\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.8653095364570618\n",
      "MODEL RECALL EXCLUDING CASES STD  0.25871453\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.22376090288162231\n",
      "MODEL ACCURACY NO EMPTY STD  0.29572362\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.24777576327323914\n",
      "MODEL PRECISION NO EMPTY STD  0.32717454\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.8653095364570618\n",
      "MODEL RECALL NO EMPTY STD  0.25871453\n",
      "\n",
      "MODEL F1 NO EMPTY  0.28018370270729065\n",
      "MODEL F1 NO EMPTY STD  0.3562211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_ensemble_nda_nf = test_dataset_aware_ensemble(\n",
    "                              model_whole_path=\"PRIVATE-FUSION-SUB-CABL-NO-DECODER-ATTENTION-NO-FUSION.ckpt\", \n",
    "                              model_patches_path=\"RESNET-PATCHES-PRIVATE.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              use_simple_fusion=True,\n",
    "                              use_decoder_attention=False,\n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\",\n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_statistics_ensemble_nda_nf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(scores_for_statistics_ensemble_nda_nf, \"scores_for_statistics_ensemble_nda_nf.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ENSEMBLE MULTI-UNET CABFL + CABFL PATCHES -filtered NO DECODER ATTENTION NO FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af58a37d3e304413989d868e7b6af0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering\n",
      "num features: 118\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "fine\n",
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7426f72225d4ae9a2df93b1ef6d9b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_ensemble_nda_nf_filtered = test_dataset_aware_ensemble(\n",
    "                              model_whole_path=\"PRIVATE-FUSION-SUB-CABL-NO-DECODER-ATTENTION-NO-FUSION.ckpt\", \n",
    "                              model_patches_path=\"RESNET-PATCHES-PRIVATE.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              use_simple_fusion=True,\n",
    "                              use_decoder_attention=False,\n",
    "                              filter=True,\n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\",\n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_statistics_ensemble_nda_nf_filtered.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(scores_for_statistics_ensemble_nda_nf_filtered, \"scores_for_statistics_ensemble_nda_nf_filtered.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db57524c4f3c4205bfc595209eac286a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7818a1cd8b0a4092a1a0a49172ab09ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae50cc4946c4324ab31ffc74b686021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529476acea0645f7804c932b65ad0422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0a7554f37845fc9b414791fe7bd69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a23eaa10534e48b9ee6ccd9e9ccb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4059f9b7804f4f18a9a5ac89daa5bcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f60f5a8f422464994fc049abd4d1414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39be6595187c444ebc3829547b7646fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd017e6e6bc486b9234e1797f9a7e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2af690aa723424d9cc12da221d34a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d50f44ad6b4895bb2b27944e62145f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbfcc680d984b1aa1d48639500b2395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0ff8a1fb844616bef6692ab4e2fbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55623419977d420487f6efbc36756c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daeeb59c945479a9101f66e89fb90ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f34d86fbaf4008948a29e61484fcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7015c35a594b378a8539ab8a83b44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d712d87027454facb4856341ee475df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b699ba9180c44ae4909921cb1880ba07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3af37d107ce4c879d7ea639196795d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.6541247\n",
      "MODEL CLASS STD IOU  0.16112955\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.69052994\n",
      "MODEL CLASS STD DICE  0.18732928\n",
      "\n",
      "MODEL DIOU 0.48139805449367845\n",
      "MODEL DIOU STD  0.4263533941207131\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.8684301972389221\n",
      "MODEL IOU MASS VOLUME STD  0.3053703308105469\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.3098789155483246\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.3217676877975464\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.37265825271606445\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.3179851770401001\n",
      "MODEL DICE MASS VOLUME  0.8821564316749573\n",
      "MODEL DICE MASS VOLUME STD  0.2927244305610657\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.3818768262863159\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.37446126341819763\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.45924246311187744\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.36482805013656616\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.654784083366394\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.16150318086147308\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.8328231573104858\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.19472531974315643\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.7097186595201492\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.18066808581352234\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.6905299\n",
      "MODEL MEAN F1 NO EMPTY STD 0.18732926\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9982134699821472\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0026066862\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.8590375781059265\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.16898239\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.41948583722114563\n",
      "MODEL RECALL EXCLUDING CASES STD  0.3611106\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.309878945350647\n",
      "MODEL ACCURACY NO EMPTY STD  0.3217677\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.6659086346626282\n",
      "MODEL PRECISION NO EMPTY STD  0.3882545\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.41948583722114563\n",
      "MODEL RECALL NO EMPTY STD  0.3611106\n",
      "\n",
      "MODEL F1 NO EMPTY  0.3818768262863159\n",
      "MODEL F1 NO EMPTY STD  0.37446126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_for_statistics_resnet = test_dataset_aware_no_patches(model_path=\"RESNET-PRIVATE.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"UNet\", get_scores_for_statistics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['miou', 'mdice', 'mf1'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_statistics_resnet.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_resnet.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_resnet, \"scores_for_statistics_resnet.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RESNET PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bdc317480e436197b46441ea89ecf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a05662b24249d9966001150728d027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec9f3e03e004ff2b8c88be2f0f3d808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d38b99ee1f49a6a871d0cc77242e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f9545548e0433bba2821d5c8a2155f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7641427d4c948b38c19fc7bc6c0982b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47d0da8bc5641ddbdaca3088b21f9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3810271803fe4e2aba2a535bc720cc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06f3931a4b740548ae8964e5055ea3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7cbdb95bcf4fbc8adeb87fdd98d671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b41e9a0bd9340acaad391e8ddb408d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9c44f80d9b400da015af32199067a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4abfe4e72b4b629f313b666ca64863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096700707f0a46338b08008d0ca59322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42051e170236490e8a3d3079c047d543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deba71545dd409d8a2c548c4d2bc4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516787c8a9a44058a803a218ce8abdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129eb2ed0d1949d1a547627a6a88d113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052d78df04434e5cb3ca7b24d6495468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f5cdd2abc0429e9b22b2bbb6397f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77c78f545da46c595c138e06dc2b138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.6039572\n",
      "MODEL CLASS STD IOU  0.1559185\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.62576866\n",
      "MODEL CLASS STD DICE  0.18208908\n",
      "\n",
      "MODEL DIOU 0.8400445631211193\n",
      "MODEL DIOU STD  0.2847975325890597\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.659709095954895\n",
      "MODEL IOU MASS VOLUME STD  0.44199663400650024\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.20883384346961975\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.3120490610599518\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.5665929913520813\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.24796366691589355\n",
      "MODEL DICE MASS VOLUME  0.6782744526863098\n",
      "MODEL DICE MASS VOLUME STD  0.44071510434150696\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.2519976794719696\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.36431097984313965\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.6837019920349121\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.2548254728317261\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.6042191907763481\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.15651041269302368\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.6232757270336151\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.18081198632717133\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.9013874530792236\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.1405234932899475\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.6258998364210129\n",
      "MODEL MEAN F1 NO EMPTY STD 0.1823989897966385\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9985897541046143\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0017009886\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.6782810688018799\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.25631887\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.803074300289154\n",
      "MODEL RECALL EXCLUDING CASES STD  0.28030655\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.20883385837078094\n",
      "MODEL ACCURACY NO EMPTY STD  0.31204906\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.2466476559638977\n",
      "MODEL PRECISION NO EMPTY STD  0.36104304\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.803074300289154\n",
      "MODEL RECALL NO EMPTY STD  0.28030655\n",
      "\n",
      "MODEL F1 NO EMPTY  0.2519976794719696\n",
      "MODEL F1 NO EMPTY STD  0.36431098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = test_dataset_aware_patches(model_path=\"RESNET-PATCHES-PRIVATE.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"patches_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"UNet\", get_scores_for_statistics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### UNETPLUSPLUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from monai.networks.nets import UNet, BasicUNetPlusPlus,SwinUNETR\n",
    "scores_for_statistics_unetplusplus = test_dataset_aware_no_patches(model_path=\"unetplusplus_model_final.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"unetplusplus\", get_scores_for_statistics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['miou', 'mdice', 'mf1'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_statistics_unetplusplus.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_unetplusplus.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_unetplusplus, \"scores_for_statistics_unetplusplus.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao\n"
     ]
    }
   ],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Skinny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7581396f0948496d8931138dc66c9eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5793640aa6a74d0d9382a221874dd623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427d9d1d7d664684b4a09dfade03472b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3664a0677aba43728b82079e8f5c38a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e7ff606c1a4fc8b047d7e660234f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6479600e9545e7ab0b14882703aea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c3dc84a67e4e59a37bf2d1aaccb38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2799349e28214f0faed7062f0211fb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3820962a55ca40fb98ceba41034d246e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f29195fe844f18a54db019a3c6a1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed2325c072e43e2943a2845bd4c6a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1458803a81e94a2f9a339cb830c8322a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53925c114ee4dd191be49136f28dce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fb9f39893949a3bdb8a08efae7f865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202be9dddffb4833b0259c26a59233cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc46dd64e48a4ca88def6c1a31ba82d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bcf5fcfea7405c96465e2ec9c3c7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bdf66fdc7e4dfcbbabb8366178ae6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7fcb5ba5dd46de82f80e2b017ed841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3c2c36a09e4256ad40cf368ff09e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ccd4c7881e45e29cb9ef14d1c86325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.590411\n",
      "MODEL CLASS STD IOU  0.1497511\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.6091358\n",
      "MODEL CLASS STD DICE  0.1755867\n",
      "\n",
      "MODEL DIOU 0.7936827036693562\n",
      "MODEL DIOU STD  0.3156589441909492\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.5905758738517761\n",
      "MODEL IOU MASS VOLUME STD  0.46091538667678833\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.1815723031759262\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.2996978461742401\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.5729680061340332\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.243257537484169\n",
      "MODEL DICE MASS VOLUME  0.6091229319572449\n",
      "MODEL DICE MASS VOLUME STD  0.4629926085472107\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.218647301197052\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.35129040479660034\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.6899616122245789\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.2534303367137909\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.590598426759243\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.15027032792568207\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.6124422252178192\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.1816750019788742\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.8732391595840454\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.13350507616996765\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.60913587\n",
      "MODEL MEAN F1 NO EMPTY STD 0.1755867\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9987809658050537\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0015120122\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.7237809300422668\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.2493639\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.7467286586761475\n",
      "MODEL RECALL EXCLUDING CASES STD  0.26651314\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.1815723031759262\n",
      "MODEL ACCURACY NO EMPTY STD  0.29969788\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.22500962018966675\n",
      "MODEL PRECISION NO EMPTY STD  0.36271152\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.7467286586761475\n",
      "MODEL RECALL NO EMPTY STD  0.26651314\n",
      "\n",
      "MODEL F1 NO EMPTY  0.218647301197052\n",
      "MODEL F1 NO EMPTY STD  0.3512904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "scores_for_statistics_skinny = test_dataset_aware_no_patches(model_path=\"skinny_model_private.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"skinny\", get_scores_for_statistics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['miou', 'mdice', 'mf1'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_statistics_skinny.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_skinny.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_skinny, \"scores_for_statistics_skinny.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FNC-FFNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "scores_for_statistics_fcn= test_dataset_aware_no_patches(model_path=\"fcn_ffnet_model_final.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"fcn_ffnet\", get_scores_for_statistics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(scores_for_statistics_fcn, \"scores_for_statistics_fcn.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SEGNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4732d3c959444f6c9cb654b7b3f3e987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf6139ee4dd48b49a574eaa91125fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3432765c19c942e6b907733c12e52840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e63d357c02a4ddeb71dd47c6b792b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70275c710bce4dfa814013ea136b18b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d42078ebd144f9c9fcfc6ef44b57afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9b12f2a0b94694bd91af302f40d15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f5bf9eae5d437e9b7f50d9d11f2ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46c7b6c556744ad9d5b1969ac57c5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3415177aaa6407d9576e6709f45ad01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4892dfb573da483fbc5756dcc8f8e952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dad8b76de64c349d0df12e8e08e6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40f403e2f394c539d74cee10efb31a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6336de5f7e34ac68d79a86833ce8cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a0e77c3c504a738a22c2604d296d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad7ba5dc64b4a8fac6ac22c1468983c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edf40f528fd4b52afb2bdf73b83be2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039ecd62ba6847369da47d8bd60b6068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93a39a28c1e4a358b410e8826a4b32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa4c7b8a51441b7865c551a7ef2764d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300036b0ba784e4884445015136ad914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.584107\n",
      "MODEL CLASS STD IOU  0.14451733\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.6023499\n",
      "MODEL CLASS STD DICE  0.17040415\n",
      "\n",
      "MODEL DIOU 0.7599096539629479\n",
      "MODEL DIOU STD  0.329311818890917\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.5851113200187683\n",
      "MODEL IOU MASS VOLUME STD  0.4631311297416687\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.16894152760505676\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.2892600893974304\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.5320152044296265\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.2651973366737366\n",
      "MODEL DICE MASS VOLUME  0.6031447649002075\n",
      "MODEL DICE MASS VOLUME STD  0.46477100253105164\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.20506411790847778\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.3409436345100403\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.6457691788673401\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.28542381525039673\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.5842891782522202\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.14511148631572723\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.6128284186124802\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.18482357263565063\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.8397898375988007\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.15069043636322021\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.60234994\n",
      "MODEL MEAN F1 NO EMPTY STD 0.17040415\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9985342621803284\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0019223948\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.7207393646240234\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.2783935\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.6797855496406555\n",
      "MODEL RECALL EXCLUDING CASES STD  0.30093428\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.16894152760505676\n",
      "MODEL ACCURACY NO EMPTY STD  0.2892601\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.22581425309181213\n",
      "MODEL PRECISION NO EMPTY STD  0.36884078\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.6797855496406555\n",
      "MODEL RECALL NO EMPTY STD  0.30093428\n",
      "\n",
      "MODEL F1 NO EMPTY  0.20506411790847778\n",
      "MODEL F1 NO EMPTY STD  0.34094363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from monai.networks.nets import UNet, BasicUNetPlusPlus,SwinUNETR\n",
    "scores_for_statistics_segnet= test_dataset_aware_no_patches(model_path=\"segnet_model_private.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"segnet\", get_scores_for_statistics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['miou', 'mdice', 'mf1'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_statistics_segnet.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_segnet_private.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_segnet, \"scores_for_statistics_segnet_private.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao\n"
     ]
    }
   ],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SWIN-UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 't_loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['t_loss'])`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec95e61e09e64991aa4113d2b559de70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54774d0c5193459d8b24460a9bc617c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4cbb423f994b36abe536e7ce92bbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c19f865c3d4b5e9b70d5029a50d61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b11e46855a43a4b47e9dd0afc2b06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726a553a747a43019120987ee8a42a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79547ebbd9a4ee094c8bd991da11643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38114214bc24ca2b46be66d80a484e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f50b38055442ca8437b61b8493b0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbac0ba36784d958196f48d438e85d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51c47b1599c4930a7ce9f169a5a5ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9cc7de3c324a658eaa2b9d1f39542a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78f27dad61e4670aca28facb6962b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dff3a2b9eab4d9c8cfe2f29c49858e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2492751eb72429f92cb472f2ce0fd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8854912217954d6c9def3c1e398069d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cee624331844a1880982bb935fdb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68265c1d0c06474287b7e60571d81c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79aafa356d548ee82cf632429ae911f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62252a56ce3f450eaa9ec1df4ea8dd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb03a878dc254db1ac1e50af36d05fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS MEAN IOU  0.6107712\n",
      "MODEL CLASS STD IOU  0.16355146\n",
      "\n",
      "MODEL CLASS MEAN DICE  0.6315172\n",
      "MODEL CLASS STD DICE  0.18841812\n",
      "\n",
      "MODEL DIOU 0.8711695413621241\n",
      "MODEL DIOU STD  0.26683839405871324\n",
      "\n",
      "MODEL IOU MASS VOLUME  0.6665454506874084\n",
      "MODEL IOU MASS VOLUME STD  0.4405349791049957\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY  0.22240541875362396\n",
      "MODEL IOU MASS VOLUME NO EMPTY STD  0.3272370994091034\n",
      "\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC  0.6016120314598083\n",
      "MODEL IOU MASS VOLUME NO EMPTY OPTIMISTIC STD  0.2480519711971283\n",
      "MODEL DICE MASS VOLUME  0.6841536164283752\n",
      "MODEL DICE MASS VOLUME STD  0.44022512435913086\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY  0.2634667456150055\n",
      "MODEL DICE MASS VOLUME NO EMPTY STD  0.37692826986312866\n",
      "\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC  0.712683916091919\n",
      "MODEL DICE MASS VOLUME NO EMPTY OPTIMISTIC STD  0.25331050157546997\n",
      "\n",
      "MODEL MEAN ACCURACY NO EMPTY 0.6110176518559456\n",
      "MODEL MEAN ACCURACY NO EMPTY STD 0.16412875056266785\n",
      "\n",
      "MODEL MEAN PRECISION NO EMPTY 0.6345874518156052\n",
      "MODEL MEAN PRECISION NO EMPTY STD 0.19190876185894012\n",
      "\n",
      "MODEL MEAN RECALL NO EMPTY 0.8893803358078003\n",
      "MODEL MEAN RECALL NO EMPTY STD 0.13539482653141022\n",
      "\n",
      "MODEL MEAN F1 NO EMPTY 0.6315173\n",
      "MODEL MEAN F1 NO EMPTY STD 0.18841812\n",
      "\n",
      "MODEL ACCURACY EXCLUDING CASES  0.9986966252326965\n",
      "MODEL ACCURACY EXCLUDING CASES STD  0.0016184495\n",
      "\n",
      "MODEL PRECISION EXCLUDING CASES  0.7398352026939392\n",
      "MODEL PRECISION EXCLUDING CASES STD  0.23525718\n",
      "\n",
      "MODEL RECALL EXCLUDING CASES  0.7790308594703674\n",
      "MODEL RECALL EXCLUDING CASES STD  0.2699875\n",
      "\n",
      "MODEL ACCURACY NO EMPTY  0.22240544855594635\n",
      "MODEL ACCURACY NO EMPTY STD  0.32723707\n",
      "\n",
      "MODEL PRECISION NO EMPTY 0.2692749798297882\n",
      "MODEL PRECISION NO EMPTY STD  0.38321552\n",
      "\n",
      "MODEL RECALL NO EMPTY  0.7790308594703674\n",
      "MODEL RECALL NO EMPTY STD  0.2699875\n",
      "\n",
      "MODEL F1 NO EMPTY  0.2634667456150055\n",
      "MODEL F1 NO EMPTY STD  0.37692827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "scores_for_statistics_swin= test_dataset_aware_no_patches(model_path=\"swin_model_final.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"swin_unetr\", get_scores_for_statistics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['miou', 'mdice', 'mf1'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_for_statistics_swin.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_statistics_swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary successfully saved to scores_for_statistics_swin.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(scores_for_statistics_swin, \"scores_for_statistics_swin.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao\n"
     ]
    }
   ],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TEST PATIENT AWARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST MULTI-UNET SUB CABFL LARGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_scores_for_statistics_fusion_mid = test_patient_aware_fusion(model_path=\"FUSION-SUB-CABL-FINAL.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", subtracted=True, strict=False, get_scores_for_statistics=True, base_channels=64\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(patient_scores_for_statistics_fusion_mid, \"patient_scores_for_statistics_fusion_mid.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_scores_for_statistics_fusion_tiny = test_patient_aware_fusion(model_path=\"FUSION-SUB-CABL-tiny.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", subtracted=True, strict=False, get_scores_for_statistics=True, base_channels=16\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(patient_scores_for_statistics_fusion_tiny, \"patient_scores_for_statistics_fusion_tiny.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENSEMBLE MULTI-UNET CABFL + CABFL PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_scores_for_statistics_ensemble = test_patient_aware_ensemble(\n",
    "                              model_whole_path=\"FUSION-SUB-CABL-tiny.ckpt\", \n",
    "                              model_patches_path=\"PATCHES-SUB-CABL-unetplusplus.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", base_channels=16,\n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", subtracted=True, get_scores_for_statistics=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(patient_scores_for_statistics_ensemble, \"patient_scores_for_statistics_ensemble.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENSEMBLE MULTI-UNET CABFL + CABFL PATCHES -filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_scores_for_statistics_ensemble_filtered = test_patient_aware_ensemble(\n",
    "                              model_whole_path=\"FUSION-SUB-CABL-tiny.ckpt\", \n",
    "                              model_patches_path=\"PATCHES-SUB-CABL-unetplusplus.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, base_channels=16,\n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              patches_dataset_key=\"patches_sub_test_ds\", subtracted=True, get_scores_for_statistics=True, filter=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(patient_scores_for_statistics_ensemble_filtered, \"patient_scores_for_statistics_ensemble_filtered.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNETPLUSPLUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "patient_scores_for_statistics_unetplusplus = test_patient_aware_no_patches(model_path=\"unetplusplus_model.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"unetplusplus\", #get_scores_for_statistics=True,\n",
    "                              subtracted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(patient_scores_for_statistics_unetplusplus, \"patient_scores_for_statistics_unetplusplus.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skinny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "patient_scores_for_statistics_skinny = test_patient_aware_no_patches(model_path=\"skinny_model.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"skinny\", #get_scores_for_statistics=True,\n",
    "                              subtracted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_to_json(patient_scores_for_statistics_skinny, \"patient_scores_for_statistics_skinny.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNC-FFNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "patient_scores_for_statistics_fcn= test_patient_aware_no_patches(model_path=\"fcn_ffnet_model.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"fcn_ffnet\", #get_scores_for_statistics=True,\n",
    "                              subtracted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_to_json(patient_scores_for_statistics_fcn, \"patient_scores_for_statistics_fcn.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEGNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from monai.networks.nets import UNet, BasicUNetPlusPlus,SwinUNETR\n",
    "patient_scores_for_statistics_segnet= test_patient_aware_no_patches(model_path=\"segnet_model_large.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"segnet\", #get_scores_for_statistics=True,\n",
    "                              subtracted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_to_json(patient_scores_for_statistics_segnet, \"patient_scores_for_statistics_segnet_large.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWIN-UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "scores_for_statistics_swin= test_patient_aware_no_patches(model_path=\"swin_model.ckpt\", \n",
    "                              patient_ids=x_test, \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              filter=False, \n",
    "                              arch_name=\"swin_unetr\", #get_scores_for_statistics=True,\n",
    "                              subtracted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# VIZ for Back analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_bbox(mask, desired_size=(150, 150)):\n",
    "    \"\"\"Calculate a fixed-size bounding box centered around the mask's non-zero region or return a bounding box equal to the image size if the mask is empty.\"\"\"\n",
    "    # Check if the mask is empty\n",
    "    if not np.any(mask):\n",
    "        # Return a bounding box equal to the image size\n",
    "        return 0, 0, mask.shape[1], mask.shape[0]\n",
    "\n",
    "    # Continue with original logic if the mask is not empty\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    ymin, ymax = np.where(rows)[0][[0, -1]]\n",
    "    xmin, xmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Calculate center of original bounding box\n",
    "    center_x = xmin + (xmax - xmin) // 2\n",
    "    center_y = ymin + (ymax - ymin) // 2\n",
    "\n",
    "    # Determine new bounding box dimensions\n",
    "    desired_width, desired_height = desired_size\n",
    "\n",
    "    # Calculate new bounding box coordinates, centered around the original bounding box\n",
    "    new_xmin = max(center_x - desired_width // 2, 0)\n",
    "    new_xmax = new_xmin + desired_width\n",
    "    new_ymin = max(center_y - desired_height // 2, 0)\n",
    "    new_ymax = new_ymin + desired_height\n",
    "\n",
    "    # Adjust if the new bounding box exceeds image boundaries\n",
    "    if new_xmax > mask.shape[1]:\n",
    "        new_xmax = mask.shape[1]\n",
    "        new_xmin = new_xmax - desired_width\n",
    "    if new_ymax > mask.shape[0]:\n",
    "        new_ymax = mask.shape[0]\n",
    "        new_ymin = new_ymax - desired_height\n",
    "\n",
    "    return new_xmin, new_ymin, new_xmax, new_ymax\n",
    "\n",
    "\n",
    "def crop_to_bbox(img, bbox):\n",
    "    \"\"\"Crop the image to the specified bounding box.\"\"\"\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    return img[ymin:ymax+1, xmin:xmax+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_2_models(iter1, iter2, model1, model2, device, prefix_fname=None, crop_to_size=None, extension='svg', patches=False):\n",
    "    model1 = model1.to(device).eval()\n",
    "    model2 = model2.to(device).eval()\n",
    "\n",
    "    data_iter1 = next(iter1)\n",
    "    data_iter2 = next(iter2)\n",
    "\n",
    "    if patches:\n",
    "\n",
    "        images1 = data_iter1[\"image\"]\n",
    "        images2 =data_iter2[\"image\"]\n",
    "        label1 =data_iter1[\"label\"]\n",
    "        label2 = data_iter2[\"label\"]\n",
    "\n",
    "    else:\n",
    "        images1 = data_iter1[\"image\"]\n",
    "        images2 =data_iter2[\"image\"]\n",
    "        label1 =data_iter1[\"label\"]\n",
    "        label2 = data_iter2[\"label\"]\n",
    "        \n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Perform inference with both models\n",
    "        logits1 = model1(images1.to(device))[0]\n",
    "        logits2 = model2(images2.to(device))[0]\n",
    "    \n",
    "        # Apply sigmoid to the logits to get the probabilities\n",
    "        pr_masks1 = logits1.sigmoid()\n",
    "        pr_masks2 = logits2.sigmoid()\n",
    "\n",
    "    saved_counter = 0\n",
    "    \n",
    "    for image1,image2, gt_mask1,gt_mask2, pr_mask1, pr_mask2 in zip(images1,images2, label1,label2, pr_masks1, pr_masks2):\n",
    "        # Threshold the probabilities to get binary masks\n",
    "        pr_mask1 = (pr_mask1.cpu().numpy().squeeze() > 0.8).astype(int)\n",
    "        pr_mask2 = (pr_mask2.cpu().numpy().squeeze() > 0.8).astype(int)\n",
    "    \n",
    "        gt_mask1 = gt_mask1.squeeze()\n",
    "        gt_mask2 = gt_mask2.squeeze()\n",
    "\n",
    "        if crop_to_size:\n",
    "            # Calculate bounding boxes for the ground truth masks\n",
    "            bbox1 = calculate_bbox(gt_mask1, crop_to_size)\n",
    "            bbox2 = calculate_bbox(gt_mask2,crop_to_size)\n",
    "        \n",
    "            # Crop images, predictions, and ground truths to the bounding box\n",
    "            image1 = crop_to_bbox(image1.cpu().numpy().transpose(1, 2, 0), bbox1)\n",
    "            image2 = crop_to_bbox(image2.cpu().numpy().transpose(1, 2, 0), bbox2)\n",
    "            pr_mask1 = crop_to_bbox(pr_mask1, bbox1)\n",
    "            pr_mask2 = crop_to_bbox(pr_mask2, bbox2)\n",
    "            gt_mask1 = crop_to_bbox(gt_mask1, bbox1)\n",
    "            gt_mask2 = crop_to_bbox(gt_mask2, bbox2)\n",
    "\n",
    "        else:\n",
    "            image1 = image1.cpu().numpy().transpose(1, 2, 0)\n",
    "            image2 = image2.cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        plt.figure(figsize=(18, 10))\n",
    "        # Original Image 1\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.imshow(image1, cmap='gray')\n",
    "        plt.title(\"Post Contrast Image\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Prediction from Model 1\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.imshow(pr_mask1, cmap='gray')\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Ground Truth 1\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.imshow(gt_mask1, cmap='gray')\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Original Image 2\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.imshow(image2, cmap='gray')\n",
    "        plt.title(\"Post Contrast Image (Subtracted)\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Prediction from Model 2\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.imshow(pr_mask2, cmap='gray')\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Ground Truth 2\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.imshow(gt_mask2, cmap='gray')\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "\n",
    "        if prefix_fname:\n",
    "            if saved_counter<30:\n",
    "                # Save the plot to a file\n",
    "                plt.savefig(f'{prefix_fname}_{saved_counter}.{extension}', dpi=300)  # Adjust the filename and DPI as needed\n",
    "                saved_counter+=1\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_2_models_fusion(iter1, iter2, model1, model2, device, prefix_fname=None, crop_to_size=None, extension='svg'):\n",
    "    model1 = model1.to(device).eval()\n",
    "    model2 = model2.to(device).eval()\n",
    "\n",
    "    data_iter1 = next(iter1)\n",
    "    data_iter2 = next(iter2)\n",
    "\n",
    "    images11 = data_iter1[0][\"image\"]\n",
    "    images12 = data_iter1[1][\"image\"]\n",
    "    images13 = data_iter1[2][\"image\"]\n",
    "        \n",
    "    images21 =data_iter2[0][\"image\"]\n",
    "    images22 =data_iter2[1][\"image\"]\n",
    "    images23 =data_iter2[2][\"image\"]\n",
    "        \n",
    "    label1 =data_iter1[0][\"label\"]\n",
    "    label2 = data_iter2[0][\"label\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Perform inference with both models\n",
    "        logits1 = model1(images11.to(device),images12.to(device),images13.to(device))\n",
    "        logits2 = model2(images21.to(device),images22.to(device),images23.to(device))\n",
    "    \n",
    "        # Apply sigmoid to the logits to get the probabilities\n",
    "        pr_masks1 = logits1.sigmoid()\n",
    "        pr_masks2 = logits2.sigmoid()\n",
    "\n",
    "    saved_counter = 0\n",
    "    \n",
    "    for image1,image2, gt_mask1,gt_mask2, pr_mask1, pr_mask2 in zip(images11,images12, label1,label2, pr_masks1, pr_masks2):\n",
    "        # Threshold the probabilities to get binary masks\n",
    "        pr_mask1 = (pr_mask1.cpu().numpy().squeeze() > 0.8).astype(int)\n",
    "        pr_mask2 = (pr_mask2.cpu().numpy().squeeze() > 0.8).astype(int)\n",
    "    \n",
    "        gt_mask1 = gt_mask1.squeeze()\n",
    "        gt_mask2 = gt_mask2.squeeze()\n",
    "\n",
    "        if crop_to_size:\n",
    "            # Calculate bounding boxes for the ground truth masks\n",
    "            bbox1 = calculate_bbox(gt_mask1, crop_to_size)\n",
    "            bbox2 = calculate_bbox(gt_mask2,crop_to_size)\n",
    "        \n",
    "            # Crop images, predictions, and ground truths to the bounding box\n",
    "            image1 = crop_to_bbox(image1.cpu().numpy().transpose(1, 2, 0), bbox1)\n",
    "            image2 = crop_to_bbox(image2.cpu().numpy().transpose(1, 2, 0), bbox2)\n",
    "            pr_mask1 = crop_to_bbox(pr_mask1, bbox1)\n",
    "            pr_mask2 = crop_to_bbox(pr_mask2, bbox2)\n",
    "            gt_mask1 = crop_to_bbox(gt_mask1, bbox1)\n",
    "            gt_mask2 = crop_to_bbox(gt_mask2, bbox2)\n",
    "\n",
    "        else:\n",
    "            image1 = image1.cpu().numpy().transpose(1, 2, 0)\n",
    "            image2 = image2.cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        plt.figure(figsize=(18, 10))\n",
    "        # Original Image 1\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.imshow(image1, cmap='gray')\n",
    "        plt.title(\"Post Contrast Image (Subtracted)\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Prediction from Model 1\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.imshow(pr_mask1, cmap='gray')\n",
    "        plt.title(\"Prediction AUFL\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Prediction from Model 2\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.imshow(pr_mask2, cmap='gray')\n",
    "        plt.title(\"Prediction CABFL\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        \n",
    "        # Ground Truth 1\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.imshow(gt_mask1, cmap='gray')\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        \n",
    "        plt.tight_layout()\n",
    "\n",
    "        if prefix_fname:\n",
    "            if saved_counter<30:\n",
    "                # Save the plot to a file\n",
    "                plt.savefig(f'{prefix_fname}_{saved_counter}.{extension}', dpi=300)  # Adjust the filename and DPI as needed\n",
    "                saved_counter+=1\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_base(model_path, patient_ids, datasets, dataset_key, strict=False, arch_name=None,filter=False):\n",
    "\n",
    "    if arch_name:\n",
    "        model = BreastModel2.load_from_checkpoint(model_path, strict=strict, arch=arch_name)\n",
    "\n",
    "    else:\n",
    "\n",
    "        model = BreastModel2.load_from_checkpoint(model_path, strict=strict)\n",
    "\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        \n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "         image_slices_sub=[]\n",
    "        \n",
    "         print(patient_id)\n",
    "         dataset = datasets[patient_id][dataset_key]\n",
    "        \n",
    "         for idx, e in tqdm(enumerate(dataset), total = len(dataset)):\n",
    "            im_path_key_sub = 'subtracted_filename_or_obj' \n",
    "            im_path_key = 'filename_or_obj'\n",
    "            original_image = np.load(e['image_meta_dict'][im_path_key])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "\n",
    "            original_image_sub = np.load(e['image_meta_dict'][im_path_key_sub])\n",
    "            original_image_sub = np.expand_dims(original_image_sub,0)\n",
    "\n",
    "            gt_label = np.load(e['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "\n",
    "            if e['keep_sample']:\n",
    "                image = torch.unsqueeze(e['image'], 0)\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    model = model.to(\"cuda\")\n",
    "                    model.eval()\n",
    "                    if arch_name:\n",
    "                        masks = model(image.to(\"cuda\"))[0]\n",
    "                    else:\n",
    "                        masks = model(image.to(\"cuda\"))[0]\n",
    "                    masks = masks.sigmoid()\n",
    "                    \n",
    "                pred_label = masks[0]\n",
    "                pred_label = (pred_label > 0.4).int()\n",
    "                pred_label = torch.squeeze(pred_label)\n",
    "                pred_label = torch.unsqueeze(pred_label, 0)\n",
    "                pred_label = reverse_transformations(dataset[idx], pred_label, mode='whole')\n",
    "                pred_label = monai.transforms.Resize(spatial_size=(original_image.shape[1], original_image.shape[2]), mode='nearest-exact')(pred_label)\n",
    "            else:\n",
    "                pred_label = torch.zeros(original_image.shape, dtype=torch.uint8)\n",
    "    \n",
    "    \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "            image_slices_sub.append(original_image.squeeze())\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    "                    \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "         images_volume_sub = np.stack(image_slices_sub, axis=-1)\n",
    "        \n",
    "    \n",
    "         if filter:\n",
    "             predicted_label_volume = filter_masses(predicted_label_volume, min_slices=3, window_size=3) # H x W x N\n",
    "\n",
    "         dice_mass_volume = compute_dice_score_npy(gt_label_volume, predicted_label_volume,reduction='none', class_id=1, exclude_empty=True)\n",
    "         results[patient_id] = {\n",
    "            'images_volume': images_volume,\n",
    "            'images_volume_sub': images_volume_sub,\n",
    "            'gt_volume': gt_label_volume,\n",
    "            'predicted_volume': predicted_label_volume,\n",
    "             'dice_mass_scores':dice_mass_volume\n",
    "        }\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def inference_patches(model_path, patient_ids, datasets, dataset_key, arch_name=None, strict=False, subtracted=False):\n",
    "\n",
    "    if arch_name:\n",
    "        model = BreastModel2.load_from_checkpoint(model_path, strict=strict, arch=arch_name)\n",
    "\n",
    "    else:\n",
    "        model = BreastModel2.load_from_checkpoint(model_path, strict=strict)\n",
    "    \n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "    \n",
    "         print(patient_id)\n",
    "         dataset = datasets[patient_id][dataset_key]\n",
    "        \n",
    "         for idx, e in tqdm(enumerate(dataset), total = len(dataset)):\n",
    "            im_path_key = 'subtracted_filename_or_obj' if subtracted else 'filename_or_obj'\n",
    "            original_image = np.load(e[0]['image_meta_dict'][im_path_key])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "\n",
    "            pred_label = torch.zeros(original_image.shape, dtype=torch.uint8)\n",
    "            \n",
    "            gt_label = np.load(e[0]['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "\n",
    "\n",
    "            merged_label = torch.zeros(original_image.shape)\n",
    "            merged_label_for_fusion = torch.zeros(original_image.shape)\n",
    "\n",
    "            for elem in e:\n",
    "                if elem['keep_sample']:\n",
    "                    image = torch.unsqueeze(elem['image'], 0)\n",
    "                    with torch.no_grad():\n",
    "                        model = model.to(\"cuda\")\n",
    "                        model.eval()\n",
    "                        if arch_name:\n",
    "                            logits = model(image.to(\"cuda\"))\n",
    "                        else:\n",
    "                            logits = model(image.to(\"cuda\"))[0]\n",
    "    \n",
    "                    pr_mask = logits.sigmoid()\n",
    "                    pr_mask = pr_mask[0]\n",
    "                    #pr_mask_to_viz = (pr_mask.cpu().numpy() > 0.4).astype(int)\n",
    "    \n",
    "                    if pr_mask.sum()>0:\n",
    "                        pr_mask = (pr_mask > 0.4).int()\n",
    "                        label = reverse_transformations(elem, pr_mask, mode='patches')\n",
    "                        merged_label += label\n",
    "                    \n",
    "\n",
    "                pred_label = merged_label\n",
    "                pred_label = monai.transforms.Resize(spatial_size=(original_image.shape[1], original_image.shape[2]), mode='nearest-exact')(pred_label)\n",
    "\n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    "       \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "         dice_mass_volume = compute_dice_score_npy(gt_label_volume, predicted_label_volume,reduction='none', class_id=1, exclude_empty=True)\n",
    "         results[patient_id] = {\n",
    "            'images_volume': images_volume,\n",
    "            'gt_volume': gt_label_volume,\n",
    "            'predicted_volume': predicted_label_volume,\n",
    "             'dice_mass_scores':dice_mass_volume\n",
    "         }\n",
    "\n",
    "        \n",
    "\n",
    "    return results\n",
    "\n",
    "def inference_fusion(model_path, patient_ids, datasets, whole_dataset_key, patches_dataset_key , subtracted=False,filter = False):\n",
    "\n",
    "    model = BreastModel.load_from_checkpoint(model_path, strict=False)\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "\n",
    "         predicted_label_slices = []\n",
    "         gt_label_slices=[]\n",
    "         image_slices=[]\n",
    "         image_slices_sub=[]\n",
    "    \n",
    "         print(patient_id)\n",
    "         patches_ds = datasets[patient_id][patches_dataset_key]\n",
    "         whole_image_ds = datasets[patient_id][whole_dataset_key]\n",
    "\n",
    "         fusion_dataset = PairedDataset(whole_image_ds, patches_ds, augment=False)\n",
    "\n",
    "         for idx, e in tqdm(enumerate(fusion_dataset), total = len(patches_ds)):\n",
    "            im_path_key_sub = 'subtracted_filename_or_obj' \n",
    "            im_path_key = 'filename_or_obj'\n",
    "            original_image = np.load(e['image_meta_dict'][im_path_key])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "\n",
    "            original_image_sub = np.load(e['image_meta_dict'][im_path_key_sub])\n",
    "            original_image_sub = np.expand_dims(original_image_sub,0)\n",
    "\n",
    "    \n",
    "            \n",
    "            pred_label = torch.zeros(original_image.shape, dtype=torch.uint8)\n",
    "    \n",
    "            gt_label = np.load(e[0]['label_meta_dict']['filename_or_obj'])\n",
    "            gt_label= np.expand_dims(gt_label,0)\n",
    "\n",
    "            if fusion_dataset[idx][0]['keep_sample']:\n",
    "    \n",
    "                whole_image = torch.unsqueeze(fusion_dataset[idx][0]['image'], 0)\n",
    "                patch_image2 = torch.unsqueeze(fusion_dataset[idx][1]['image'], 0)\n",
    "                patch_image3 = torch.unsqueeze(fusion_dataset[idx][2]['image'], 0)\n",
    "                    \n",
    "        \n",
    "                with torch.no_grad():\n",
    "                    masks = []\n",
    "                    # pass to model\n",
    "                    model = model.to(\"cuda\")\n",
    "                    model.eval()\n",
    "                    \n",
    "                    masks = model(whole_image.to(\"cuda\"),patch_image2.to(\"cuda\"),patch_image3.to(\"cuda\"))\n",
    "                    masks = masks.sigmoid()\n",
    "                    \n",
    "    \n",
    "                pred_label = masks[0]\n",
    "                pred_label = (pred_label > 0.4).int()\n",
    "                pred_label = reverse_transformations(fusion_dataset[idx][0], pred_label, mode='whole')\n",
    "                \n",
    "\n",
    "                \n",
    "            pred_label = monai.transforms.Resize(spatial_size=(original_image.shape[1], original_image.shape[2]), mode='nearest-exact')(pred_label)\n",
    "            \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            gt_label_slices.append(gt_label.squeeze())\n",
    "            images_volume = np.stack(image_slices, axis=-1)\n",
    "            images_volume_sub = np.stack(image_slices_sub, axis=-1)\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    "                    \n",
    "         gt_label_volume = np.stack(gt_label_slices, axis=-1)\n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "    \n",
    "         if filter:\n",
    "             predicted_label_volume = filter_masses(predicted_label_volume, min_slices=3, window_size=3) # H x W x N \n",
    "         \n",
    "         dice_mass_volume = compute_dice_score_npy(gt_label_volume, predicted_label_volume,reduction='none', class_id=1, exclude_empty=True)\n",
    "         results[patient_id] = {\n",
    "            'images_volume': images_volume,\n",
    "             'images_volume_sub': images_volume_sub,\n",
    "            'gt_volume': gt_label_volume,\n",
    "            'predicted_volume': predicted_label_volume,\n",
    "             'dice_mass_scores':dice_mass_volume\n",
    "         }\n",
    "\n",
    "    return results\n",
    "\n",
    "def inference_ensemble(model_whole_path, model_patches_path, patient_ids, datasets, whole_dataset_key, patches_dataset_key,filter = False):\n",
    "\n",
    "    model_whole = BreastModel.load_from_checkpoint(model_whole_path, strict=False)\n",
    "    model_patches = BreastModel2.load_from_checkpoint(model_patches_path, strict=False)\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "         predicted_label_slices = []         \n",
    "         image_slices=[]\n",
    "         image_slices_sub=[]\n",
    "\n",
    "         print(patient_id)\n",
    "         patches_ds = datasets[patient_id][patches_dataset_key]\n",
    "         whole_image_ds = datasets[patient_id][whole_dataset_key]\n",
    "\n",
    "         fusion_dataset = PairedDataset(whole_image_ds, patches_ds, augment=False)\n",
    "        \n",
    "         prev_had_mask=False\n",
    "\n",
    "         for idx, e in tqdm(enumerate(patches_ds), total = len(patches_ds)):\n",
    "            im_path_key_sub = 'subtracted_filename_or_obj' \n",
    "            im_path_key = 'filename_or_obj'\n",
    "            original_image = np.load(e['image_meta_dict'][im_path_key])\n",
    "            original_image = np.expand_dims(original_image,0)\n",
    "\n",
    "            original_image_sub = np.load(e['image_meta_dict'][im_path_key_sub])\n",
    "            original_image_sub = np.expand_dims(original_image_sub,0)\n",
    "\n",
    "    \n",
    "            merged_label_for_fusion = torch.zeros(original_image.shape)\n",
    "            \n",
    "            ## FIRST MODEL\n",
    "            for elem in e:\n",
    "                if elem['keep_sample']:\n",
    "                    image = torch.unsqueeze(elem['image'], 0)\n",
    "                    with torch.no_grad():\n",
    "                        model_patches = model_patches.to(\"cuda\")\n",
    "                        model_patches.eval()\n",
    "                        logits = model_patches(image.to(\"cuda\"))[0]\n",
    "    \n",
    "                    pr_mask = logits.sigmoid()\n",
    "                    pr_mask = pr_mask[0]\n",
    "                    #pr_mask_to_viz = (pr_mask.cpu().numpy() > 0.4).astype(int)\n",
    "    \n",
    "                    if pr_mask.sum()>0:\n",
    "                        #label = pr_mask\n",
    "                        label = reverse_transformations(elem, pr_mask, mode='patches')\n",
    "                        merged_label_for_fusion += label\n",
    "    \n",
    "    \n",
    "            original_image = np.transpose(original_image, (1,2,0))\n",
    "    \n",
    "            label_patches_for_fusion = merged_label_for_fusion[0]\n",
    "    \n",
    "            # SECOND MODEL\n",
    "\n",
    "            if fusion_dataset[idx][0]['keep_sample'] or fusion_dataset[idx][1]['keep_sample'] or fusion_dataset[idx][2]['keep_sample']:\n",
    "    \n",
    "                whole_image = torch.unsqueeze(fusion_dataset[idx][0]['image'], 0)\n",
    "                patch_image2 = torch.unsqueeze(fusion_dataset[idx][1]['image'], 0)\n",
    "                patch_image3 = torch.unsqueeze(fusion_dataset[idx][2]['image'], 0)\n",
    "                    \n",
    "        \n",
    "                with torch.no_grad():\n",
    "                    masks = []\n",
    "                    # pass to model\n",
    "                    model_whole = model_whole.to(\"cuda\")\n",
    "                    model_whole.eval()\n",
    "                    \n",
    "                    masks = model_whole(whole_image.to(\"cuda\"),patch_image2.to(\"cuda\"),patch_image3.to(\"cuda\"))\n",
    "                    masks = masks.sigmoid()\n",
    "                    \n",
    "    \n",
    "                label_whole = masks[0]\n",
    "                label_whole = (label_whole > 0.4).int()\n",
    "                label_whole = reverse_transformations(whole_image_ds[idx], label_whole, mode='whole')\n",
    "                \n",
    "                label_whole = label_whole.squeeze()\n",
    "        \n",
    "                label_whole_for_fusion= masks[0]\n",
    "                label_whole_for_fusion = reverse_transformations(whole_image_ds[idx], label_whole_for_fusion, mode='whole')\n",
    "                \n",
    "                # Plot the first image\n",
    "            else:\n",
    "                label_whole_for_fusion = torch.zeros(original_image.shape)\n",
    "                \n",
    "            original_image_squeeze = np.load(e[0]['image_meta_dict']['filename_or_obj'])\n",
    "    \n",
    "            fusion = fuse_segmentations(label_whole_for_fusion.numpy(), label_patches_for_fusion.numpy(), prob_threshold=0.4, boost_factor=3, penalty_factor=0.5, kernel_size=150)\n",
    "            \n",
    "            fusion = (fusion > 0.4).astype(int)\n",
    "    \n",
    "            fusion = np.expand_dims(fusion, 0)\n",
    "            pred_label=fusion\n",
    "    \n",
    "            predicted_label_slices.append(pred_label.squeeze())\n",
    "            image_slices.append(original_image.squeeze())\n",
    "\n",
    "         predicted_label_volume = np.stack(predicted_label_slices, axis=-1)  # Stack along the first axis to create a 3D volume\n",
    " \n",
    "         images_volume = np.stack(image_slices, axis=-1)\n",
    "\n",
    "\n",
    "         if filter:\n",
    "             predicted_label_volume = filter_masses(predicted_label_volume, min_slices=3, window_size=3) # H x W x N\n",
    "\n",
    "         dice_mass_volume = compute_dice_score_npy(gt_label_volume, predicted_label_volume,reduction='none', class_id=1, exclude_empty=True)\n",
    "         results[patient_id] = {\n",
    "            'images_volume': images_volume,\n",
    "            'images_volume_sub': images_volume_sub,\n",
    "            'gt_volume': gt_label_volume,\n",
    "            'predicted_volume': predicted_label_volume,\n",
    "             'dice_mass_scores':dice_mass_volume\n",
    "         }\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_dict(d, key_to_del='gt_volume', old_key_name='predicted_volume', new_key_name='label_volume'):\n",
    "    del [key_to_del]\n",
    "    d[new_key_name] = d.pop(old_key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_groups_multiple_ids(results_groups, captions, num_plots, indexes, bbox_size=None, plot_captions=True):\n",
    "    # Determine the figure size dynamically based on the number of patients and plots\n",
    "    width_per_plot = max(2, 10 / num_plots)  # Ensure a minimum width\n",
    "    total_width = width_per_plot * num_plots\n",
    "    total_height = max(3, 5 - num_plots) * len(indexes)  # Height increases with more patients\n",
    "\n",
    "    fig, axs = plt.subplots(len(indexes), num_plots, figsize=(total_width, total_height))\n",
    "\n",
    "    # If there is only one patient, axs might not be a 2D array\n",
    "    if len(indexes) == 1:\n",
    "        axs = [axs]  # Make it a 2D array for consistency\n",
    "\n",
    "    # Flatten axs array for easier indexing if multiple patients\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Use subplots_adjust to reduce horizontal spacing\n",
    "    fig.subplots_adjust(wspace=0.0001, hspace=0.000007)  # Adjust this value as needed\n",
    "\n",
    "    # Loop over each index\n",
    "    for elem_idx, index in enumerate(indexes):\n",
    "        for i, (results_current, caption) in enumerate(zip(results_groups, captions)):\n",
    "            # Calculate index in axs for the current patient\n",
    "            ax_index =  elem_idx * num_plots + i\n",
    "            images_volume = results_current['images_volume']\n",
    "            predicted_volume = results_current['label_volume']\n",
    "            dice_mass_scores = results_current['dice_mass_scores']\n",
    "            img = images_volume[:,:, index] \n",
    "            pred = predicted_volume[:,:, index]\n",
    "            dice_mass_score = dice_mass_scores[index]\n",
    "\n",
    "            if bbox_size:\n",
    "                gt_mask = results_groups[-1]['label_volume'][:,:, index]\n",
    "                bbox = calculate_bbox(gt_mask, bbox_size)\n",
    "                pred = crop_to_bbox(pred, bbox)\n",
    "                img = crop_to_bbox(img, bbox)\n",
    "                \n",
    "            axs[ax_index].imshow(img, cmap='gray', interpolation='bilinear')\n",
    "            axs[ax_index].contour(pred, colors='lime', linewidths=1, alpha=1)\n",
    "            axs[ax_index].axis('off')\n",
    "\n",
    "            if plot_captions:\n",
    "                if i != len(results_groups)-1:\n",
    "                    # Place captions below each image\n",
    "                    caption_detail = f\"{caption}\\n(mDSC:{dice_mass_score:.4f})\"\n",
    "                else:\n",
    "                    caption_detail = f\"{caption}\"\n",
    "                axs[ax_index].set_title(caption_detail, fontsize=15, pad=3)\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.savefig(\"multiple_patients.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_image_groups(results_groups, captions, indexes, suffixes, save_images=False, bbox_size=None):\n",
    "    for elem_idx, index in enumerate(indexes):\n",
    "        for i, (results_current, caption) in enumerate(zip(results_groups, captions)):\n",
    "            plt.figure(figsize=(10, 10))  # Higher DPI for better resolution\n",
    "            images_volume = results_current['images_volume']\n",
    "            predicted_volume = results_current['label_volume']\n",
    "            dice_mass_scores = results_current['dice_mass_scores']\n",
    "            img = images_volume[:, :, index]\n",
    "            pred = predicted_volume[:, :, index]\n",
    "\n",
    "            if bbox_size:\n",
    "                gt_mask = results_groups[-1]['label_volume'][:, :, index]\n",
    "                bbox = calculate_bbox(gt_mask, bbox_size)\n",
    "                pred = crop_to_bbox(pred, bbox)\n",
    "                img = crop_to_bbox(img, bbox)\n",
    "\n",
    "            plt.imshow(img, cmap='gray', interpolation='bilinear')\n",
    "            plt.contour(pred, colors='lime', linewidths=1)\n",
    "            plt.axis('off')\n",
    "\n",
    "            if save_images:\n",
    "                image_filename = f\"{caption.replace(' ', '_')}_{suffixes[elem_idx]}.png\"\n",
    "                plt.savefig(image_filename, bbox_inches='tight', pad_inches=0)\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Viz for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LAXXX', 'AS0170', 'PR0760', 'GF0380', 'FP211261', 'D2MP3(VR)', 'MG0477', 'OL1062R', 'BV1252', 'D1AP7(VR)', 'SD080569', 'CC0167', 'RHCL031174', 'LA0248', 'RP271052', 'SL191251', 'LGM0159(1,5)', 'PA150139', 'HF230274', 'CF160366', 'GLA1074'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "rc('font', **{'family': 'serif', 'serif': ['cmr10']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d957cc05564be3a5985d5d4e62b438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f21bcac48ad4cb7b88af9925124a15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdd8317a1964ac8a3ea73dbf83110b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GF0380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a70e0682a0470fae37f8232b79761e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP211261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545eef94e3f040c7b7434a750a4c7d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2MP3(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac96c2e5d1334b3bb29a78adcaee702c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG0477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa856c648194d35b8064556b466f6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL1062R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0d688db15a4d6f963f633431658018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BV1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f614da38d04dbcb97676d0b0f917ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1AP7(VR)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1cae6398a3427b9380572acb9805ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD080569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e6576f4443407fb9d60b3b08f81da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC0167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f81aa6262a146e7b1d01f16e8c5bdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RHCL031174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a15e554229f4628b0629108d3cb8f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9a364c59d946539bf1706d1e862062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP271052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e1f676b8544830bee7c65a2925f2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL191251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d34288f8df470c9c1a7efac2a88ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGM0159(1,5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9a9f62be3b49b8b8fcdca7e037c23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA150139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c886b0a8234536992b8ee9c6bcf78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF230274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7722aafb94e945f888c314ecc94972f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF160366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a594c74b5544a8dbf31c90137ce723d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLA1074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f633fb5eb541a2a148ffb254b35ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# resnet, segnet, fcffnet (skinny), multiunet cabfl\n",
    "resnet_results = inference_base(model_path=\"RESNET-PRIVATE.ckpt\", \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", arch_name='UNet', patient_ids = datasets.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAXXX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12753656710a4b65974193b95d5d5db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skinny_results = inference_base(model_path=\"skinny_model_private.ckpt\", \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", arch_name='skinny', patient_ids = datasets.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiunet_cabfl = inference_fusion(model_path=\"FUSION-SUB-CABL.ckpt\", \n",
    "                              datasets=datasets, \n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              patches_dataset_key=\"patches_sub_test_ds\",patient_ids = datasets.keys())\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segnet_results = inference_base(model_path=\"segnet_model_private.ckpt\", \n",
    "                              datasets=datasets, \n",
    "                              dataset_key=\"no_thorax_sub_test_ds\", arch_name='segnet', patient_ids = datasets.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = copy.deepcopy(skinny_results)\n",
    "process_data_dict(ground_truth, key_to_del='predicted_volume', old_key_name='gt_volume', new_key_name='label_volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data_dict(multiunet_cabfl)\n",
    "process_data_dict(segnet_results)\n",
    "process_data_dict(skinny_results)\n",
    "process_data_dict(resnet_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for g in results_groups[:-1]:\n",
    "    res.append(g['dice_mass_scores'])\n",
    "\n",
    "\n",
    "# Function to find increasing indices across arrays\n",
    "def find_increasing_indices(arrays):\n",
    "    num_arrays = len(arrays)\n",
    "    num_elements = arrays[0].size  # Assumes all arrays have the same length\n",
    "    increasing_indices = []\n",
    "\n",
    "    for index in range(num_elements):\n",
    "        is_increasing = True\n",
    "        for i in range(num_arrays - 1):\n",
    "            if arrays[i][index] >= arrays[i + 1][index]:\n",
    "                is_increasing = False\n",
    "                break\n",
    "        if is_increasing:\n",
    "            increasing_indices.append(index)\n",
    "\n",
    "    return increasing_indices\n",
    "# Get the increasing indexes\n",
    "increasing_indexes = find_increasing_indices(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet, segnet, fcffnet (skinny), multiunet cabfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_groups = [no_thorax_sub_cabfl_base_unet_results, patches_sub_cabfl_results, multiunet_cabfl,cabl_cabfl_ensemble, ground_truth]\n",
    "captions = ['ResNet50-UNet \\nFull Breasts', ,'Multi-UNet', 'Ensemble & \\n Post-Processing', 'Ground Truth'] \n",
    "\n",
    "plot_image_groups_multiple_ids(results_groups=results_groups, captions=captions, num_plots= len(results_groups), indexes=indexes, bbox_size = (100,100), plot_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "save_image_groups(results_groups=results_groups, captions=captions, suffixes=['mass1', 'mass2'], indexes=indexes, save_images=True, bbox_size = (100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing_dict = {'GF220280': [0.7589285969734192, 0.7589285969734192, 1.7999992370605469],\n",
    " 'MP140270': [0.8035714030265808, 0.8035714030265808, 1.7999992370605469],\n",
    " 'SL191251': [0.7589285969734192, 0.7589285969734192, 1.8000030517578125],\n",
    " 'LPA310774': [0.7031000256538391, 0.7031000256538391, 1.0],\n",
    " 'AM14051962': [0.7031000256538391, 0.7031000256538391, 1.0],\n",
    " 'MA020377': [0.7031000256538391, 0.7031000256538391, 1.0],\n",
    " 'PBS050277': [0.8088235259056091, 0.8088235259056091, 1.7999992370605469],\n",
    " 'VF260656': [0.7031000256538391, 0.7031000256538391, 1.0],\n",
    " 'SD080569': [0.7031000256538391, 0.7031000256538391, 1.0],\n",
    " 'CC100582': [0.625, 0.625, 0.9999945163726807],\n",
    " 'BP130964': [0.5468999743461609, 0.5468999743461609, 1.0],\n",
    " 'CF160366': [0.5859000086784363, 0.5859000086784363, 1.0],\n",
    " 'EA030650': [0.7031000256538391, 0.7031000256538391, 1.0],\n",
    " 'HF230274': [0.625, 0.625, 1.0],\n",
    " 'IV100377': [0.625, 0.625, 1.0],\n",
    " 'PV200741': [0.7031000256538391, 0.7031000256538391, 1.0],\n",
    " 'PA150139': [0.5625, 0.5625, 1.5999984741210938],\n",
    " 'RP271052': [0.59375, 0.59375, 2.0],\n",
    " 'SG170880': [0.7031000256538391, 0.7031000256538391, 1.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nifti_volumes(results_dict, dest_folder, key='label_volume', interpolator=sitk.sitkNearestNeighbor):\n",
    "    def resample_image(volume, original_spacing, new_spacing, interpolator=sitk.sitkNearestNeighbor):\n",
    "        # Ensure the spacing is 3D\n",
    "\n",
    "        if len(new_spacing) == 3:\n",
    "            new_spacing = (new_spacing[0],new_spacing[1], 1)\n",
    "            \n",
    "        \n",
    "        if len(original_spacing) == 2:\n",
    "            original_spacing = (*original_spacing, 1)  # assuming the third dimension has unit length\n",
    "            \n",
    "        if len(new_spacing) == 2:\n",
    "            new_spacing = (*new_spacing, 1)  # assuming the third dimension has unit length\n",
    "    \n",
    "        # Create a SimpleITK image from the numpy array\n",
    "        sitk_volume = sitk.GetImageFromArray(volume)\n",
    "        sitk_volume.SetSpacing(original_spacing)\n",
    "    \n",
    "        # Calculate the new size\n",
    "        original_size = sitk_volume.GetSize()\n",
    "        new_size = [int(np.floor(os * osp / nsp)) for os, osp, nsp in zip(original_size, original_spacing, new_spacing)]\n",
    "    \n",
    "        # Create the resampler\n",
    "        resampler = sitk.ResampleImageFilter()\n",
    "        resampler.SetOutputSpacing(new_spacing)\n",
    "        resampler.SetSize(new_size)\n",
    "        resampler.SetInterpolator(interpolator)  \n",
    "    \n",
    "        # Resample the image\n",
    "        resampled_sitk_volume = resampler.Execute(sitk_volume)\n",
    "        resampled_volume = sitk.GetArrayFromImage(resampled_sitk_volume)\n",
    "    \n",
    "        return resampled_volume\n",
    "        \n",
    "    original_pixel_spacing = [0.5,0.5, 1]\n",
    "    \n",
    "    for patient_id in results_dict.keys():\n",
    "\n",
    "        patient_dir = os.path.join(dest_folder, str(patient_id))\n",
    "        os.makedirs(patient_dir, exist_ok=True)\n",
    "\n",
    "        volume = results_dict[patient_id][key]  \n",
    "        volume = volume.astype(np.int16)\n",
    "        volume = np.transpose(volume, (2,0,1))\n",
    "\n",
    "        #label_volume = np.expand_dims(label_volume, 2)\n",
    "    \n",
    "        new_pixel_spacing = spacing_dict[patient_id]\n",
    "    \n",
    "        # Assuming label_volume is your final NumPy array\n",
    "\n",
    "        print(f\"patient id: {patient_id}\")\n",
    "        print(f\"new_pixel_spacing: {new_pixel_spacing}\")\n",
    "        print(f\"original_pixel_spacing: {original_pixel_spacing}\")\n",
    "        print(f\"original_shape: {volume.shape}\")\n",
    "        \n",
    "        if new_pixel_spacing != original_pixel_spacing:\n",
    "            print(\"resampling..\")\n",
    "\n",
    "            volume =  resample_image(volume, original_pixel_spacing, new_pixel_spacing, interpolator=interpolator)\n",
    "        \n",
    "        print(f\"new_shape{volume.shape}\")\n",
    "        #print(f\"unique vals: {np.unique(volume)}\")\n",
    "        print()\n",
    "        \n",
    "        volume_sitk = sitk.GetImageFromArray(volume)\n",
    "        \n",
    "        \n",
    "        # Specify the file name for your NIfTI file\n",
    "        nifti_file_name = f'{patient_dir}/{patient_id}.nii.gz'\n",
    "        \n",
    "        # Save the SimpleITK Image as a NIfTI file\n",
    "        sitk.WriteImage(volume_sitk, nifti_file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabl_ensemble =inference_ensemble(\n",
    "                              model_whole_path=\"PRIVATE-FUSION-SUB-CABL.ckpt\", \n",
    "                              model_patches_path=\"PRIVATE-PATCHES-SUB-CABFL.ckpt\", \n",
    "                              patient_ids=p_id, \n",
    "                              datasets=datasets, \n",
    "                              whole_dataset_key=\"no_thorax_sub_test_ds\", \n",
    "                              patches_dataset_key=\"patches_sub_test_ds\",\n",
    "                              filter=True,\n",
    "                              subtracted=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_nifti_volumes(cabl_ensemble, dest_folder='images-subtracted-blank-december2024', key='images_volume', interpolator=sitk.sitkLinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_nifti_volumes(cabl_ensemble, dest_folder='predicted_volume-blank-december2024', key='predicted_volume', interpolator=sitk.sitkNearestNeighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabl_ensemble.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume1, volume2 = cabl_ensemble['HF230274']['images_volume'], cabl_ensemble['HF230274']['predicted_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_slices_side_by_side(volume1, volume2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume1, volume2 = cabl_ensemble['SD080569']['images_volume'], cabl_ensemble['SD080569']['predicted_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slices_side_by_side(volume1, volume2) #MM0478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08761bc078014a4fbda6b6af0c769ce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16c9df9a45224d28a65e0a1a1176eec9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b9c36ed81634100b1193d1432fd1da9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2eace97994e74d6ba825357d16e132ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2efc142160d8482e8894adeda7332c3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2188f84ad7644d29b21164c41e0eb57",
      "max": 84,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4db4bebea814d5fa82ea5bd47ef9f0c",
      "value": 66
     }
    },
    "3095f7a9d7f44208866b183640156ee9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9c36ed81634100b1193d1432fd1da9",
      "placeholder": "​",
      "style": "IPY_MODEL_47f1ac29c81d49c99f09e5c34c8c8e15",
      "value": " 79%"
     }
    },
    "45e429031f1f427b9871771145231557": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47f1ac29c81d49c99f09e5c34c8c8e15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71baf95b25a641f2881287c995997407": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9527de8dc2ed4a69afb918bb4f17cac7",
      "placeholder": "​",
      "style": "IPY_MODEL_16c9df9a45224d28a65e0a1a1176eec9",
      "value": " 66/84 [00:00&lt;00:00, 87.69it/s]"
     }
    },
    "7a5af0fc698a4a98b57c128e829e73d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8494164b9084404c92a09bdc3376130b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45e429031f1f427b9871771145231557",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7a5af0fc698a4a98b57c128e829e73d6",
      "value": 1
     }
    },
    "9527de8dc2ed4a69afb918bb4f17cac7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac0e0fe7cb7d48e3a7e0180743825e4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2188f84ad7644d29b21164c41e0eb57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba260460fc854d47aaf1f7cd18f3d726": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3095f7a9d7f44208866b183640156ee9",
       "IPY_MODEL_2efc142160d8482e8894adeda7332c3c",
       "IPY_MODEL_71baf95b25a641f2881287c995997407"
      ],
      "layout": "IPY_MODEL_e0dbd480f7684746aa5385c4e9fe8ecf"
     }
    },
    "e0dbd480f7684746aa5385c4e9fe8ecf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4db4bebea814d5fa82ea5bd47ef9f0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f7464ca2abfe47aeb7abe43533614a77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f93a6386f7f94dc291ce00dbeaf208f8",
       "IPY_MODEL_8494164b9084404c92a09bdc3376130b"
      ],
      "layout": "IPY_MODEL_ac0e0fe7cb7d48e3a7e0180743825e4f"
     }
    },
    "f93a6386f7f94dc291ce00dbeaf208f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2eace97994e74d6ba825357d16e132ae",
      "placeholder": "​",
      "style": "IPY_MODEL_08761bc078014a4fbda6b6af0c769ce3",
      "value": "0.018 MB of 0.018 MB uploaded\r"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
