{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PRIVATE DATASET INFERENCE NOTEBOOK - CLEAN VERSION\n",
        "\n",
        "This notebook performs inference using trained models on the private dataset.\n",
        "\n",
        "**Compatible with models trained in:**\n",
        "- `training_private_DECEMBER_2024_baselines_CLEAN.ipynb`\n",
        "- `training_private_DECEMBER_2024_venus_CLEAN.ipynb`\n",
        "\n",
        "**FOCUS:** Private dataset ONLY - Individual tests clearly separated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Setup and Imports\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pprint\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import sys\n",
        "\n",
        "# PyTorch and related\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# MONAI imports\n",
        "import monai\n",
        "from monai.config import print_config\n",
        "from monai.data import CacheDataset, DataLoader, Dataset\n",
        "from monai.transforms import (\n",
        "    LoadImaged, EnsureChannelFirstd, Compose\n",
        ")\n",
        "\n",
        "# Backend imports\n",
        "from breast_segmentation.config.settings import config\n",
        "from breast_segmentation.utils.seed import set_deterministic_mode, seed_worker, reseed\n",
        "from breast_segmentation.data.dataset import PairedDataset, PairedDataLoader\n",
        "from breast_segmentation.data import custom_collate_no_patches, custom_collate\n",
        "from breast_segmentation.transforms.compose import Preprocess\n",
        "from breast_segmentation.metrics.evaluation import compute_dice_score, compute_iou\n",
        "from breast_segmentation.models.lightning_module import BreastSegmentationModel\n",
        "from breast_segmentation.models.fusion_module import BreastFusionModel\n",
        "\n",
        "from breast_segmentation.data.private_dataset import (\n",
        "    PATIENT_INFO, get_filenames, get_train_val_test_dicts, PATIENTS_TO_EXCLUDE,\n",
        "    filter_samples_sample_aware, filter_samples_to_exclude, get_samples_size\n",
        ")\n",
        "\n",
        "from breast_segmentation.metrics.losses import (\n",
        "    CABFL, SurfaceLossBinary, AsymmetricUnifiedFocalLoss, \n",
        "    AsymmetricFocalLoss, AsymmetricFocalTverskyLoss, SoftDiceLoss\n",
        ")\n",
        "\n",
        "from breast_segmentation.inference.private_dataset_aware_test import (\n",
        "    test_dataset_aware_ensemble,\n",
        "    test_dataset_aware_no_patches,\n",
        "    test_dataset_aware_fusion\n",
        ")\n",
        "\n",
        "\n",
        "import time \n",
        "import os\n",
        "\n",
        "# Set precision and print config\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "print_config()\n",
        "\n",
        "# Pretty printer for results\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "# Converter function for PIL images\n",
        "def convert_to_grayscale(image):\n",
        "    \"\"\"Convert PIL image to grayscale - replaces lambda for pickling compatibility.\"\"\"\n",
        "    return image.convert(\"L\")\n",
        "\n",
        "\n",
        "# Register loss classes for checkpoint loading\n",
        "sys.modules['__main__'].CABFL = CABFL\n",
        "sys.modules['__main__'].SurfaceLossBinary = SurfaceLossBinary\n",
        "sys.modules['__main__'].AsymmetricUnifiedFocalLoss = AsymmetricUnifiedFocalLoss\n",
        "sys.modules['__main__'].AsymmetricFocalLoss = AsymmetricFocalLoss\n",
        "sys.modules['__main__'].AsymmetricFocalTverskyLoss = AsymmetricFocalTverskyLoss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Setup - Private Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "SEED = 200\n",
        "USE_SUBTRACTED = True\n",
        "\n",
        "# Data paths\n",
        "dataset_base_path = \"Dataset-arrays-4-FINAL\"\n",
        "CHECKPOINTS_DIR = \"./checkpoints/private-dataset\"\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set seeds\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Using private dataset backend functions\")\n",
        "\n",
        "# Use patient IDs from PATIENT_INFO dictionary\n",
        "patient_ids = os.listdir(dataset_base_path)\n",
        "\n",
        "# Apply exclusions\n",
        "print(f\"Initial patients from PATIENT_INFO: {len(patient_ids)}\")\n",
        "print(f\"Patients after exclusion: {len(patient_ids)}\")\n",
        "\n",
        "\n",
        "x_train_val, x_test = train_test_split(patient_ids, test_size=0.2, random_state=SEED)\n",
        "x_train, x_val = train_test_split(x_train_val, test_size=0.25, random_state=SEED)\n",
        "\n",
        "print(f\"Dataset base path: {dataset_base_path}\")\n",
        "print(f\"Total patients: {len(patient_ids)}\")\n",
        "print(f\"Train patients: {len(x_train)}\")\n",
        "print(f\"Validation patients: {len(x_val)}\")\n",
        "print(f\"Test patients: {len(x_test)}\")\n",
        "print(f\"Test patient IDs: {x_test}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Create Test Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use subtracted images as in training\n",
        "sub_third_images_path_prefixes = (\"Dataset-arrays-4-FINAL\", \"Dataset-arrays-FINAL\")\n",
        "\n",
        "\n",
        "mean_no_thorax_third_sub= 43.1498\n",
        "std_no_thorax_third_sub= 172.6704\n",
        "mean_patches_sub= 86.13536834716797\n",
        "std_patches_sub= 238.13461303710938\n",
        "\n",
        "# Create test transforms for global (no patches) data\n",
        "test_transforms_no_thorax_third_sub = Compose([\n",
        "    LoadImaged(\n",
        "        keys=[\"image\", \"label\"], \n",
        "        image_only=False, \n",
        "        reader=monai.data.NumpyReader()\n",
        "    ),\n",
        "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "    Preprocess(\n",
        "        keys=None, \n",
        "        mode='test',  \n",
        "        dataset=\"private\", \n",
        "        subtracted_images_path_prefixes=sub_third_images_path_prefixes, \n",
        "        subtrahend=mean_no_thorax_third_sub, \n",
        "        divisor=std_no_thorax_third_sub, \n",
        "        get_patches=False,\n",
        "        get_boundaryloss=True\n",
        "    )\n",
        "])\n",
        "\n",
        "# Create test transforms for patches data\n",
        "test_transforms_patches_sub = Compose([\n",
        "    LoadImaged(\n",
        "        keys=[\"image\", \"label\"], \n",
        "        image_only=False, \n",
        "        reader=monai.data.NumpyReader()\n",
        "    ),\n",
        "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "    Preprocess(\n",
        "        keys=None, \n",
        "        mode='test',  \n",
        "        dataset=\"private\", \n",
        "        subtracted_images_path_prefixes=sub_third_images_path_prefixes, \n",
        "        subtrahend=mean_patches_sub, \n",
        "        divisor=std_patches_sub, \n",
        "        get_patches=True,\n",
        "        get_boundaryloss=True\n",
        "    )\n",
        "])\n",
        "\n",
        "print(\"Test transforms created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets = {}\n",
        "\n",
        "for patient_id in x_test:\n",
        "    print(patient_id)\n",
        "\n",
        "\n",
        "    patient_id = [patient_id]\n",
        "    images_fnames, _ = get_filenames(suffix=\"images\",\n",
        "                                       base_path='Dataset-arrays-4-FINAL',\n",
        "                                       patient_ids=patient_id,\n",
        "                                       remove_black_samples=False,\n",
        "                                       get_random_samples_and_remove_black_samples=False,\n",
        "                                       random_samples_indexes_list=None)\n",
        "\n",
        "    labels_fnames, _ = get_filenames(suffix=\"masks\",\n",
        "                                      base_path='Dataset-arrays-4-FINAL',\n",
        "                                      patient_ids=patient_id,\n",
        "                                      remove_black_samples=False,\n",
        "                                      get_random_samples_and_remove_black_samples=False,\n",
        "                                      random_samples_indexes_list=None, remove_picked_samples=False)\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    test_dicts = [{\"image\": image_name, \"label\":label_name} for image_name, label_name in zip(images_fnames,labels_fnames)]\n",
        "\n",
        "    no_thorax_sub_test_ds = CacheDataset(data=test_dicts, transform=test_transforms_no_thorax_third_sub,num_workers=NUM_WORKERS)\n",
        "    patches_sub_test_ds = CacheDataset(data=test_dicts, transform=test_transforms_patches_sub,num_workers=NUM_WORKERS)\n",
        "\n",
        "    datasets[patient_id[0]]={\n",
        "        \"no_thorax_sub_test_ds\": no_thorax_sub_test_ds,\n",
        "        \"patches_sub_test_ds\": patches_sub_test_ds\n",
        "        \n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Checkpoint Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_paths = {\n",
        "    # VENUS fusion models\n",
        "    'venus_large': f'{CHECKPOINTS_DIR}/venus-large-best.ckpt',\n",
        "    \n",
        "    # Baseline models\n",
        "    'unetplusplus': f'{CHECKPOINTS_DIR}/unetplusplus_model.ckpt',\n",
        "    'skinny': f'{CHECKPOINTS_DIR}/skinny_model.ckpt',\n",
        "    'resnet50': f'{CHECKPOINTS_DIR}/resnet50-model.ckpt',\n",
        "    'fcn': f'{CHECKPOINTS_DIR}/unetplusplus_model.ckpt',  \n",
        "    'segnet': f'{CHECKPOINTS_DIR}/segnet_model_large.ckpt',\n",
        "    'swin': f'{CHECKPOINTS_DIR}/swin_model.ckpt',\n",
        "    \n",
        "    # Patches models\n",
        "    'resnet18-patches':  f'{CHECKPOINTS_DIR}/resnet18-patches-model.ckpt'\n",
        "}\n",
        "\n",
        "# Check which models are available\n",
        "print(\"Available models:\")\n",
        "for name, path in model_paths.items():\n",
        "    if os.path.exists(path):\n",
        "        print(f\"  ✓ {name}: {path}\")\n",
        "    else:\n",
        "        print(f\"  ✗ {name}: {path} (not found)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Baseline Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test baseline models (6 tests)\n",
        "baseline_tests = [\n",
        "    ('unetplusplus', 'UNet++', 'unetplusplus'),\n",
        "    ('skinny', 'SkinnyNet', 'skinny') ,\n",
        "    ('fcn', 'FCN', 'unetplusplus'), \n",
        "    ('segnet', 'SegNet', 'segnet'),\n",
        "    ('swin', 'Swin-UNETR', 'swin_unetr'),\n",
        "    ('resnet50', 'ResNet50', 'resnet50')\n",
        "]\n",
        "\n",
        "baseline_results = {}\n",
        "for model_key, model_name, arch_name in baseline_tests:\n",
        "    if os.path.exists(model_paths[model_key]):\n",
        "        print(f\"Testing {model_name} model...\")\n",
        "        result = test_dataset_aware_no_patches(\n",
        "            model_path=model_paths[model_key],\n",
        "            patient_ids=x_test,\n",
        "            datasets=datasets,\n",
        "            dataset_key=\"no_thorax_sub_test_ds\",\n",
        "            filter=False,\n",
        "            get_scores_for_statistics=False,\n",
        "            get_only_masses=False,\n",
        "            arch_name=arch_name,\n",
        "            strict=True,\n",
        "            subtracted=True\n",
        "        )\n",
        "        baseline_results[model_key] = result\n",
        "        print(f\"\\n{model_name} Results:\")\n",
        "        pp.pprint(result)\n",
        "    else:\n",
        "        print(f\"{model_paths[model_key]} not found.\")\n",
        "        baseline_results[model_key] = None\n",
        "\n",
        "print(f\"\\nCompleted {len([r for r in baseline_results.values() if r is not None])} baseline model tests.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test VENUS Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: venus-large-best.ckpt\n",
        "if os.path.exists(model_paths['venus_large']):\n",
        "    print(\"Testing VENUS Large ...\")\n",
        "    scores_for_statistics_fusion_large2 = test_dataset_aware_fusion(\n",
        "        model_path=model_paths['venus_large'],\n",
        "        patient_ids=x_test,\n",
        "        datasets=datasets,\n",
        "        whole_dataset_key=\"no_thorax_sub_test_ds\",\n",
        "        patches_dataset_key=\"patches_sub_test_ds\",\n",
        "        use_simple_fusion=False,\n",
        "        use_decoder_attention=True,\n",
        "        strict=True,\n",
        "        filter=False,\n",
        "        subtracted=True,\n",
        "        get_scores_for_statistics=False,\n",
        "        get_only_masses=False,\n",
        "        base_channels=64\n",
        "    )\n",
        "    print(\"\\nVENUS Large:\")\n",
        "    pp.pprint(scores_for_statistics_fusion_large2)\n",
        "else:\n",
        "    print(\"venus-large.ckpt not found.\")\n",
        "    scores_for_statistics_fusion_large2 = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Ensemble Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ensemble_tests = [\n",
        "\n",
        "    ('venus_large', 'resnet18-patches', False, 64, 'VENUS Large + ResNet18 patches'),\n",
        "    ('venus_large', 'resnet18-patches', True, 64, 'VENUS Large + ResNet18 patches (filtered)'),\n",
        "]\n",
        "\n",
        "\n",
        "ensemble_results = {}\n",
        "for whole_key, patches_key, use_filter, base_channels, description in ensemble_tests:\n",
        "    if os.path.exists(model_paths[whole_key]) and os.path.exists(model_paths[patches_key]):\n",
        "        print(f\"Testing Ensemble: {description}...\")\n",
        "        \n",
        "        result = test_dataset_aware_ensemble(\n",
        "            model_whole_path=model_paths[whole_key],\n",
        "            model_patches_path=model_paths[patches_key],\n",
        "            patient_ids=x_test,\n",
        "            datasets=datasets,\n",
        "            whole_dataset_key=\"no_thorax_sub_test_ds\",\n",
        "            patches_dataset_key=\"patches_sub_test_ds\",\n",
        "            filter=use_filter,\n",
        "            get_scores_for_statistics=False,\n",
        "            get_only_masses=False,\n",
        "            subtracted=True,\n",
        "            base_channels=base_channels\n",
        "        )\n",
        "        \n",
        "        ensemble_key = f\"{whole_key}+{patches_key}{'_filtered' if use_filter else ''}\"\n",
        "        ensemble_results[ensemble_key] = result\n",
        "        print(f\"\\n{description} Results:\")\n",
        "        pp.pprint(result)\n",
        "    else:\n",
        "        print(f\"Required models not found for ensemble: {description}\")\n",
        "        ensemble_key = f\"{whole_key}+{patches_key}{'_filtered' if use_filter else ''}\"\n",
        "        ensemble_results[ensemble_key] = None\n",
        "\n",
        "print(f\"\\nCompleted {len([r for r in ensemble_results.values() if r is not None])} ensemble model tests.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venus-nCPuPPcI-py3.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
